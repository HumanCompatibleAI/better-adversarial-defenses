{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock Paper Scissors actions\n",
    "action_to_descr = 'RPS'\n",
    "n_act = len(action_to_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards(a1, a2):\n",
    "    \"\"\"Rock paper scissors game.\"\"\"\n",
    "    a1 = action_to_descr[a1]\n",
    "    a2 = action_to_descr[a2]\n",
    "    if a1 == a2:\n",
    "        return (0, 0)\n",
    "    outcomes = {'RP': (-1, 1),\n",
    "     'RS': (1, -1),\n",
    "     'PS': (-1, 1),\n",
    "     \n",
    "    }\n",
    "    a1a2 = a1 + a2\n",
    "    a2a1 = a1a2[::-1]\n",
    "    if a1a2 in outcomes:\n",
    "        return outcomes[a1a2]\n",
    "    elif a2a1 in outcomes:\n",
    "        return outcomes[a2a1][::-1]\n",
    "    else:\n",
    "        raise Exception(\"Unkown action pair %s\" % a1a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All outcomes (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R vs R => tie\n",
      "R vs P => R loses\n",
      "R vs S => R wins\n",
      "P vs R => P wins\n",
      "P vs P => tie\n",
      "P vs S => P loses\n",
      "S vs R => S loses\n",
      "S vs P => S wins\n",
      "S vs S => tie\n"
     ]
    }
   ],
   "source": [
    "for a1 in range(n_act):\n",
    "    for a2 in range(n_act):\n",
    "        r = rewards(a1, a2)\n",
    "        ad1 = action_to_descr[a1]\n",
    "        ad2 = action_to_descr[a2]\n",
    "        descr = \"\"\n",
    "        if r[0] > r[1]:\n",
    "            descr = \"%s wins\" % ad1\n",
    "        elif r[1] > r[0]:\n",
    "            descr = \"%s loses\" % ad1\n",
    "        else:\n",
    "            descr = \"tie\"\n",
    "        print(\"%s vs %s => %s\" % (ad1, ad2, descr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPCAgent(object):\n",
    "    \"\"\"Rock Paper Scissors agent.\"\"\"\n",
    "    def __init__(self, noise_dim, identity=None):\n",
    "        self.noise_dim = noise_dim\n",
    "        self.identity = identity\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(10, input_shape=(self.noise_dim,), activation='relu'),\n",
    "            tf.keras.layers.Dense(n_act, activation=None),\n",
    "          tf.keras.layers.Softmax(),\n",
    "        ])\n",
    "        \n",
    "        self.data = []\n",
    "    def step(self, xi):\n",
    "        p = self.model(np.array([xi]))[0].numpy()\n",
    "        p = p / np.sum(p)\n",
    "        return np.random.choice(range(n_act), p=p)\n",
    "\n",
    "    def register_episode(self, h, opponent=None):\n",
    "        self.data.append(deepcopy(list(h)) + [opponent])\n",
    "        self._train()\n",
    "        \n",
    "    def _train(self):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        opponents = np.unique([x[-1].identity for x in self.data])\n",
    "        return \"<Agent id=%d wisdom=%d opponents=%d>\" % (self.identity, len(self.data),\n",
    "                                            len(opponents))\n",
    "\n",
    "class RPCEnv(object):\n",
    "    \"\"\"Rock paper scissors environment.\"\"\"\n",
    "    def __init__(self, noise_dim=10):\n",
    "        self.noise_dim = noise_dim\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def step(self, a1, a2):\n",
    "        R = rewards(a1, a2)\n",
    "        assert np.sum(R) == 0\n",
    "        return R\n",
    "    def reset(self):\n",
    "        return np.random.randn(self.noise_dim)\n",
    "    def __repr__(self):\n",
    "        return \"<Env noise_dim=%d>\" % self.noise_dim\n",
    "    \n",
    "class Universe(object):\n",
    "    \"\"\"Interaction between 2 agents in the environment.\"\"\"\n",
    "    def __init__(self, environment, agents, monitor):\n",
    "        self.environment = environment\n",
    "        self.monitor = monitor\n",
    "        self.agents = agents\n",
    "        assert len(self.agents) == 2\n",
    "        \n",
    "    def episode(self):\n",
    "        xi = self.environment.reset()\n",
    "        a1 = self.agents[0].step(xi)\n",
    "        a2 = self.agents[1].step(xi)\n",
    "        rews = self.environment.step(a1, a2)\n",
    "        episode = (xi, a1, a2, rews)\n",
    "        self.agents[0].register_episode(episode, opponent=self.agents[1])\n",
    "        self.agents[1].register_episode(episode, opponent=self.agents[0])\n",
    "        self.monitor.register(A1=self.agents[0],\n",
    "                              A2=self.agents[1],\n",
    "                              episode=episode)\n",
    "        return rews\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Universe\\n  Environment=%s\\n  Agents=%s\\n>\" % (self.environment, self.agents)\n",
    "    \n",
    "class Monitor(object):\n",
    "    \"\"\"Tracks agent's performance.\"\"\"\n",
    "    def __init__(self, agents):\n",
    "        self.agents = agents\n",
    "        self.data = []\n",
    "        \n",
    "    def register(self, A1, A2, episode):\n",
    "        assert A1 in self.agents\n",
    "        assert A2 in self.agents\n",
    "        self.data.append([A1, A2, deepcopy(episode)])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"<Monitor games=%d>\" % len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "env = RPCEnv(noise_dim=4)\n",
    "\n",
    "# how many agents to create?\n",
    "N_AGENTS = 10\n",
    "\n",
    "# how many games to play during test?\n",
    "N_GAMES = 100\n",
    "\n",
    "# creating agents\n",
    "AGENTS = [RPCAgent(noise_dim=env.noise_dim, identity=i) for i in range(N_AGENTS)]\n",
    "\n",
    "# to track the performance\n",
    "m = Monitor(agents=AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_agent():\n",
    "    \"\"\"Get one of the agents.\"\"\"\n",
    "    return np.random.choice(AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d0ff0fbac949c7b3242fa376e0eea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Running training games\n",
    "for _ in tqdm(range(N_GAMES)):\n",
    "    A1, A2 = sample_agent(), sample_agent()\n",
    "    U = Universe(environment=env, agents=[A1, A2], monitor=m)\n",
    "    rew = U.episode()\n",
    "    del U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Agent id=0 wisdom=22 opponents=9>,\n",
       " <Agent id=1 wisdom=23 opponents=9>,\n",
       " <Agent id=2 wisdom=15 opponents=7>,\n",
       " <Agent id=3 wisdom=20 opponents=9>,\n",
       " <Agent id=4 wisdom=18 opponents=8>,\n",
       " <Agent id=5 wisdom=26 opponents=10>,\n",
       " <Agent id=6 wisdom=22 opponents=9>,\n",
       " <Agent id=7 wisdom=16 opponents=6>,\n",
       " <Agent id=8 wisdom=20 opponents=8>,\n",
       " <Agent id=9 wisdom=18 opponents=9>]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Monitor games=100>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_stats = {A: [0 for _ in range(n_act)] for A in AGENTS}\n",
    "reward_stats = {A: {x: 0 for x in [-1, 0, 1]} for A in AGENTS}\n",
    "\n",
    "for (A1, A2, (xi, a1, a2, (r1, r2))) in m.data:\n",
    "    action_stats[A1][a1] += 1\n",
    "    action_stats[A2][a2] += 1\n",
    "    reward_stats[A1][r1] += 1\n",
    "    reward_stats[A2][r2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Agent id=0 wisdom=22 opponents=9>: [13, 5, 4],\n",
       " <Agent id=1 wisdom=23 opponents=9>: [8, 5, 10],\n",
       " <Agent id=2 wisdom=15 opponents=7>: [7, 8, 0],\n",
       " <Agent id=3 wisdom=20 opponents=9>: [9, 7, 4],\n",
       " <Agent id=4 wisdom=18 opponents=8>: [8, 6, 4],\n",
       " <Agent id=5 wisdom=26 opponents=10>: [12, 11, 3],\n",
       " <Agent id=6 wisdom=22 opponents=9>: [7, 9, 6],\n",
       " <Agent id=7 wisdom=16 opponents=6>: [2, 7, 7],\n",
       " <Agent id=8 wisdom=20 opponents=8>: [6, 6, 8],\n",
       " <Agent id=9 wisdom=18 opponents=9>: [2, 9, 7]}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<Agent id=0 wisdom=22 opponents=9>: {-1: 7, 0: 10, 1: 5},\n",
       " <Agent id=1 wisdom=23 opponents=9>: {-1: 10, 0: 5, 1: 8},\n",
       " <Agent id=2 wisdom=15 opponents=7>: {-1: 6, 0: 6, 1: 3},\n",
       " <Agent id=3 wisdom=20 opponents=9>: {-1: 6, 0: 7, 1: 7},\n",
       " <Agent id=4 wisdom=18 opponents=8>: {-1: 2, 0: 9, 1: 7},\n",
       " <Agent id=5 wisdom=26 opponents=10>: {-1: 6, 0: 14, 1: 6},\n",
       " <Agent id=6 wisdom=22 opponents=9>: {-1: 5, 0: 9, 1: 8},\n",
       " <Agent id=7 wisdom=16 opponents=6>: {-1: 3, 0: 7, 1: 6},\n",
       " <Agent id=8 wisdom=20 opponents=8>: {-1: 7, 0: 7, 1: 6},\n",
       " <Agent id=9 wisdom=18 opponents=9>: {-1: 7, 0: 8, 1: 3}}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
