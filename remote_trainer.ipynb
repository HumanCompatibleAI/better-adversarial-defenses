{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "import uuid\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "import gym_compete_rllib.gym_compete_to_rllib\n",
    "from gym_compete_rllib.gym_compete_to_rllib import created_envs, create_env\n",
    "from gym_compete_rllib.load_gym_compete_policy import nets_to_weight_array, nets_to_weights, load_weights_from_vars\n",
    "from config import get_config_test_external\n",
    "from train import build_trainer_config, ray_init\n",
    "\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from jsonrpcclient.clients.http_client import HTTPClient\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.25.237.248',\n",
       " 'raylet_ip_address': '172.25.237.248',\n",
       " 'redis_address': '172.25.237.248:6379',\n",
       " 'object_store_address': '/tmp/session_2020-09-07_00-23-13_968271_10516/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/session_2020-09-07_00-23-13_968271_10516/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/session_2020-09-07_00-23-13_968271_10516'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    }
   ],
   "source": [
    "config_ = get_config_test_external()\n",
    "config = build_trainer_config(config_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "from ray.rllib.agents import with_common_config\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import ray\n",
    "from ray.rllib.evaluation.metrics import get_learner_stats, LEARNER_STATS_KEY\n",
    "from ray.rllib.evaluation.worker_set import WorkerSet\n",
    "from ray.rllib.execution.common import SampleBatchType, \\\n",
    "    STEPS_SAMPLED_COUNTER, STEPS_TRAINED_COUNTER, LEARNER_INFO, \\\n",
    "    APPLY_GRADS_TIMER, COMPUTE_GRADS_TIMER, WORKER_UPDATE_TIMER, \\\n",
    "    LEARN_ON_BATCH_TIMER, LOAD_BATCH_TIMER, LAST_TARGET_UPDATE_TS, \\\n",
    "    NUM_TARGET_UPDATES, _get_global_vars, _check_sample_batch_type, \\\n",
    "    _get_shared_metrics\n",
    "from ray.rllib.execution.multi_gpu_impl import LocalSyncParallelOptimizer\n",
    "from ray.rllib.policy.policy import PolicyID\n",
    "from ray.rllib.policy.sample_batch import SampleBatch, DEFAULT_POLICY_ID, \\\n",
    "    MultiAgentBatch\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.utils.sgd import do_minibatch_sgd, averaged\n",
    "\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG as config_ppo\n",
    "\n",
    "from ray.rllib.agents import with_common_config\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "from ray.rllib.execution.rollout_ops import ParallelRollouts, ConcatBatches, \\\n",
    "    StandardizeFields, SelectExperiences\n",
    "from ray.rllib.execution.train_ops import TrainOneStep, TrainTFMultiGPU\n",
    "from ray.rllib.execution.metric_ops import StandardMetricsReporting\n",
    "\n",
    "tf = try_import_tf()\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rllib_samples_to_dict(samples):\n",
    "    \"\"\"Convert rllib MultiAgentBatch to a dict.\"\"\"\n",
    "    samples = samples.policy_batches\n",
    "    samples = {x: dict(y) for x, y in samples.items()}\n",
    "    return samples\n",
    "\n",
    "def filter_dict_pickleable(d, do_print=False):\n",
    "    \"\"\"Keep only simple types, recurse.\"\"\"\n",
    "    result = {}\n",
    "    allowed_types = [int, float, np.ndarray, list, dict, set, bool, str, type(None)]\n",
    "    deleted_info = '_deleted'\n",
    "    for x, y in d.items():\n",
    "        if isinstance(y, dict):\n",
    "            result[x] = filter_dict_pickleable(y, do_print=do_print)\n",
    "        elif type(y) in allowed_types:\n",
    "            result[x] = y\n",
    "        else:\n",
    "            if do_print:\n",
    "                print('deleting', x, y)\n",
    "            result[x] = deleted_info\n",
    "    return result\n",
    "\n",
    "def dict_get_any_value(d):\n",
    "    \"\"\"Return any value of a dict.\"\"\"\n",
    "    return list(d.values())[0]\n",
    "\n",
    "def unlink_ignore_error(p):\n",
    "    \"\"\"Unlink without complaining if the file does not exist.\"\"\"\n",
    "    try:\n",
    "        os.unlink(p)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_external(policies, samples, config):\n",
    "    infos = {}\n",
    "    \n",
    "    for policy in policies:\n",
    "        # only training the requested policies\n",
    "        if policy not in config['multiagent']['policies_to_train']:\n",
    "            continue\n",
    "        \n",
    "        # identifier for this run\n",
    "        run_uid = config['run_uid']\n",
    "        \n",
    "        # identifier for the run+policy\n",
    "        run_policy_uid = f\"{run_uid}_policy_{policy}\"\n",
    "        \n",
    "        # unique step information\n",
    "        iteration = str(uuid.uuid1())\n",
    "        \n",
    "        # identifier for run+policy_current step\n",
    "        run_policy_step_uid = f\"{run_uid}_policy_{policy}_step{iteration}\"\n",
    "\n",
    "        # data to pickle\n",
    "        data_policy = {}\n",
    "        \n",
    "        # config to send\n",
    "        config = filter_dict_pickleable(trainer.config)\n",
    "        \n",
    "        # data: rollouts and weights\n",
    "        data_policy['rollouts'] = rllib_samples_to_dict(samples)[policy]\n",
    "        data_policy['weights'] = nets_to_weights(policies[policy].model._nets)\n",
    "\n",
    "        # paths for data/answer\n",
    "        data_path = run_policy_step_uid + '.pkl'\n",
    "        answer_path = run_policy_step_uid + '_answer.pkl'\n",
    "        \n",
    "        # saving pickle data\n",
    "        pickle.dump(data_policy, open(data_path, 'wb'))\n",
    "\n",
    "        # connecting to the RPC server\n",
    "        client = HTTPClient(config['http_remote_port'])\n",
    "        result = client.process(run_policy_uid, uid=0, config=config, data_path=data_path, answer_path=answer_path).data.result\n",
    "        \n",
    "        # checking for result correctness\n",
    "        if result != True:\n",
    "            raise ValueError(\"Wrong result\", str(result))\n",
    "\n",
    "        # loading weights and information\n",
    "        weights_info = pickle.load(open(answer_path, 'rb'))\n",
    "        weights = weights_info['weights']\n",
    "        info = weights_info['info']\n",
    "\n",
    "        # loading weights into the model\n",
    "        def load_weights(model, weights):\n",
    "            \"\"\"Load weights into a model.\"\"\"\n",
    "            load_weights_from_vars(weights, model._nets['value'], model._nets['policy'])\n",
    "        load_weights(policies[policy].model, weights)\n",
    "\n",
    "        # removing pickle files to save space\n",
    "        unlink_ignore_error(data_path)\n",
    "        unlink_ignore_error(answer_path)\n",
    "        \n",
    "        infos[policy] = (dict(info))\n",
    "        \n",
    "    return infos\n",
    "\n",
    "class ExternalTrainOp:\n",
    "    \"\"\"Train using the function above externally.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 workers: WorkerSet,\n",
    "                 config: dict,\n",
    "                 policies: List[PolicyID] = frozenset([])):\n",
    "        self.workers = workers\n",
    "        self.policies = policies or workers.local_worker().policies_to_train\n",
    "        self.config = config\n",
    "\n",
    "    def __call__(self,\n",
    "                 samples: SampleBatchType) -> (SampleBatchType, List[dict]):\n",
    "        _check_sample_batch_type(samples)\n",
    "\n",
    "        # Handle everything as if multiagent\n",
    "        if isinstance(samples, SampleBatch):\n",
    "            samples = MultiAgentBatch({\n",
    "                DEFAULT_POLICY_ID: samples\n",
    "            }, samples.count)\n",
    "\n",
    "        # data: samples\n",
    "            \n",
    "        metrics = _get_shared_metrics()\n",
    "        load_timer = metrics.timers[LOAD_BATCH_TIMER]\n",
    "        learn_timer = metrics.timers[LEARN_ON_BATCH_TIMER]\n",
    "\n",
    "        p = {k: self.workers.local_worker().get_policy(k) for k in self.policies}\n",
    "        info = train_external(policies=p, samples=samples, config=self.config)\n",
    "                \n",
    "        load_timer.push_units_processed(samples.count)\n",
    "        learn_timer.push_units_processed(samples.count)\n",
    "        \n",
    "        fetches = info\n",
    "\n",
    "        metrics.counters[STEPS_TRAINED_COUNTER] += samples.count\n",
    "        metrics.info[LEARNER_INFO] = fetches\n",
    "        if self.workers.remote_workers():\n",
    "            with metrics.timers[WORKER_UPDATE_TIMER]:\n",
    "                weights = ray.put(self.workers.local_worker().get_weights(\n",
    "                    self.policies))\n",
    "                for e in self.workers.remote_workers():\n",
    "                    e.set_weights.remote(weights, _get_global_vars())\n",
    "        # Also update global vars of the local worker.\n",
    "        self.workers.local_worker().set_global_vars(_get_global_vars())\n",
    "        return samples, fetches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_plan(workers, config):\n",
    "    rollouts = ParallelRollouts(workers, mode=\"bulk_sync\")\n",
    "\n",
    "    # Collect large batches of relevant experiences & standardize.\n",
    "    rollouts = rollouts.for_each(\n",
    "        SelectExperiences(workers.trainable_policies()))\n",
    "    rollouts = rollouts.combine(\n",
    "        ConcatBatches(min_batch_size=config[\"train_batch_size\"]))\n",
    "    rollouts = rollouts.for_each(StandardizeFields([\"advantages\"]))\n",
    "\n",
    "    train_op = rollouts.for_each(\n",
    "        ExternalTrainOp(workers=workers,\n",
    "                        config=config))\n",
    "\n",
    "    return StandardMetricsReporting(train_op, workers, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = deepcopy(config_ppo)\n",
    "DEFAULT_CONFIG.update({'http_remote_port': \"http://127.0.0.1:50001\", 'run_uid': 'aba'})\n",
    "DEFAULT_CONFIG = with_common_config(DEFAULT_CONFIG)\n",
    "\n",
    "ExternalTrainer = build_trainer(\n",
    "    name=\"External\",\n",
    "    default_config=DEFAULT_CONFIG,\n",
    "    default_policy=PPOTFPolicy,\n",
    "    execution_plan=execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "It looks like variables {<Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>} were created as part of <gym_compete_rllib.gym_compete_to_rllib.GymCompetePretrainedModel object at 0x7f59103ee880> but does not appear in model.variables() ([]). Did you forget to call model.register_variables() on the variables in question?\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "It looks like variables {<Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>} were created as part of <gym_compete_rllib.gym_compete_to_rllib.GymCompetePretrainedModel object at 0x7f59103eebb0> but does not appear in model.variables() ([]). Did you forget to call model.register_variables() on the variables in question?\n"
     ]
    }
   ],
   "source": [
    "trainer = ExternalTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res['info']['learner']['player_1']['approxkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sergei/ray_results/External_multicomp_2020-09-07_00-27-32omqoqp9z/'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
