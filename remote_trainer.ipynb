{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\n",
    "\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "import gym_compete_rllib.gym_compete_to_rllib\n",
    "from gym_compete_rllib.gym_compete_to_rllib import created_envs, create_env\n",
    "from gym_compete_rllib.load_gym_compete_policy import nets_to_weight_array\n",
    "from config import get_config_test_external\n",
    "from train import build_trainer_config, ray_init\n",
    "\n",
    "import os\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.20.190.40',\n",
       " 'raylet_ip_address': '172.20.190.40',\n",
       " 'redis_address': '172.20.190.40:6379',\n",
       " 'object_store_address': '/tmp/session_2020-09-03_19-38-36_301848_3825/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/session_2020-09-03_19-38-36_301848_3825/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/session_2020-09-03_19-38-36_301848_3825'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    }
   ],
   "source": [
    "config_ = get_config_test_external()\n",
    "config = build_trainer_config(config_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'env': 'multicomp',\n",
       " 'env_config': {'with_video': False,\n",
       "  'env_name': 'multicomp/YouShallNotPassHumans-v0'},\n",
       " 'multiagent': {'policies_to_train': ['player_1', 'player_2'],\n",
       "  'policies': {'player_1': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "    Box(380,),\n",
       "    Box(17,),\n",
       "    {'model': {'custom_model': 'GymCompetePretrainedModel',\n",
       "      'custom_model_config': {'agent_id': 0,\n",
       "       'env_name': 'multicomp/YouShallNotPassHumans-v0',\n",
       "       'model_config': {},\n",
       "       'name': 'model_0',\n",
       "       'load_weights': False}},\n",
       "     'framework': 'tfe'}),\n",
       "   'player_2': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "    Box(380,),\n",
       "    Box(17,),\n",
       "    {'model': {'custom_model': 'GymCompetePretrainedModel',\n",
       "      'custom_model_config': {'agent_id': 1,\n",
       "       'env_name': 'multicomp/YouShallNotPassHumans-v0',\n",
       "       'model_config': {},\n",
       "       'name': 'model_1',\n",
       "       'load_weights': True}},\n",
       "     'framework': 'tfe'})},\n",
       "  'policy_mapping_fn': functools.partial(<function select_policy_default at 0x7f3019d5ad30>, config={'kl_coeff': 1.0, '_num_workers_tf': 4, 'use_gae': True, 'num_gpus': 0, '_env_name_rllib': 'multicomp', '_env_fcn': <function create_env at 0x7f3019d5aaf0>, '_policies': [None, 'from_scratch_sb', 'pretrained'], '_env': {'with_video': False, 'env_name': 'multicomp/YouShallNotPassHumans-v0'}, 'framework': 'tfe', '_train_policies': ['player_1', 'player_2'], '_call': {'checkpoint_freq': 0, 'name': 'adversarial_youshallnotpass'}, '_trainer': 'PPO', '_policy': 'PPO', '_train_steps': 10, '_update_config': None, '_run_inline': True, 'num_envs_per_worker': 4, '_log_error': True, '_model_params': {'use_lstm': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'free_log_std': True}, '_select_policy': <function select_policy_default at 0x7f3019d5ad30>, '_get_policies': <function get_policies_default at 0x7f3019d5aca0>, 'train_batch_size': 2048, 'lr': 0.0001, 'sgd_minibatch_size': 512, 'num_sgd_iter': 1, 'rollout_fragment_length': 128, 'num_workers': 2, 'multiagent': {'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 0, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_0', 'load_weights': False}}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1', 'load_weights': True}}, 'framework': 'tfe'})}}})},\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 4,\n",
       "  'inter_op_parallelism_threads': 4,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': True,\n",
       "  'device_count': {'CPU': 4},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 4,\n",
       "  'inter_op_parallelism_threads': 4},\n",
       " 'kl_coeff': 1.0,\n",
       " 'use_gae': True,\n",
       " 'num_gpus': 0,\n",
       " 'framework': 'tfe',\n",
       " 'num_envs_per_worker': 4,\n",
       " 'train_batch_size': 2048,\n",
       " 'lr': 0.0001,\n",
       " 'sgd_minibatch_size': 512,\n",
       " 'num_sgd_iter': 1,\n",
       " 'rollout_fragment_length': 128,\n",
       " 'num_workers': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation plan: change TrainOneStep, keep everything else the same!\n",
    "\n",
    "or, do we need TrainTFMultiGPU ?\n",
    "\"simple_optimizer\": False, by default, need multigpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "from ray.rllib.agents import with_common_config\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import List\n",
    "\n",
    "import ray\n",
    "from ray.rllib.evaluation.metrics import get_learner_stats, LEARNER_STATS_KEY\n",
    "from ray.rllib.evaluation.worker_set import WorkerSet\n",
    "from ray.rllib.execution.common import SampleBatchType, \\\n",
    "    STEPS_SAMPLED_COUNTER, STEPS_TRAINED_COUNTER, LEARNER_INFO, \\\n",
    "    APPLY_GRADS_TIMER, COMPUTE_GRADS_TIMER, WORKER_UPDATE_TIMER, \\\n",
    "    LEARN_ON_BATCH_TIMER, LOAD_BATCH_TIMER, LAST_TARGET_UPDATE_TS, \\\n",
    "    NUM_TARGET_UPDATES, _get_global_vars, _check_sample_batch_type, \\\n",
    "    _get_shared_metrics\n",
    "from ray.rllib.execution.multi_gpu_impl import LocalSyncParallelOptimizer\n",
    "from ray.rllib.policy.policy import PolicyID\n",
    "from ray.rllib.policy.sample_batch import SampleBatch, DEFAULT_POLICY_ID, \\\n",
    "    MultiAgentBatch\n",
    "from ray.rllib.utils import try_import_tf\n",
    "from ray.rllib.utils.sgd import do_minibatch_sgd, averaged\n",
    "\n",
    "from ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG as config_ppo\n",
    "\n",
    "from ray.rllib.agents import with_common_config\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.agents.trainer_template import build_trainer\n",
    "from ray.rllib.execution.rollout_ops import ParallelRollouts, ConcatBatches, \\\n",
    "    StandardizeFields, SelectExperiences\n",
    "from ray.rllib.execution.train_ops import TrainOneStep, TrainTFMultiGPU\n",
    "from ray.rllib.execution.metric_ops import StandardMetricsReporting\n",
    "\n",
    "tf = try_import_tf()\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "def train_external(policies, samples, config):\n",
    "    global data\n",
    "    data['samples'] = samples\n",
    "    data['policies'] = policies\n",
    "    data['config'] = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalTrainOp:\n",
    "\n",
    "    def __init__(self,\n",
    "                 workers: WorkerSet,\n",
    "                 config: dict,\n",
    "                 policies: List[PolicyID] = frozenset([])):\n",
    "        self.workers = workers\n",
    "        self.policies = policies or workers.local_worker().policies_to_train\n",
    "        self.config = config\n",
    "\n",
    "    def __call__(self,\n",
    "                 samples: SampleBatchType) -> (SampleBatchType, List[dict]):\n",
    "        _check_sample_batch_type(samples)\n",
    "\n",
    "        # Handle everything as if multiagent\n",
    "        if isinstance(samples, SampleBatch):\n",
    "            samples = MultiAgentBatch({\n",
    "                DEFAULT_POLICY_ID: samples\n",
    "            }, samples.count)\n",
    "\n",
    "        # data: samples\n",
    "            \n",
    "        metrics = _get_shared_metrics()\n",
    "        load_timer = metrics.timers[LOAD_BATCH_TIMER]\n",
    "        learn_timer = metrics.timers[LEARN_ON_BATCH_TIMER]\n",
    "\n",
    "        p = {k: self.workers.local_worker().get_policy(k) for k in self.policies}\n",
    "        info = train_external(policies=p, samples=samples, config=self.config)\n",
    "                \n",
    "        load_timer.push_units_processed(samples.count)\n",
    "        learn_timer.push_units_processed(samples.count)\n",
    "        \n",
    "        fetches = info\n",
    "\n",
    "        metrics.counters[STEPS_TRAINED_COUNTER] += samples.count\n",
    "        metrics.info[LEARNER_INFO] = fetches\n",
    "        if self.workers.remote_workers():\n",
    "            with metrics.timers[WORKER_UPDATE_TIMER]:\n",
    "                weights = ray.put(self.workers.local_worker().get_weights(\n",
    "                    self.policies))\n",
    "                for e in self.workers.remote_workers():\n",
    "                    e.set_weights.remote(weights, _get_global_vars())\n",
    "        # Also update global vars of the local worker.\n",
    "        self.workers.local_worker().set_global_vars(_get_global_vars())\n",
    "        return samples, fetches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_plan(workers, config):\n",
    "    rollouts = ParallelRollouts(workers, mode=\"bulk_sync\")\n",
    "\n",
    "    # Collect large batches of relevant experiences & standardize.\n",
    "    rollouts = rollouts.for_each(\n",
    "        SelectExperiences(workers.trainable_policies()))\n",
    "    rollouts = rollouts.combine(\n",
    "        ConcatBatches(min_batch_size=config[\"train_batch_size\"]))\n",
    "    rollouts = rollouts.for_each(StandardizeFields([\"advantages\"]))\n",
    "\n",
    "    train_op = rollouts.for_each(\n",
    "        ExternalTrainOp(workers=workers,\n",
    "                        config=config))\n",
    "\n",
    "    return StandardMetricsReporting(train_op, workers, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CONFIG = with_common_config({})\n",
    "\n",
    "ExternalTrainer = build_trainer(\n",
    "    name=\"External\",\n",
    "    default_config=config_ppo,#DEFAULT_CONFIG,\n",
    "    default_policy=PPOTFPolicy,\n",
    "    execution_plan=execution_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "It looks like variables {<Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_1/Variable:0' shape=() dtype=int64, numpy=0>>} were created as part of <gym_compete_rllib.gym_compete_to_rllib.GymCompetePretrainedModel object at 0x7f2f662a3eb0> but does not appear in model.variables() ([]). Did you forget to call model.register_variables() on the variables in question?\n",
      "WARNING:tensorflow:From /home/sergei/miniconda3/envs/chai3.8/lib/python3.8/site-packages/ray/rllib/models/tf/tf_action_dist.py:233: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "It looks like variables {<Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>, <Reference wrapping <tf.Variable 'player_2/Variable:0' shape=() dtype=int64, numpy=0>>} were created as part of <gym_compete_rllib.gym_compete_to_rllib.GymCompetePretrainedModel object at 0x7f2f40110580> but does not appear in model.variables() ([]). Did you forget to call model.register_variables() on the variables in question?\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    }
   ],
   "source": [
    "trainer = ExternalTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 6.447296843267395,\n",
       " 'episode_reward_min': -1.7270120276662055,\n",
       " 'episode_reward_mean': 5.756043243100456,\n",
       " 'episode_len_mean': 167.0952380952381,\n",
       " 'episodes_this_iter': 13,\n",
       " 'policy_reward_min': {'player_1': -10.0, 'player_2': -11.727012027666206},\n",
       " 'policy_reward_max': {'player_1': 10.0, 'player_2': 16.447296843267395},\n",
       " 'policy_reward_mean': {'player_1': -9.047619047619047,\n",
       "  'player_2': 14.803662290719505},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [6.034291087775273,\n",
       "   6.447296843267395,\n",
       "   6.057017626040761,\n",
       "   6.214644974646035,\n",
       "   6.0413624337816145,\n",
       "   -1.7270120276662055,\n",
       "   6.094030339911114,\n",
       "   6.229001018389681,\n",
       "   6.2460088277330215,\n",
       "   5.951184870675206,\n",
       "   6.069746574103192,\n",
       "   5.880199638797421,\n",
       "   5.864570258629101,\n",
       "   6.085906413092744,\n",
       "   6.087659520359011,\n",
       "   6.405629814777058,\n",
       "   6.363404599993373,\n",
       "   6.106483787531033,\n",
       "   5.914504009790107,\n",
       "   6.307038996921619,\n",
       "   6.2039384965610225],\n",
       "  'episode_lengths': [163,\n",
       "   178,\n",
       "   163,\n",
       "   144,\n",
       "   170,\n",
       "   144,\n",
       "   140,\n",
       "   156,\n",
       "   176,\n",
       "   165,\n",
       "   181,\n",
       "   185,\n",
       "   169,\n",
       "   152,\n",
       "   165,\n",
       "   172,\n",
       "   210,\n",
       "   157,\n",
       "   162,\n",
       "   163,\n",
       "   194],\n",
       "  'policy_player_1_reward': [-10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0,\n",
       "   -10.0],\n",
       "  'policy_player_2_reward': [16.034291087775273,\n",
       "   16.447296843267395,\n",
       "   16.05701762604076,\n",
       "   16.214644974646035,\n",
       "   16.041362433781615,\n",
       "   -11.727012027666206,\n",
       "   16.094030339911114,\n",
       "   16.22900101838968,\n",
       "   16.24600882773302,\n",
       "   15.951184870675206,\n",
       "   16.069746574103192,\n",
       "   15.880199638797421,\n",
       "   15.8645702586291,\n",
       "   16.085906413092744,\n",
       "   16.08765952035901,\n",
       "   16.405629814777058,\n",
       "   16.363404599993373,\n",
       "   16.106483787531033,\n",
       "   15.914504009790107,\n",
       "   16.30703899692162,\n",
       "   16.203938496561022]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 10.563853555698865,\n",
       "  'mean_processing_ms': 1.1722044048580187,\n",
       "  'mean_inference_ms': 10.835132013199626},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 2,\n",
       " 'timesteps_total': 4096,\n",
       " 'timers': {'sample_time_ms': 7468.215,\n",
       "  'sample_throughput': 274.229,\n",
       "  'load_time_ms': 0.0,\n",
       "  'load_throughput': 0.0,\n",
       "  'learn_time_ms': 0.0,\n",
       "  'learn_throughput': 0.0,\n",
       "  'update_time_ms': 35.745},\n",
       " 'info': {'learner': None,\n",
       "  'num_steps_sampled': 4096,\n",
       "  'num_steps_trained': 4096},\n",
       " 'done': False,\n",
       " 'episodes_total': 21,\n",
       " 'training_iteration': 2,\n",
       " 'experiment_id': 'bcb9430a76e44841885da7ae4135c28f',\n",
       " 'date': '2020-09-03_19-43-40',\n",
       " 'timestamp': 1599155020,\n",
       " 'time_this_iter_s': 5.420260429382324,\n",
       " 'time_total_s': 15.023324251174927,\n",
       " 'pid': 3825,\n",
       " 'hostname': 'sergeivolodin',\n",
       " 'node_ip': '172.20.190.40',\n",
       " 'config': {'num_workers': 2,\n",
       "  'num_envs_per_worker': 4,\n",
       "  'rollout_fragment_length': 128,\n",
       "  'sample_batch_size': -1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 2048,\n",
       "  'model': {'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'state_shape': None,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'custom_options': -1},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {'with_video': False,\n",
       "   'env_name': 'multicomp/YouShallNotPassHumans-v0'},\n",
       "  'env': 'multicomp',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 0.0001,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tfe',\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 4,\n",
       "   'inter_op_parallelism_threads': 4,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': True,\n",
       "   'device_count': {'CPU': 4},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 4,\n",
       "   'inter_op_parallelism_threads': 4},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'player_1': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "     Box(380,),\n",
       "     Box(17,),\n",
       "     {'model': {'custom_model': 'GymCompetePretrainedModel',\n",
       "       'custom_model_config': {'agent_id': 0,\n",
       "        'env_name': 'multicomp/YouShallNotPassHumans-v0',\n",
       "        'model_config': {},\n",
       "        'name': 'model_0',\n",
       "        'load_weights': False}},\n",
       "      'framework': 'tfe'}),\n",
       "    'player_2': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "     Box(380,),\n",
       "     Box(17,),\n",
       "     {'model': {'custom_model': 'GymCompetePretrainedModel',\n",
       "       'custom_model_config': {'agent_id': 1,\n",
       "        'env_name': 'multicomp/YouShallNotPassHumans-v0',\n",
       "        'model_config': {},\n",
       "        'name': 'model_1',\n",
       "        'load_weights': True}},\n",
       "      'framework': 'tfe'})},\n",
       "   'policy_mapping_fn': functools.partial(<function select_policy_default at 0x7f3019d5ad30>, config={'kl_coeff': 1.0, '_num_workers_tf': 4, 'use_gae': True, 'num_gpus': 0, '_env_name_rllib': 'multicomp', '_env_fcn': <function create_env at 0x7f3019d5aaf0>, '_policies': [None, 'from_scratch_sb', 'pretrained'], '_env': {'with_video': False, 'env_name': 'multicomp/YouShallNotPassHumans-v0'}, 'framework': 'tfe', '_train_policies': ['player_1', 'player_2'], '_call': {'checkpoint_freq': 0, 'name': 'adversarial_youshallnotpass'}, '_trainer': 'PPO', '_policy': 'PPO', '_train_steps': 10, '_update_config': None, '_run_inline': True, 'num_envs_per_worker': 4, '_log_error': True, '_model_params': {'use_lstm': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'free_log_std': True}, '_select_policy': <function select_policy_default at 0x7f3019d5ad30>, '_get_policies': <function get_policies_default at 0x7f3019d5aca0>, 'train_batch_size': 2048, 'lr': 0.0001, 'sgd_minibatch_size': 512, 'num_sgd_iter': 1, 'rollout_fragment_length': 128, 'num_workers': 2, 'multiagent': {'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 0, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_0', 'load_weights': False}}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1', 'load_weights': True}}, 'framework': 'tfe'})}}}),\n",
       "   'policies_to_train': ['player_1', 'player_2'],\n",
       "   'observation_fn': None},\n",
       "  'use_pytorch': -1,\n",
       "  'eager': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 1.0,\n",
       "  'sgd_minibatch_size': 512,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 1,\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False,\n",
       "  '_fake_gpus': False},\n",
       " 'time_since_restore': 15.023324251174927,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 2,\n",
       " 'perf': {'cpu_util_percent': 3.049636803874092,\n",
       "  'ram_util_percent': 50.3365617433414}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rllib_samples_to_dict(samples):\n",
    "    \"\"\"Convert rllib MultiAgentBatch to a dict.\"\"\"\n",
    "    samples = samples.policy_batches\n",
    "    samples = {x: dict(y) for x, y in samples.items()}\n",
    "    return samples\n",
    "\n",
    "def filter_dict_pickleable(d, do_print=False):\n",
    "    \"\"\"Keep only simple types, recurse.\"\"\"\n",
    "    result = {}\n",
    "    allowed_types = [int, float, np.ndarray, list, dict, set, bool, str, type(None)]\n",
    "    deleted_info = '_deleted'\n",
    "    for x, y in d.items():\n",
    "        if isinstance(y, dict):\n",
    "            result[x] = filter_dict_pickleable(y, do_print=do_print)\n",
    "        elif type(y) in allowed_types:\n",
    "            result[x] = y\n",
    "        else:\n",
    "            if do_print:\n",
    "                print('deleting', x, y)\n",
    "            result[x] = deleted_info\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['config'] = filter_dict_pickleable(data['config'])\n",
    "data['samples'] = rllib_samples_to_dict(data['samples'])\n",
    "data['policies'] = {k: nets_to_weight_array(v.model._nets) for k, v in data['policies'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data['samples'], open('rollout.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2035, 380)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['samples']['player_1']['obs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
