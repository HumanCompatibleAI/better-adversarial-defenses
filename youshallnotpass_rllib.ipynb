{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym_compete_to_rllib import created_envs, env_name, create_env, env_name_rllib\n",
    "\n",
    "import os\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 04:54:20,040\tINFO resource_spec.py:223 -- Starting Ray with 106.2 GiB memory available for workers and up to 49.53 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-21 04:54:20,829\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '128.32.175.10',\n",
       " 'raylet_ip_address': '128.32.175.10',\n",
       " 'redis_address': '128.32.175.10:15454',\n",
       " 'object_store_address': '/scratch/sergei/tmp/session_2020-07-21_04-54-20_036388_3839/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/scratch/sergei/tmp/session_2020-07-21_04-54-20_036388_3839/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/scratch/sergei/tmp/session_2020-07-21_04-54-20_036388_3839'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 04:54:22,351\tWARNING worker.py:1115 -- The dashboard on node svm failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/sergei/better-adversarial-defenses/ray/python/ray/dashboard/dashboard.py\", line 960, in <module>\n",
      "    metrics_export_address=metrics_export_address)\n",
      "  File \"/scratch/sergei/better-adversarial-defenses/ray/python/ray/dashboard/dashboard.py\", line 513, in __init__\n",
      "    build_dir = setup_static_dir(self.app)\n",
      "  File \"/scratch/sergei/better-adversarial-defenses/ray/python/ray/dashboard/dashboard.py\", line 414, in setup_static_dir\n",
      "    \"&& npm run build)\", build_dir)\n",
      "FileNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm ci && npm run build): '/scratch/sergei/better-adversarial-defenses/ray/python/ray/dashboard/client/build'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ray_init():\n",
    "    ray.shutdown()\n",
    "    return ray.init(ignore_reinit_error=True,\n",
    "                    temp_dir='/scratch/sergei/tmp') # Skip or set to ignore if already called\n",
    "\n",
    "ray_init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 256, 'train_steps': 10, 'train_policies': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLFW error: 65544, desc: b'X11: RandR gamma ramp support seems broken'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 04:54:43,204\tINFO trainer.py:598 -- Executing eagerly, with eager_tracing=False\n",
      "2020-07-21 04:54:43,205\tINFO trainer.py:628 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config\n",
      "{'env': 'YouShallNotPassHumans-v0_rllib', 'env_config': {'with_video': True}, 'num_workers': 0, 'train_batch_size': 256, 'multiagent': {'policies_to_train': [], 'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'use_lstm': False, 'fcnet_hiddens': [64, 64]}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1'}}, 'framework': 'tfe'})}, 'policy_mapping_fn': <function build_trainer_config.<locals>.select_policy at 0x7f7717042170>}, 'framework': 'tfe'}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-21 04:54:44,655\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           24384       observation_preprocessing_layer[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer  (None, 17)           0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer ( multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n"
     ]
    }
   ],
   "source": [
    "env_cls = create_env\n",
    "env_config = {'with_video': True}#True}\n",
    "\n",
    "def build_trainer_config(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Build configuration for 1 run.\"\"\"\n",
    "    obs_space = env_cls(env_config).observation_space\n",
    "    act_space = env_cls(env_config).action_space\n",
    "\n",
    "    policy_template = \"player_%d\"\n",
    "\n",
    "    def get_agent_config(agent_id):\n",
    "        agent_config_pretrained = (PPOTFPolicy, obs_space, act_space, {\n",
    "            'model': {\n",
    "                        \"custom_model\": \"GymCompetePretrainedModel\",\n",
    "                        \"custom_model_config\": {\n",
    "                            \"agent_id\": agent_id - 1,\n",
    "                            \"env_name\": env_name,\n",
    "                            \"model_config\": {},\n",
    "                            \"name\": \"model_%s\" % (agent_id - 1)\n",
    "                        },           \n",
    "                        \n",
    "                    },\n",
    "            \n",
    "            \"framework\": \"tfe\",\n",
    "        })\n",
    "        \n",
    "        agent_config_from_scratch = (PPOTFPolicy, obs_space, act_space, {\n",
    "                    \"model\": {\n",
    "                        \"use_lstm\": False,\n",
    "                        \"fcnet_hiddens\": [64, 64],\n",
    "                        #\"custom_action_dist\": \"DiagGaussian\",\n",
    "                    },\n",
    "                    \"framework\": \"tfe\",\n",
    "                })\n",
    "        \n",
    "        if agent_id == 1:\n",
    "            return agent_config_from_scratch\n",
    "        elif agent_id == 2:\n",
    "            return agent_config_pretrained\n",
    "        else:\n",
    "            raise KeyError(\"Wrong agent id %s\" % agent_id)\n",
    "\n",
    "    N_POLICIES = 2\n",
    "\n",
    "    policies = {policy_template % i: get_agent_config(i) for i in range(1, 1  + N_POLICIES)}\n",
    "    policies_keys = list(sorted(policies.keys()))\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        assert agent_id in [\"player_1\", \"player_2\"]\n",
    "        agent_ids = [\"player_1\", \"player_2\"]\n",
    "        \n",
    "        # selecting the corresponding policy (only for 2 policies)\n",
    "        return policies_keys[agent_ids.index(agent_id)]\n",
    "\n",
    "        # randomly choosing an opponent\n",
    "        # return np.random.choice(list(policies.keys()))\n",
    "    \n",
    "    if train_policies is None:\n",
    "        train_policies = list(policies.keys())\n",
    "        \n",
    "    for k in train_policies:\n",
    "        assert k in policies.keys()\n",
    "\n",
    "    config = {\n",
    "        \"env\": env_name_rllib,\n",
    "        \"env_config\": env_config,\n",
    "    #    \"gamma\": 0.9,\n",
    "      \"num_workers\": 0,\n",
    "    #  \"num_envs_per_worker\": 10,\n",
    "    #   \"rollout_fragment_length\": 10,\n",
    "       \"train_batch_size\": config['train_batch_size'],\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": train_policies,\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"framework\": \"tfe\",\n",
    "        #\"train_batch_size\": 512\n",
    "        #\"num_cpus_per_worker\": 2\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_trainer(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Create a RPS trainer for 2 agents, restore state, and only train specific policies.\"\"\"\n",
    "    \n",
    "    print(\"Using config\")\n",
    "    print(config)\n",
    "    cls = PPOTrainer\n",
    "    trainer = cls(config=config)\n",
    "    env = trainer.workers.local_worker().env\n",
    "    if restore_state is not None:\n",
    "        trainer.restore_from_object(restore_state)\n",
    "    return trainer\n",
    "\n",
    "stats = []\n",
    "\n",
    "def train(trainer, stop_iters, do_track=True):\n",
    "    \"\"\"Train the agents and return the state of the trainer.\"\"\"\n",
    "    for _ in range(stop_iters):\n",
    "        results = trainer.train()\n",
    "        print(pretty_print(results))\n",
    "        if do_track:\n",
    "            track.log(**results)\n",
    "        global stats\n",
    "        stats.append(results)\n",
    "    o = trainer.save_to_object()\n",
    "    return o\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# try changing learning rate\n",
    "config = {'train_batch_size': 256}\n",
    "\n",
    "config['train_steps'] = 10\n",
    "\n",
    "# ['humanoid_blocker', 'humanoid'],\n",
    "config['train_policies'] = []#'player_1']\n",
    "#config['num_workers'] = 32\n",
    "\n",
    "\n",
    "print(config)\n",
    "restore_state=None\n",
    "rl_config = build_trainer_config(restore_state=restore_state,\n",
    "                          train_policies=config['train_policies'],\n",
    "                          config=config)\n",
    "trainer = build_trainer(restore_state=None, config=rl_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c9d6b96101f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/sergei/better-adversarial-defenses/gym_compete_to_rllib.py\u001b[0m in \u001b[0;36mcreate_env\u001b[0;34m(config, env_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m#env = gym.make(env_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'with_video'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym_compete_env_with_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "env_cls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ray.tune.sample.loguniform(min_bound, max_bound, base=10)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.loguniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'sample_batch_size': -1,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'num_gpus': 0,\n",
       " 'train_batch_size': 256,\n",
       " 'model': {'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action_reward': False,\n",
       "  'state_shape': None,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'custom_options': -1},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env_config': {'with_video': True},\n",
       " 'env': 'YouShallNotPassHumans-v0_rllib',\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'monitor': False,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tfe',\n",
       " 'eager_tracing': False,\n",
       " 'no_eager_on_workers': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " '_use_trajectory_view_api': False,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'memory': 0,\n",
       " 'object_store_memory': 0,\n",
       " 'memory_per_worker': 0,\n",
       " 'object_store_memory_per_worker': 0,\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {'player_1': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "    Box(380,),\n",
       "    Box(17,),\n",
       "    {'model': {'use_lstm': False, 'fcnet_hiddens': [64, 64]},\n",
       "     'framework': 'tfe'}),\n",
       "   'player_2': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "    Box(380,),\n",
       "    Box(17,),\n",
       "    {'model': {'custom_model': 'GymCompetePretrainedModel',\n",
       "      'custom_model_config': {'agent_id': 1,\n",
       "       'env_name': 'multicomp/YouShallNotPassHumans-v0',\n",
       "       'model_config': {},\n",
       "       'name': 'model_1'}},\n",
       "     'framework': 'tfe'})},\n",
       "  'policy_mapping_fn': <function __main__.build_trainer_config.<locals>.select_policy(agent_id)>,\n",
       "  'policies_to_train': [],\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent'},\n",
       " 'replay_sequence_length': 1,\n",
       " 'use_pytorch': -1,\n",
       " 'eager': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_share_layers': False,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'simple_optimizer': True,\n",
       " '_fake_gpus': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    }
   ],
   "source": [
    "env = create_env(config={'with_video': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_1': array([-9.32710249e-01,  6.95427847e-02,  1.45666368e+00,  9.97072535e-01,\n",
       "        -5.13707862e-02,  5.61041578e-02,  7.72820599e-03, -9.36057776e-02,\n",
       "         5.49654006e-02, -9.29106058e-02,  1.13921151e-02,  2.91018816e-02,\n",
       "        -2.83979130e-02,  2.63614763e-02, -2.62624640e-02,  9.60320431e-03,\n",
       "        -9.83927686e-03,  7.26802975e-02,  1.12754140e-02, -4.86952067e-02,\n",
       "        -8.62042804e-02, -3.93374184e-02, -9.38066434e-02, -4.81061004e-02,\n",
       "        -2.15188954e-01,  1.44322431e-01,  5.57476580e-02, -2.90068343e-03,\n",
       "        -1.05695891e-01, -5.17121895e-02, -3.71732076e-03, -7.33339077e-02,\n",
       "        -1.07224233e-01,  7.51855207e-02, -5.59667248e-02, -9.54777046e-02,\n",
       "        -5.26137892e-02,  5.30313616e-02, -7.81256268e-02, -1.66563527e-01,\n",
       "         2.04433247e-01, -3.86015833e-02,  5.47227976e-02, -6.57769617e-02,\n",
       "        -2.04718834e-02, -3.61235890e-02, -6.84215077e-02,  2.20450794e+00,\n",
       "         2.17186677e+00,  1.00412146e-01, -3.13784764e-02, -2.20787314e-01,\n",
       "        -3.05214122e-01,  4.15190649e-01,  5.98042402e-01,  4.08457127e+00,\n",
       "         8.32207894e+00,  8.83318942e-02,  8.13789669e-02,  1.07777308e-02,\n",
       "        -7.19703008e-04, -2.15193117e-03, -1.59166287e-02,  1.11728110e-02,\n",
       "         8.35997500e-02,  3.97431719e-01,  2.03575204e+00,  4.76745626e-02,\n",
       "         4.26628723e-02,  4.89112404e-02,  3.44757414e-03,  9.29043896e-03,\n",
       "        -1.08148813e-03, -2.42562086e-01,  1.01913730e-01,  2.20300826e-01,\n",
       "         5.85278711e+00,  2.26297936e-01,  1.80110131e-01,  8.54814095e-02,\n",
       "        -3.08208276e-02, -4.59964508e-02, -8.70593437e-02, -2.56743968e-01,\n",
       "        -5.23087628e-01, -6.80703450e-01,  4.52555626e+00,  8.39023228e-01,\n",
       "         7.81337776e-01,  1.18104706e-01, -4.97103820e-02, -1.46044058e-01,\n",
       "        -2.53514078e-01, -2.73105240e-01, -4.74367574e-01, -1.38202016e+00,\n",
       "         2.63249442e+00,  1.00410969e+00,  9.48978704e-01,  1.14740065e-01,\n",
       "        -4.80344548e-02, -1.59727609e-01, -2.75822919e-01, -2.21711109e-01,\n",
       "        -3.82858075e-01, -1.27310709e+00,  1.76714587e+00,  2.48734558e-01,\n",
       "         2.34790305e-01,  4.63478055e-02,  1.43839816e-02, -5.01786616e-02,\n",
       "         5.16115417e-02, -2.20967360e-01,  3.29181458e-01, -8.55368838e-01,\n",
       "         4.52555626e+00,  8.61291237e-01,  8.84561166e-01,  2.79773379e-02,\n",
       "         4.74110839e-04, -1.43035001e-01,  6.02042663e-04, -2.50735301e-01,\n",
       "         8.62447060e-03, -1.48094430e+00,  2.63249442e+00,  1.02350043e+00,\n",
       "         1.04367348e+00,  2.75806926e-02, -6.12833364e-03, -1.49260312e-01,\n",
       "        -4.17892099e-02, -1.96674246e-01, -5.50639431e-02, -1.34112498e+00,\n",
       "         1.76714587e+00,  3.79884627e-01,  3.48137931e-01,  8.82233190e-02,\n",
       "         4.04556165e-02, -8.44958597e-02,  1.25584790e-01,  1.98365604e-01,\n",
       "        -2.93646752e-01,  7.08284026e-01,  1.59405984e+00,  2.53260059e-01,\n",
       "         3.57161483e-01,  1.95640299e-01,  7.20461930e-02, -1.74762943e-01,\n",
       "         8.96840728e-02,  4.14054667e-01, -2.23158461e-01,  4.95988153e-01,\n",
       "         1.19834313e+00,  4.11632759e-01,  2.91142377e-01,  1.84287097e-01,\n",
       "        -6.68579801e-02, -7.77443108e-02, -1.90183760e-01,  2.07766040e-01,\n",
       "         4.87570930e-01,  6.34660701e-01,  1.59405984e+00,  2.92442734e-01,\n",
       "         3.39905800e-01,  2.51159390e-01, -1.15284827e-01, -1.68329186e-01,\n",
       "        -1.31465138e-01,  4.15949539e-01,  3.43249693e-01,  4.72032133e-01,\n",
       "         1.19834313e+00, -6.38841511e-03, -1.10495676e-01, -4.00509638e-02,\n",
       "        -1.68107217e-01,  1.43359473e-01,  5.08944653e-02, -1.20539821e-02,\n",
       "        -1.83567303e-01, -3.55655158e-02, -1.48964874e-01,  1.41820110e-01,\n",
       "         4.99956552e-02, -1.17659666e-01, -1.74120660e-01, -1.95893847e-02,\n",
       "        -1.49652434e-01,  1.28023478e-01,  5.36086470e-02, -5.55934310e-02,\n",
       "        -2.84537305e-01, -6.72846162e-02, -1.43590372e-01,  1.27404617e-01,\n",
       "         6.29299527e-02, -5.40107233e-02, -2.32870989e-01, -7.70977537e-02,\n",
       "        -1.23585409e-01,  1.25999761e-01,  5.87598581e-02, -5.40107233e-02,\n",
       "        -2.32870989e-01, -7.70977537e-02, -1.23585409e-01,  1.25999761e-01,\n",
       "         5.87598581e-02, -1.69005338e-01, -3.20064821e-01,  9.39576128e-02,\n",
       "        -1.41178436e-01,  1.31834420e-01,  6.23388460e-02, -1.83445450e-01,\n",
       "        -5.20718014e-01,  1.30326993e-01, -2.19384876e-01,  1.40515696e-01,\n",
       "         7.91831264e-02, -1.83445450e-01, -5.20718014e-01,  1.30326993e-01,\n",
       "        -2.19384876e-01,  1.40515696e-01,  7.91831264e-02, -3.36817744e-02,\n",
       "        -1.62975739e-01, -8.65697802e-03, -1.43426715e-01,  1.27145988e-01,\n",
       "         4.52478121e-02, -4.02199736e-02, -1.21057085e-01, -5.89231407e-02,\n",
       "        -1.42854563e-01,  1.35341670e-01,  5.20080480e-02, -2.71030924e-02,\n",
       "        -1.31969285e-01, -6.89265238e-02, -1.64544301e-01,  1.34623631e-01,\n",
       "         5.48350093e-02, -2.14183613e-02, -7.74923626e-02, -2.79210180e-02,\n",
       "        -1.64031368e-01,  1.26799015e-01,  6.51591133e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         9.30893534e-01, -5.64171173e-02,  1.30734419e+00, -4.35478367e-03,\n",
       "         4.97657391e-02,  7.36175153e-02,  9.96034572e-01, -3.84575136e-02,\n",
       "        -5.38668729e-02, -6.36117609e-02, -9.38888831e-03, -7.06361738e-02,\n",
       "         2.51821583e-02,  8.26020343e-02, -6.46122046e-02, -9.11993429e-02,\n",
       "         2.25491002e-02,  6.66918828e-02,  8.03985363e-02,  8.81808764e-02,\n",
       "         8.33166634e-02, -1.81191178e-02, -7.03654075e-02,  5.64274151e-02]),\n",
       " 'player_2': array([ 9.30893534e-01, -5.64171173e-02,  1.30734419e+00, -4.35478367e-03,\n",
       "         4.97657391e-02,  7.36175153e-02,  9.96034572e-01, -3.84575136e-02,\n",
       "        -5.38668729e-02, -6.36117609e-02, -9.38888831e-03, -7.06361738e-02,\n",
       "         2.51821583e-02,  8.26020343e-02, -6.46122046e-02, -9.11993429e-02,\n",
       "         2.25491002e-02,  6.66918828e-02,  8.03985363e-02,  8.81808764e-02,\n",
       "         8.33166634e-02, -1.81191178e-02, -7.03654075e-02,  5.64274151e-02,\n",
       "        -6.09711730e-02, -1.79741505e-01,  3.74862424e-02, -7.86615827e-02,\n",
       "         1.37170280e-01, -3.26045902e-02, -2.48496382e-02, -1.60478009e-02,\n",
       "         3.02290072e-03, -2.40367754e-02, -6.84628408e-02, -5.37811722e-02,\n",
       "        -5.30694911e-02,  5.68012976e-02,  1.06608801e-01,  3.04257264e-02,\n",
       "         8.64338062e-02, -1.72589630e-01, -6.66428713e-02,  1.72250873e-01,\n",
       "         1.07414173e-01, -1.94818315e-01, -7.75196124e-03,  2.16764304e+00,\n",
       "         2.18337150e+00,  1.29393541e-01, -4.54001635e-02, -3.56583203e-01,\n",
       "        -2.68277335e-01,  7.12985266e-01,  5.16332513e-01,  4.05633840e+00,\n",
       "         8.32207894e+00,  8.26294600e-02,  8.57519884e-02,  1.52688767e-02,\n",
       "        -2.28653159e-03, -2.34289566e-02, -6.48053269e-03,  1.22419849e-01,\n",
       "         3.66832094e-02,  3.89809820e-01,  2.03575204e+00,  4.17217698e-02,\n",
       "         4.51825192e-02,  5.48216906e-02, -2.95067643e-04, -8.35781450e-03,\n",
       "         1.29331541e-03,  3.17970483e-01, -2.84226960e-03,  1.54568720e-01,\n",
       "         5.85278711e+00,  2.48550400e-01,  2.21048864e-01,  3.83651199e-02,\n",
       "        -3.79320746e-03, -4.84991979e-04,  6.07910842e-02,  3.79126010e-02,\n",
       "         3.57550232e-01, -8.42608886e-01,  4.52555626e+00,  8.64221144e-01,\n",
       "         8.67965620e-01,  1.87165749e-02,  6.66868702e-03, -8.93012149e-02,\n",
       "         6.78221706e-02, -1.49590992e-01,  1.23546881e-01, -1.47929730e+00,\n",
       "         2.63249442e+00,  1.01831654e+00,  1.03382230e+00,  2.35300043e-02,\n",
       "         5.95666364e-03, -1.33213169e-01,  4.52660066e-02, -1.76005090e-01,\n",
       "         5.98067565e-02, -1.33750167e+00,  1.76714587e+00,  2.59022063e-01,\n",
       "         1.91969669e-01,  7.74063902e-02, -1.89833793e-04, -6.73718727e-03,\n",
       "        -9.93761479e-02,  5.88179396e-03, -5.52308684e-01, -7.61857470e-01,\n",
       "         4.52555626e+00,  8.80211244e-01,  8.12339150e-01,  9.39258835e-02,\n",
       "        -2.93881978e-02, -9.47133026e-02, -2.50537413e-01, -1.65536661e-01,\n",
       "        -4.55375240e-01, -1.42759083e+00,  2.63249442e+00,  1.03339952e+00,\n",
       "         9.79882582e-01,  9.53610989e-02, -3.70376814e-02, -1.34608158e-01,\n",
       "        -2.63312727e-01, -1.82918597e-01, -3.57814824e-01, -1.30042683e+00,\n",
       "         1.76714587e+00,  3.94147553e-01,  2.67943912e-01,  1.41019851e-01,\n",
       "         1.06497647e-02,  4.67858674e-03, -1.80073578e-01, -2.49731388e-02,\n",
       "         4.57085381e-01,  6.38141245e-01,  1.59405984e+00,  3.31359510e-01,\n",
       "         3.01097812e-01,  1.35284056e-01,  6.12358063e-02,  1.13843554e-01,\n",
       "        -1.33346233e-01, -2.46444606e-01,  3.08349756e-01,  5.37094188e-01,\n",
       "         1.19834313e+00,  4.24272168e-01,  3.69058566e-01,  6.82489123e-02,\n",
       "        -7.02447292e-03,  9.16085346e-04,  1.39172246e-01, -9.35318983e-03,\n",
       "        -2.99662802e-01,  7.57291677e-01,  1.59405984e+00,  3.57957538e-01,\n",
       "         3.72473437e-01,  8.83256075e-02, -3.51361970e-02,  1.27502481e-01,\n",
       "         9.34208793e-02, -2.42347719e-01, -1.91350909e-01,  6.16038924e-01,\n",
       "         1.19834313e+00,  7.72525976e-02, -1.40367894e-01, -1.98816519e-02,\n",
       "         7.89502655e-04, -1.43561225e-01,  2.20259054e-02,  7.50632018e-02,\n",
       "        -1.28161018e-01, -4.67374734e-02, -3.05998363e-03, -1.42232441e-01,\n",
       "         2.29437082e-02,  7.20799146e-02, -1.28022824e-01, -4.62696820e-02,\n",
       "        -3.07364394e-03, -1.42637608e-01,  2.29762839e-02,  9.05620964e-02,\n",
       "        -8.07881572e-02, -1.22211182e-01, -9.53936019e-03, -1.40589512e-01,\n",
       "         2.26765871e-02,  8.50738800e-02, -1.33360394e-01, -1.17477234e-01,\n",
       "        -3.00959538e-02, -1.38317950e-01,  2.40712163e-02,  8.50738800e-02,\n",
       "        -1.33360394e-01, -1.17477234e-01, -3.00959538e-02, -1.38317950e-01,\n",
       "         2.40712163e-02,  1.13161709e-01, -1.76220567e-01, -1.54996867e-01,\n",
       "         8.00722515e-03, -1.39960726e-01,  2.59764746e-02,  1.07294497e-01,\n",
       "        -9.07952726e-02, -1.66782357e-01,  4.20189393e-02, -1.38078536e-01,\n",
       "         2.26870596e-02,  1.07294497e-01, -9.07952726e-02, -1.66782357e-01,\n",
       "         4.20189393e-02, -1.38078536e-01,  2.26870596e-02,  2.08954297e-01,\n",
       "        -1.36815523e-01, -1.49768125e-01, -3.11971136e-02, -6.93538055e-02,\n",
       "        -8.37818008e-03,  2.10107933e-01,  7.84181584e-03, -5.62625819e-02,\n",
       "        -4.12298578e-02, -5.60765843e-02, -2.87948665e-02, -1.99699955e-02,\n",
       "         2.76346414e-02, -1.28577999e-01, -7.73423496e-02, -1.85714997e-01,\n",
       "         2.67568896e-02, -1.94298563e-02,  2.29275520e-02, -1.22442491e-01,\n",
       "        -7.72437049e-02, -1.84741629e-01,  2.74949620e-02,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        -9.32710249e-01,  6.95427847e-02,  1.45666368e+00,  9.97072535e-01,\n",
       "        -5.13707862e-02,  5.61041578e-02,  7.72820599e-03, -9.36057776e-02,\n",
       "         5.49654006e-02, -9.29106058e-02,  1.13921151e-02,  2.91018816e-02,\n",
       "        -2.83979130e-02,  2.63614763e-02, -2.62624640e-02,  9.60320431e-03,\n",
       "        -9.83927686e-03,  7.26802975e-02,  1.12754140e-02, -4.86952067e-02,\n",
       "        -8.62042804e-02, -3.93374184e-02, -9.38066434e-02, -4.81061004e-02])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'player_1': array([-9.39808841e-01,  7.39562418e-02,  1.45406302e+00,  9.97036832e-01,\n",
       "         -5.16322090e-02,  5.66815568e-02,  6.23479142e-03, -8.58902598e-02,\n",
       "          4.45365572e-02, -9.31284658e-02,  1.14381325e-02,  2.69046436e-02,\n",
       "         -3.59658580e-02, -7.26028531e-04, -2.24370903e-02,  9.82796781e-03,\n",
       "         -3.10664467e-02,  2.70188504e-02,  8.68489840e-03, -4.22359319e-02,\n",
       "         -8.99328920e-02, -3.64478519e-02, -9.69855953e-02, -4.99961009e-02,\n",
       "         -2.52029448e-01,  1.49253393e-01, -2.32011649e-01, -2.59225816e-02,\n",
       "          8.41853158e-02, -1.14977499e-01,  3.18957916e-01, -5.70065192e-01,\n",
       "          3.31320373e-02, -1.32683991e-02, -5.43304664e-02, -4.27107097e-02,\n",
       "         -1.00685282e+00,  1.49245766e-01, -3.09021263e-02, -5.53327677e-01,\n",
       "         -1.79934933e+00, -8.45789919e-02,  2.61221658e-01, -1.49226937e-01,\n",
       "          1.28913172e-01, -1.32254648e-01, -7.72207923e-02,  2.20824900e+00,\n",
       "          2.17155733e+00,  9.88490817e-02, -2.97004152e-02, -2.06522311e-01,\n",
       "         -3.09620699e-01,  3.85568504e-01,  6.06586357e-01,  4.08716595e+00,\n",
       "          8.32207894e+00,  8.87036354e-02,  8.15475018e-02,  1.08732688e-02,\n",
       "         -4.32037579e-04, -8.51161883e-04, -1.62859791e-02,  4.49349487e-03,\n",
       "          8.53700381e-02,  3.98000837e-01,  2.03575204e+00,  4.77389836e-02,\n",
       "          4.35067916e-02,  4.99007544e-02,  3.80798341e-03,  9.65096806e-03,\n",
       "         -1.17394628e-03, -2.53078482e-01,  1.04767030e-01,  2.19736771e-01,\n",
       "          5.85278711e+00,  2.26691356e-01,  1.79483477e-01,  8.41405521e-02,\n",
       "         -2.96669670e-02, -4.38743355e-02, -8.71903759e-02, -2.48293270e-01,\n",
       "         -5.22248351e-01, -6.81865536e-01,  4.52555626e+00,  8.40471314e-01,\n",
       "          7.80522631e-01,  1.16457313e-01, -4.80618787e-02, -1.41214766e-01,\n",
       "         -2.54168996e-01, -2.63227689e-01, -4.75207651e-01, -1.38316463e+00,\n",
       "          2.63249442e+00,  1.00490013e+00,  9.48671044e-01,  1.14294594e-01,\n",
       "         -4.74564658e-02, -1.57535992e-01, -2.76434847e-01, -2.18613711e-01,\n",
       "         -3.83610420e-01, -1.27342917e+00,  1.76714587e+00,  2.49596106e-01,\n",
       "          2.34039918e-01,  4.41564354e-02,  1.34195399e-02, -4.57900330e-02,\n",
       "          5.07525760e-02, -2.05816724e-01,  3.26060009e-01, -8.57589733e-01,\n",
       "          4.52555626e+00,  8.63507843e-01,  8.83371913e-01,  2.45695387e-02,\n",
       "         -2.99853690e-04, -1.32923419e-01, -3.54694263e-03, -2.31784072e-01,\n",
       "          1.36611060e-03, -1.48299684e+00,  2.63249442e+00,  1.02496691e+00,\n",
       "          1.04313880e+00,  2.63609099e-02, -6.53573064e-03, -1.43740408e-01,\n",
       "         -4.63276393e-02, -1.89300876e-01, -6.10118117e-02, -1.34183356e+00,\n",
       "          1.76714587e+00,  3.80591077e-01,  3.48021448e-01,  8.63434055e-02,\n",
       "          3.92641921e-02, -8.21450048e-02,  1.25219329e-01,  1.92689657e-01,\n",
       "         -2.92174976e-01,  7.09732304e-01,  1.59405984e+00,  2.54403807e-01,\n",
       "          3.56222807e-01,  1.92875218e-01,  7.10769180e-02, -1.73916820e-01,\n",
       "          8.96404428e-02,  4.10527409e-01, -2.22246207e-01,  4.97780272e-01,\n",
       "          1.19834313e+00,  4.13042787e-01,  2.90727675e-01,  1.84110818e-01,\n",
       "         -6.57978070e-02, -7.62072202e-02, -1.90991264e-01,  2.03690875e-01,\n",
       "          4.88938936e-01,  6.35443069e-01,  1.59405984e+00,  2.93345219e-01,\n",
       "          3.38710619e-01,  2.49314515e-01, -1.14529688e-01, -1.67548486e-01,\n",
       "         -1.31793488e-01,  4.13144885e-01,  3.43410936e-01,  4.73020260e-01,\n",
       "          1.19834313e+00, -4.02092148e-02,  7.17413225e-02, -1.19308348e-01,\n",
       "         -2.92586568e-01,  1.36397954e-01, -2.26074598e-01, -4.38524771e-02,\n",
       "         -4.60553618e-01,  2.59580129e-01, -1.36148161e-01,  1.30861735e-01,\n",
       "         -2.32348093e-01, -1.11481946e-02, -4.63287900e-01,  2.54922027e-01,\n",
       "         -1.35953387e-01,  1.35129616e-01, -2.33485824e-01, -3.37977103e-02,\n",
       "         -5.14091333e-01,  2.12051348e-01, -1.31305504e-01,  1.33263756e-01,\n",
       "         -2.33730285e-01, -5.45091014e-03,  4.74909594e-01,  2.33508037e-02,\n",
       "          2.52444337e-01,  1.08008751e-01, -3.08447306e-01, -5.45091014e-03,\n",
       "          4.74909594e-01,  2.33508037e-02,  2.52444337e-01,  1.08008751e-01,\n",
       "         -3.08447306e-01, -1.90273435e-01, -9.88874325e-01,  4.06695592e-01,\n",
       "         -1.32564200e-01,  1.43175709e-01, -2.01622526e-01, -7.13162395e-02,\n",
       "          7.77202291e-01,  7.93120901e-02,  5.58222628e-01,  7.24611309e-02,\n",
       "         -3.32091226e-01, -7.13162395e-02,  7.77202291e-01,  7.93120901e-02,\n",
       "          5.58222628e-01,  7.24611309e-02, -3.32091226e-01, -8.68056724e-02,\n",
       "         -1.32713212e-01,  5.81098791e-02, -2.01411380e-01,  1.02217876e-01,\n",
       "         -2.41517381e-01, -1.01267689e-01, -3.78800595e-02, -5.62179332e-02,\n",
       "         -2.00171244e-01,  1.20635196e-01, -2.26397371e-01,  6.00997725e-02,\n",
       "         -7.96681689e-02, -1.53348139e-01, -2.26267940e-01,  1.87466273e-01,\n",
       "         -2.57798468e-01,  6.66362150e-02, -1.82804623e-02, -1.06957805e-01,\n",
       "         -2.25642572e-01,  1.78739627e-01, -2.46338729e-01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          9.29677475e-01, -6.18149391e-02,  1.30402027e+00, -3.14618962e-03,\n",
       "          4.59536870e-02,  7.24263360e-02,  9.96309583e-01, -3.64171325e-02,\n",
       "         -5.86927358e-02, -6.26629537e-02, -1.07490119e-02, -6.73470290e-02,\n",
       "          4.67526008e-03,  3.08427386e-02, -6.21662183e-02, -8.56949697e-02,\n",
       "          5.55179832e-03,  2.27827617e-02,  7.48291612e-02,  9.09799275e-02,\n",
       "          8.47358618e-02, -1.21707180e-02, -7.67122747e-02,  5.61610907e-02]),\n",
       "  'player_2': array([ 9.29677475e-01, -6.18149391e-02,  1.30402027e+00, -3.14618962e-03,\n",
       "          4.59536870e-02,  7.24263360e-02,  9.96309583e-01, -3.64171325e-02,\n",
       "         -5.86927358e-02, -6.26629537e-02, -1.07490119e-02, -6.73470290e-02,\n",
       "          4.67526008e-03,  3.08427386e-02, -6.21662183e-02, -8.56949697e-02,\n",
       "          5.55179832e-03,  2.27827617e-02,  7.48291612e-02,  9.09799275e-02,\n",
       "          8.47358618e-02, -1.21707180e-02, -7.67122747e-02,  5.61610907e-02,\n",
       "         -2.41053224e-02, -1.80785691e-01, -2.61257822e-01, -9.09180016e-02,\n",
       "          2.75785460e-01, -1.17374379e-01,  1.39419904e-01, -3.64941184e-01,\n",
       "          5.44326490e-02, -7.80745961e-02,  1.18791045e-01, -5.08173403e-01,\n",
       "         -1.93694324e+00,  8.10260835e-02,  1.75234573e-01, -4.15617351e-01,\n",
       "         -1.68998531e+00, -1.77189547e-01,  1.46943805e-01, -3.04703525e-02,\n",
       "          2.14974940e-01, -2.19799195e-01, -4.01977604e-02,  2.16933321e+00,\n",
       "          2.18470302e+00,  1.25788653e-01, -4.35409015e-02, -3.50624503e-01,\n",
       "         -2.62075728e-01,  7.02092868e-01,  5.03751499e-01,  4.05951990e+00,\n",
       "          8.32207894e+00,  8.26812208e-02,  8.60042120e-02,  1.53645348e-02,\n",
       "         -2.21756139e-03, -2.36798404e-02, -6.19550782e-03,  1.23643312e-01,\n",
       "          3.51200727e-02,  3.90089313e-01,  2.03575204e+00,  4.17499005e-02,\n",
       "          4.58686955e-02,  5.54938809e-02, -1.65691067e-04, -8.54422907e-03,\n",
       "          1.33454851e-03,  3.24027699e-01, -5.31812049e-03,  1.55048380e-01,\n",
       "          5.85278711e+00,  2.47805513e-01,  2.20104411e-01,  3.88946229e-02,\n",
       "         -3.23616983e-03, -2.93693984e-03,  6.14166222e-02,  2.99376115e-02,\n",
       "          3.60242214e-01, -8.40222650e-01,  4.52555626e+00,  8.65459805e-01,\n",
       "          8.68418977e-01,  1.87627262e-02,  6.94834502e-03, -8.83410754e-02,\n",
       "          7.02488393e-02, -1.49226653e-01,  1.27876233e-01, -1.47973941e+00,\n",
       "          2.63249442e+00,  1.02250090e+00,  1.03552506e+00,  2.14174318e-02,\n",
       "          5.80026356e-03, -1.24423852e-01,  4.73775879e-02, -1.64068577e-01,\n",
       "          6.24733387e-02, -1.34014142e+00,  1.76714587e+00,  2.58377662e-01,\n",
       "          1.91825180e-01,  7.73522898e-02, -9.81402582e-04, -8.50296481e-03,\n",
       "         -9.89524887e-02, -9.80757797e-05, -5.51349740e-01, -7.61322689e-01,\n",
       "          4.52555626e+00,  8.80581806e-01,  8.13935184e-01,  9.17484417e-02,\n",
       "         -2.86781739e-02, -9.32650934e-02, -2.48087837e-01, -1.63926572e-01,\n",
       "         -4.50543939e-01, -1.42921142e+00,  2.63249442e+00,  1.03628254e+00,\n",
       "          9.82559559e-01,  9.10303893e-02, -3.42501622e-02, -1.26613587e-01,\n",
       "         -2.60207787e-01, -1.71611951e-01, -3.52685419e-01, -1.30378261e+00,\n",
       "          1.76714587e+00,  3.94631442e-01,  2.68887881e-01,  1.40483059e-01,\n",
       "          1.08833175e-02,  5.01726391e-03, -1.79985297e-01, -2.57032531e-02,\n",
       "          4.56105291e-01,  6.39368801e-01,  1.59405984e+00,  3.31114538e-01,\n",
       "          3.01263561e-01,  1.35395272e-01,  6.12623002e-02,  1.14106540e-01,\n",
       "         -1.33150697e-01, -2.47011398e-01,  3.07955945e-01,  5.37077609e-01,\n",
       "          1.19834313e+00,  4.25248146e-01,  3.69341281e-01,  6.91225118e-02,\n",
       "         -7.72969250e-03,  2.64886404e-03,  1.40253164e-01, -1.30058888e-02,\n",
       "         -3.01818209e-01,  7.57503847e-01,  1.59405984e+00,  3.57893804e-01,\n",
       "          3.73281805e-01,  8.96056534e-02, -3.56637245e-02,  1.28816612e-01,\n",
       "          9.36181040e-02, -2.45049122e-01, -1.91809166e-01,  6.15812345e-01,\n",
       "          1.19834313e+00,  8.33991963e-02, -2.90002478e-01, -8.42993410e-02,\n",
       "          1.01285370e-01, -1.36804772e-01, -2.88513826e-01,  1.05173734e-01,\n",
       "          9.11171961e-02, -7.82372061e-04,  6.24297380e-03, -1.37153011e-01,\n",
       "         -2.62145231e-01,  5.14263274e-02,  9.36202602e-02,  7.50242928e-03,\n",
       "          5.98879802e-03, -1.44456362e-01, -2.61587648e-01,  1.98378372e-01,\n",
       "          6.02271742e-01,  6.92546336e-02,  2.13224415e-02, -1.49186558e-01,\n",
       "         -2.59114685e-01,  2.36665187e-03, -1.31837271e+00,  2.34773829e-01,\n",
       "         -7.27978569e-01, -6.74104352e-02, -1.97544328e-01,  2.36665187e-03,\n",
       "         -1.31837271e+00,  2.34773829e-01, -7.27978569e-01, -6.74104352e-02,\n",
       "         -1.97544328e-01,  7.84261493e-02,  4.76516480e-01, -2.31585031e-01,\n",
       "          3.10372568e-02, -1.38253614e-01, -2.48825320e-01,  1.81869868e-01,\n",
       "         -1.19585279e+00, -6.08794463e-03, -6.33032184e-01, -1.69915742e-01,\n",
       "         -1.79010479e-01,  1.81869868e-01, -1.19585279e+00, -6.08794463e-03,\n",
       "         -6.33032184e-01, -1.69915742e-01, -1.79010479e-01,  2.21108757e-01,\n",
       "         -1.05956044e-01, -9.86887449e-02,  9.07880386e-03, -6.91331598e-02,\n",
       "         -3.05401729e-01,  2.20875055e-01, -1.31459406e-01, -1.15415757e-01,\n",
       "          1.07740115e-02, -7.15143617e-02, -3.01794840e-01, -9.91047799e-02,\n",
       "         -5.72727552e-02, -1.68670731e-01, -1.36486828e-02, -2.26622461e-01,\n",
       "         -2.87652450e-01, -9.64519542e-02, -8.15986349e-02, -1.36794971e-01,\n",
       "         -1.32138375e-02, -2.21541240e-01, -2.83810923e-01,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         -9.39808841e-01,  7.39562418e-02,  1.45406302e+00,  9.97036832e-01,\n",
       "         -5.16322090e-02,  5.66815568e-02,  6.23479142e-03, -8.58902598e-02,\n",
       "          4.45365572e-02, -9.31284658e-02,  1.14381325e-02,  2.69046436e-02,\n",
       "         -3.59658580e-02, -7.26028531e-04, -2.24370903e-02,  9.82796781e-03,\n",
       "         -3.10664467e-02,  2.70188504e-02,  8.68489840e-03, -4.22359319e-02,\n",
       "         -8.99328920e-02, -3.64478519e-02, -9.69855953e-02, -4.99961009e-02])},\n",
       " {'player_1': 0.05, 'player_2': 0.000788007670094153},\n",
       " {'player_1': False, 'player_2': False, '__all__': False},\n",
       " {'player_1': {'reward_forward': 0.0,\n",
       "   'reward_ctrl': 0.0,\n",
       "   'reward_contact': 0.0,\n",
       "   'reward_survive': 5.0,\n",
       "   'reward_move': 9.929677475428244,\n",
       "   'agent_done': False,\n",
       "   'reward_remaining': 0.0},\n",
       "  'player_2': {'reward_forward': 0.008478242437659489,\n",
       "   'reward_ctrl': 0.0,\n",
       "   'reward_contact': 0.0,\n",
       "   'reward_survive': 5.0,\n",
       "   'reward_goal_dist': -4.929677475428244,\n",
       "   'reward_move': 0.0788007670094153,\n",
       "   'agent_done': False,\n",
       "   'reward_remaining': 0.0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step({'player_1': np.zeros(380), 'player_2': np.zeros(380)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sergei/ray_results/./adversarial/train_one_3_2020-07-17_21-34-39m9qon2mn/checkpoint_0/checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ddfe84de2cbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sergei/ray_results/./adversarial/train_one_3_2020-07-17_21-34-39m9qon2mn/checkpoint_0/checkpoint'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mck\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'base64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sergei/ray_results/./adversarial/train_one_3_2020-07-17_21-34-39m9qon2mn/checkpoint_0/checkpoint'"
     ]
    }
   ],
   "source": [
    "import pickle, json, codecs\n",
    "ck = json.load(open('/home/sergei/ray_results/./adversarial/train_one_3_2020-07-17_21-34-39m9qon2mn/checkpoint_0/checkpoint', 'r'))\n",
    "w = codecs.decode(ck['weights'].encode(), 'base64')\n",
    "trainer.set_weights(pickle.loads(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'sample_batch_size': -1,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'num_gpus': 0,\n",
       " 'train_batch_size': 256,\n",
       " 'model': {'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action_reward': False,\n",
       "  'state_shape': None,\n",
       "  'framestack': True,\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'custom_options': -1},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env_config': {'with_video': True},\n",
       " 'env': 'YouShallNotPassHumans-v0_rllib',\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'monitor': False,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tfe',\n",
       " 'eager_tracing': False,\n",
       " 'no_eager_on_workers': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " '_use_trajectory_view_api': False,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'memory': 0,\n",
       " 'object_store_memory': 0,\n",
       " 'memory_per_worker': 0,\n",
       " 'object_store_memory_per_worker': 0,\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {'player_1': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "    Box(380,),\n",
       "    Box(17,),\n",
       "    {'model': {'use_lstm': False, 'fcnet_hiddens': [64, 64]},\n",
       "     'framework': 'tfe'}),\n",
       "   'player_2': (ray.rllib.policy.tf_policy_template.PPOTFPolicy,\n",
       "    Box(380,),\n",
       "    Box(17,),\n",
       "    {'model': {'custom_model': 'GymCompetePretrainedModel',\n",
       "      'custom_model_config': {'agent_id': 1,\n",
       "       'env_name': 'multicomp/YouShallNotPassHumans-v0',\n",
       "       'model_config': {},\n",
       "       'name': 'model_1'}},\n",
       "     'framework': 'tfe'})},\n",
       "  'policy_mapping_fn': <function __main__.build_trainer_config.<locals>.select_policy(agent_id)>,\n",
       "  'policies_to_train': [],\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent'},\n",
       " 'replay_sequence_length': 1,\n",
       " 'use_pytorch': -1,\n",
       " 'eager': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_share_layers': False,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'simple_optimizer': True,\n",
       " '_fake_gpus': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer observation_preprocessing_layer_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sergei/better-adversarial-defenses/ray/python/ray/rllib/policy/tf_policy.py:871: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-43-20\n",
      "done: false\n",
      "episode_len_mean: 152.0\n",
      "episode_reward_max: -0.3383652470135523\n",
      "episode_reward_mean: -0.48690533688773474\n",
      "episode_reward_min: -0.6354454267619172\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 2\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.999656677246094\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009587731212377548\n",
      "      policy_loss: -0.1705915331840515\n",
      "      total_loss: 7.459905624389648\n",
      "      vf_explained_var: 0.019690439105033875\n",
      "      vf_loss: 7.628579139709473\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6572747230529785\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03490670397877693\n",
      "      policy_loss: -0.43064308166503906\n",
      "      total_loss: 289547.875\n",
      "      vf_explained_var: -1.4901161193847656e-08\n",
      "      vf_loss: 289548.3125\n",
      "  num_steps_sampled: 400\n",
      "  num_steps_trained: 400\n",
      "iterations_since_restore: 1\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.854304635761586\n",
      "  ram_util_percent: 11.24635761589404\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -16.41731323965064\n",
      "  player_2: 16.293715177661962\n",
      "policy_reward_mean:\n",
      "  player_1: -16.67323692203726\n",
      "  player_2: 16.186331585149524\n",
      "policy_reward_min:\n",
      "  player_1: -16.929160604423878\n",
      "  player_2: 16.078947992637083\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 171.56388991491457\n",
      "  mean_inference_ms: 17.908048154112706\n",
      "  mean_processing_ms: 0.8262220463550596\n",
      "time_since_restore: 86.06222319602966\n",
      "time_this_iter_s: 86.06222319602966\n",
      "time_total_s: 86.06222319602966\n",
      "timers:\n",
      "  learn_throughput: 41.098\n",
      "  learn_time_ms: 9732.761\n",
      "  sample_throughput: 5.241\n",
      "  sample_time_ms: 76324.751\n",
      "timestamp: 1595047400\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-44-46\n",
      "done: false\n",
      "episode_len_mean: 147.4\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: 0.23397020375976788\n",
      "episode_reward_min: -0.6354454267619172\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 5\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.99884033203125\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008243048563599586\n",
      "      policy_loss: -0.15107716619968414\n",
      "      total_loss: 7.984766006469727\n",
      "      vf_explained_var: 0.004296213388442993\n",
      "      vf_loss: 8.134195327758789\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.670058012008667\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.027297019958496094\n",
      "      policy_loss: -0.27210700511932373\n",
      "      total_loss: 314019.34375\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 314019.59375\n",
      "  num_steps_sampled: 800\n",
      "  num_steps_trained: 800\n",
      "iterations_since_restore: 2\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.897520661157024\n",
      "  ram_util_percent: 11.400000000000004\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -14.711568007019647\n",
      "  player_2: 16.293715177661962\n",
      "policy_reward_mean:\n",
      "  player_1: -15.923255963916494\n",
      "  player_2: 16.157226167676264\n",
      "policy_reward_min:\n",
      "  player_1: -16.929160604423878\n",
      "  player_2: 15.979230703914018\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 172.46289960338225\n",
      "  mean_inference_ms: 17.765181633356956\n",
      "  mean_processing_ms: 0.8225194016129288\n",
      "time_since_restore: 172.32843685150146\n",
      "time_this_iter_s: 86.2662136554718\n",
      "time_total_s: 172.32843685150146\n",
      "timers:\n",
      "  learn_throughput: 42.407\n",
      "  learn_time_ms: 9432.483\n",
      "  sample_throughput: 5.213\n",
      "  sample_time_ms: 76727.483\n",
      "timestamp: 1595047486\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 800\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-46-14\n",
      "done: false\n",
      "episode_len_mean: 152.28571428571428\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: 0.03550381428958919\n",
      "episode_reward_min: -0.6354454267619172\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 7\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.97022819519043\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009374849498271942\n",
      "      policy_loss: -0.11781615763902664\n",
      "      total_loss: 4.8440680503845215\n",
      "      vf_explained_var: 0.010950207710266113\n",
      "      vf_loss: 4.960009574890137\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6566810607910156\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.02631378546357155\n",
      "      policy_loss: -0.2674381136894226\n",
      "      total_loss: 203663.1875\n",
      "      vf_explained_var: 4.470348358154297e-08\n",
      "      vf_loss: 203663.4375\n",
      "  num_steps_sampled: 1200\n",
      "  num_steps_trained: 1200\n",
      "iterations_since_restore: 3\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.57822580645162\n",
      "  ram_util_percent: 11.478225806451611\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -14.711568007019647\n",
      "  player_2: 16.3411272843522\n",
      "policy_reward_mean:\n",
      "  player_1: -16.128770689195857\n",
      "  player_2: 16.164274503485444\n",
      "policy_reward_min:\n",
      "  player_1: -16.929160604423878\n",
      "  player_2: 15.979230703914018\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 173.02726304265022\n",
      "  mean_inference_ms: 17.729138629650073\n",
      "  mean_processing_ms: 0.8196992393070752\n",
      "time_since_restore: 259.987939119339\n",
      "time_this_iter_s: 87.65950226783752\n",
      "time_total_s: 259.987939119339\n",
      "timers:\n",
      "  learn_throughput: 42.428\n",
      "  learn_time_ms: 9427.847\n",
      "  sample_throughput: 5.179\n",
      "  sample_time_ms: 77229.922\n",
      "timestamp: 1595047574\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1200\n",
      "training_iteration: 3\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-47-41\n",
      "done: false\n",
      "episode_len_mean: 153.1\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.5773715129351997\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 10\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.96613121032715\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008200100623071194\n",
      "      policy_loss: -0.18487787246704102\n",
      "      total_loss: 4.58615779876709\n",
      "      vf_explained_var: 0.014774680137634277\n",
      "      vf_loss: 4.76939582824707\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.660581111907959\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.017345130443572998\n",
      "      policy_loss: -0.10836160182952881\n",
      "      total_loss: 180444.28125\n",
      "      vf_explained_var: 1.4901161193847656e-08\n",
      "      vf_loss: 180444.390625\n",
      "  num_steps_sampled: 1600\n",
      "  num_steps_trained: 1600\n",
      "iterations_since_restore: 4\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.817213114754104\n",
      "  ram_util_percent: 11.5155737704918\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -6.061309052052611\n",
      "  player_2: 16.3411272843522\n",
      "policy_reward_mean:\n",
      "  player_1: -15.23743876881793\n",
      "  player_2: 13.660067255882728\n",
      "policy_reward_min:\n",
      "  player_1: -17.330246802826228\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 173.55593458948593\n",
      "  mean_inference_ms: 17.727375834283485\n",
      "  mean_processing_ms: 0.8096939889686385\n",
      "time_since_restore: 347.3342533111572\n",
      "time_this_iter_s: 87.34631419181824\n",
      "time_total_s: 347.3342533111572\n",
      "timers:\n",
      "  learn_throughput: 42.318\n",
      "  learn_time_ms: 9452.229\n",
      "  sample_throughput: 5.17\n",
      "  sample_time_ms: 77376.865\n",
      "timestamp: 1595047661\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1600\n",
      "training_iteration: 4\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-48-59\n",
      "done: false\n",
      "episode_len_mean: 155.0\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.439494340881015\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 12\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.96213150024414\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009404505603015423\n",
      "      policy_loss: -0.14755314588546753\n",
      "      total_loss: 3.558978796005249\n",
      "      vf_explained_var: 0.0027324706315994263\n",
      "      vf_loss: 3.70465087890625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.652601957321167\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01636825129389763\n",
      "      policy_loss: -0.23857149481773376\n",
      "      total_loss: 109106.5\n",
      "      vf_explained_var: -2.9802322387695312e-08\n",
      "      vf_loss: 109106.7265625\n",
      "  num_steps_sampled: 2000\n",
      "  num_steps_trained: 2000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.961467889908253\n",
      "  ram_util_percent: 8.231192660550457\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -6.061309052052611\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.505438037843183\n",
      "  player_2: 14.065943696962167\n",
      "policy_reward_min:\n",
      "  player_1: -17.330246802826228\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 173.20117654705004\n",
      "  mean_inference_ms: 17.64784966013266\n",
      "  mean_processing_ms: 0.8003299021580826\n",
      "time_since_restore: 424.4683210849762\n",
      "time_this_iter_s: 77.13406777381897\n",
      "time_total_s: 424.4683210849762\n",
      "timers:\n",
      "  learn_throughput: 44.097\n",
      "  learn_time_ms: 9070.879\n",
      "  sample_throughput: 5.276\n",
      "  sample_time_ms: 75818.66\n",
      "timestamp: 1595047739\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2000\n",
      "training_iteration: 5\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-50-21\n",
      "done: false\n",
      "episode_len_mean: 152.6\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.9749229397147223\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 15\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.970947265625\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008085336536169052\n",
      "      policy_loss: -0.10574042052030563\n",
      "      total_loss: 6.095095634460449\n",
      "      vf_explained_var: 0.06141003966331482\n",
      "      vf_loss: 6.199219226837158\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.649232864379883\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0186447836458683\n",
      "      policy_loss: -0.2303936779499054\n",
      "      total_loss: 65946.4375\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 65946.65625\n",
      "  num_steps_sampled: 2400\n",
      "  num_steps_trained: 2400\n",
      "iterations_since_restore: 6\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.82672413793104\n",
      "  ram_util_percent: 10.979310344827587\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -14.831289111257624\n",
      "  player_2: 12.856366171542902\n",
      "policy_reward_min:\n",
      "  player_1: -17.330246802826228\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 172.6732041024897\n",
      "  mean_inference_ms: 17.521459813542126\n",
      "  mean_processing_ms: 0.7898994336784765\n",
      "time_since_restore: 506.4436948299408\n",
      "time_this_iter_s: 81.9753737449646\n",
      "time_total_s: 506.4436948299408\n",
      "timers:\n",
      "  learn_throughput: 44.281\n",
      "  learn_time_ms: 9033.224\n",
      "  sample_throughput: 5.307\n",
      "  sample_time_ms: 75369.592\n",
      "timestamp: 1595047821\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2400\n",
      "training_iteration: 6\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-51-39\n",
      "done: false\n",
      "episode_len_mean: 154.44444444444446\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.7755078830009445\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 18\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.939289093017578\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00987166352570057\n",
      "      policy_loss: -0.23939475417137146\n",
      "      total_loss: 3.8266749382019043\n",
      "      vf_explained_var: 0.011059343814849854\n",
      "      vf_loss: 4.064095497131348\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.633679151535034\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.018415728583931923\n",
      "      policy_loss: -0.1773742139339447\n",
      "      total_loss: 105285.9296875\n",
      "      vf_explained_var: -7.450580596923828e-08\n",
      "      vf_loss: 105286.09375\n",
      "  num_steps_sampled: 2800\n",
      "  num_steps_trained: 2800\n",
      "iterations_since_restore: 7\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.353153153153155\n",
      "  ram_util_percent: 11.299999999999997\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.188734549141568\n",
      "  player_2: 13.413226666140627\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 171.98180472514215\n",
      "  mean_inference_ms: 17.411099994632508\n",
      "  mean_processing_ms: 0.7824945340823684\n",
      "time_since_restore: 584.5595397949219\n",
      "time_this_iter_s: 78.11584496498108\n",
      "time_total_s: 584.5595397949219\n",
      "timers:\n",
      "  learn_throughput: 44.355\n",
      "  learn_time_ms: 9018.055\n",
      "  sample_throughput: 5.37\n",
      "  sample_time_ms: 74486.239\n",
      "timestamp: 1595047899\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2800\n",
      "training_iteration: 7\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-52-57\n",
      "done: false\n",
      "episode_len_mean: 155.15\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.679943581202425\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 20\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.962997436523438\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.010265516117215157\n",
      "      policy_loss: -0.13697034120559692\n",
      "      total_loss: 2.7211174964904785\n",
      "      vf_explained_var: 0.029545217752456665\n",
      "      vf_loss: 2.856034755706787\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6312501430511475\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0172463096678257\n",
      "      policy_loss: -0.19734029471874237\n",
      "      total_loss: 32541.359375\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 32541.546875\n",
      "  num_steps_sampled: 3200\n",
      "  num_steps_trained: 3200\n",
      "iterations_since_restore: 8\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.437272727272724\n",
      "  ram_util_percent: 11.361818181818188\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.366843078630193\n",
      "  player_2: 13.686899497427765\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 171.4839889759408\n",
      "  mean_inference_ms: 17.34639999421464\n",
      "  mean_processing_ms: 0.777455374754709\n",
      "time_since_restore: 662.4604878425598\n",
      "time_this_iter_s: 77.90094804763794\n",
      "time_total_s: 662.4604878425598\n",
      "timers:\n",
      "  learn_throughput: 44.578\n",
      "  learn_time_ms: 8972.951\n",
      "  sample_throughput: 5.418\n",
      "  sample_time_ms: 73830.583\n",
      "timestamp: 1595047977\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 3200\n",
      "training_iteration: 8\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-54-16\n",
      "done: false\n",
      "episode_len_mean: 156.36363636363637\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.582332371233702\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 22\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.929397583007812\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009487688541412354\n",
      "      policy_loss: -0.17401744425296783\n",
      "      total_loss: 2.2138137817382812\n",
      "      vf_explained_var: 0.062478989362716675\n",
      "      vf_loss: 2.3859336376190186\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.626664161682129\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.021737566217780113\n",
      "      policy_loss: -0.22351984679698944\n",
      "      total_loss: 80086.203125\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 80086.4140625\n",
      "  num_steps_sampled: 3600\n",
      "  num_steps_trained: 3600\n",
      "iterations_since_restore: 9\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.355357142857144\n",
      "  ram_util_percent: 11.400000000000004\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.501638686661261\n",
      "  player_2: 13.919306315427555\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 170.9780379833169\n",
      "  mean_inference_ms: 17.291891740543726\n",
      "  mean_processing_ms: 0.7730398913108744\n",
      "time_since_restore: 741.2890477180481\n",
      "time_this_iter_s: 78.82855987548828\n",
      "time_total_s: 741.2890477180481\n",
      "timers:\n",
      "  learn_throughput: 44.566\n",
      "  learn_time_ms: 8975.384\n",
      "  sample_throughput: 5.451\n",
      "  sample_time_ms: 73385.763\n",
      "timestamp: 1595048056\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 3600\n",
      "training_iteration: 9\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-55-36\n",
      "done: false\n",
      "episode_len_mean: 156.32\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.4059728346553628\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 25\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.951248168945312\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.010141189210116863\n",
      "      policy_loss: -0.09169439971446991\n",
      "      total_loss: 2.4856820106506348\n",
      "      vf_explained_var: 0.10534541308879852\n",
      "      vf_loss: 2.575348138809204\n",
      "    player_2:\n",
      "      cur_kl_coeff: 1.0125000476837158\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6213395595550537\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.014543559402227402\n",
      "      policy_loss: -0.1882309913635254\n",
      "      total_loss: 28095.765625\n",
      "      vf_explained_var: -4.470348358154297e-08\n",
      "      vf_loss: 28095.9375\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.66283185840708\n",
      "  ram_util_percent: 11.400000000000004\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.589867557575491\n",
      "  player_2: 14.183894722920126\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 170.30594774297373\n",
      "  mean_inference_ms: 17.21855754244262\n",
      "  mean_processing_ms: 0.767352779800888\n",
      "time_since_restore: 821.1280779838562\n",
      "time_this_iter_s: 79.8390302658081\n",
      "time_total_s: 821.1280779838562\n",
      "timers:\n",
      "  learn_throughput: 44.597\n",
      "  learn_time_ms: 8969.301\n",
      "  sample_throughput: 5.469\n",
      "  sample_time_ms: 73139.335\n",
      "timestamp: 1595048136\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 10\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sergei/ray_results/PPO_YouShallNotPassHumans-v0_rllib_2020-07-17_21-41-32btsd_9s9/tmpoccvyew6save_to_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0e323b12d4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-02b3e772c635>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainer, stop_iters, do_track)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sergei/better-adversarial-defenses/ray/python/ray/tune/trainable.py\u001b[0m in \u001b[0;36msave_to_object\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mObject\u001b[0m \u001b[0mholding\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mtmpdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"save_to_object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# Save all files in subtree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sergei/miniconda3/lib/python3.7/tempfile.py\u001b[0m in \u001b[0;36mmkdtemp\u001b[0;34m(suffix, prefix, dir)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0o700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mcontinue\u001b[0m    \u001b[0;31m# try again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sergei/ray_results/PPO_YouShallNotPassHumans-v0_rllib_2020-07-17_21-41-32btsd_9s9/tmpoccvyew6save_to_object'"
     ]
    }
   ],
   "source": [
    "train(trainer, config['train_steps'], do_track=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array([y for x in stats for y in x['hist_stats']['policy_player_1_reward']]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.close() for x in created_envs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gym_compete_policy import get_policy_value_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env_cls(env_config).reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = get_policy_value_nets(env_name, 0)\n",
    "policy_net_orig = nets['policy_mean_logstd_flat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainer.get_policy('player_1').model\n",
    "m.policy_net(obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets['policy_mean_logstd'](obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logstd = tf.split(policy_net_orig(obs['player_1'].reshape(1, -1)), 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.clip(np.random.normal(loc=mean[0], scale=np.exp(logstd[0]), size=(17,)), -0.4, 0.4) for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compute_action(obs['player_1'], policy_id='player_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cls(env_config).action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gANjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxAGNudW1weQpuZGFycmF5CnEB\\nSwCFcQJDAWJxA4dxBFJxBShLAUsBhXEGY251bXB5CmR0eXBlCnEHWAIAAABpOHEIiYiHcQlScQoo\\nSwNYAQAAADxxC05OTkr/////Sv////9LAHRxDGKJQwgBAAAAAAAAAHENdHEOYi4=\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "codecs.encode(pickle.dumps(np.array([1])), 'base64').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
