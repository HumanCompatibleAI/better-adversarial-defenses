{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym_compete_to_rllib import created_envs, env_name, create_env, env_name_rllib\n",
    "\n",
    "import os\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-17 19:50:37,780\tINFO resource_spec.py:223 -- Starting Ray with 152.1 GiB memory available for workers and up to 69.19 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-07-17 19:50:38,556\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '128.32.175.10',\n",
       " 'raylet_ip_address': '128.32.175.10',\n",
       " 'redis_address': '128.32.175.10:19203',\n",
       " 'object_store_address': '/scratch/sergei/tmp/session_2020-07-17_19-50-37_777157_55814/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/scratch/sergei/tmp/session_2020-07-17_19-50-37_777157_55814/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/scratch/sergei/tmp/session_2020-07-17_19-50-37_777157_55814'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ray_init():\n",
    "    ray.shutdown()\n",
    "    return ray.init(ignore_reinit_error=True,\n",
    "                    temp_dir='/scratch/sergei/tmp') # Skip or set to ignore if already called\n",
    "\n",
    "ray_init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 256, 'train_steps': 10, 'train_policies': []}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Using config\n",
      "{'env': 'YouShallNotPassHumans-v0_rllib', 'env_config': {'with_video': True}, 'num_workers': 0, 'train_batch_size': 256, 'multiagent': {'policies_to_train': [], 'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'use_lstm': False, 'fcnet_hiddens': [64, 64]}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1'}}, 'framework': 'tfe'})}, 'policy_mapping_fn': <function build_trainer_config.<locals>.select_policy at 0x7f5c3809a680>}, 'framework': 'tfe'}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           24384       observation_preprocessing_layer_4\n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer_ (None, 17)           0           concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer_2 multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-17 21:41:33,704\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "env_cls = create_env\n",
    "env_config = {'with_video': True}#True}\n",
    "\n",
    "def build_trainer_config(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Build configuration for 1 run.\"\"\"\n",
    "    obs_space = env_cls(env_config).observation_space\n",
    "    act_space = env_cls(env_config).action_space\n",
    "\n",
    "    policy_template = \"player_%d\"\n",
    "\n",
    "    def get_agent_config(agent_id):\n",
    "        agent_config_pretrained = (PPOTFPolicy, obs_space, act_space, {\n",
    "            'model': {\n",
    "                        \"custom_model\": \"GymCompetePretrainedModel\",\n",
    "                        \"custom_model_config\": {\n",
    "                            \"agent_id\": agent_id - 1,\n",
    "                            \"env_name\": env_name,\n",
    "                            \"model_config\": {},\n",
    "                            \"name\": \"model_%s\" % (agent_id - 1)\n",
    "                        },           \n",
    "                        \n",
    "                    },\n",
    "            \n",
    "            \"framework\": \"tfe\",\n",
    "        })\n",
    "        \n",
    "        agent_config_from_scratch = (PPOTFPolicy, obs_space, act_space, {\n",
    "                    \"model\": {\n",
    "                        \"use_lstm\": False,\n",
    "                        \"fcnet_hiddens\": [64, 64],\n",
    "                        #\"custom_action_dist\": \"DiagGaussian\",\n",
    "                    },\n",
    "                    \"framework\": \"tfe\",\n",
    "                })\n",
    "        \n",
    "        if agent_id == 1:\n",
    "            return agent_config_from_scratch\n",
    "        elif agent_id == 2:\n",
    "            return agent_config_pretrained\n",
    "        else:\n",
    "            raise KeyError(\"Wrong agent id %s\" % agent_id)\n",
    "\n",
    "    N_POLICIES = 2\n",
    "\n",
    "    policies = {policy_template % i: get_agent_config(i) for i in range(1, 1  + N_POLICIES)}\n",
    "    policies_keys = list(sorted(policies.keys()))\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        assert agent_id in [\"player_1\", \"player_2\"]\n",
    "        agent_ids = [\"player_1\", \"player_2\"]\n",
    "        \n",
    "        # selecting the corresponding policy (only for 2 policies)\n",
    "        return policies_keys[agent_ids.index(agent_id)]\n",
    "\n",
    "        # randomly choosing an opponent\n",
    "        # return np.random.choice(list(policies.keys()))\n",
    "    \n",
    "    if train_policies is None:\n",
    "        train_policies = list(policies.keys())\n",
    "        \n",
    "    for k in train_policies:\n",
    "        assert k in policies.keys()\n",
    "\n",
    "    config = {\n",
    "        \"env\": env_name_rllib,\n",
    "        \"env_config\": env_config,\n",
    "    #    \"gamma\": 0.9,\n",
    "      \"num_workers\": 0,\n",
    "    #  \"num_envs_per_worker\": 10,\n",
    "    #   \"rollout_fragment_length\": 10,\n",
    "       \"train_batch_size\": config['train_batch_size'],\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": train_policies,\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"framework\": \"tfe\",\n",
    "        #\"train_batch_size\": 512\n",
    "        #\"num_cpus_per_worker\": 2\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_trainer(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Create a RPS trainer for 2 agents, restore state, and only train specific policies.\"\"\"\n",
    "    \n",
    "    print(\"Using config\")\n",
    "    print(config)\n",
    "    cls = PPOTrainer\n",
    "    trainer = cls(config=config)\n",
    "    env = trainer.workers.local_worker().env\n",
    "    if restore_state is not None:\n",
    "        trainer.restore_from_object(restore_state)\n",
    "    return trainer\n",
    "\n",
    "stats = []\n",
    "\n",
    "def train(trainer, stop_iters, do_track=True):\n",
    "    \"\"\"Train the agents and return the state of the trainer.\"\"\"\n",
    "    for _ in range(stop_iters):\n",
    "        results = trainer.train()\n",
    "        print(pretty_print(results))\n",
    "        if do_track:\n",
    "            track.log(**results)\n",
    "        global stats\n",
    "        stats.append(results)\n",
    "    o = trainer.save_to_object()\n",
    "    return o\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# try changing learning rate\n",
    "config = {'train_batch_size': 256}\n",
    "\n",
    "config['train_steps'] = 10\n",
    "\n",
    "# ['humanoid_blocker', 'humanoid'],\n",
    "config['train_policies'] = []#'player_1']\n",
    "#config['num_workers'] = 32\n",
    "\n",
    "\n",
    "print(config)\n",
    "restore_state=None\n",
    "rl_config = build_trainer_config(restore_state=restore_state,\n",
    "                          train_policies=config['train_policies'],\n",
    "                          config=config)\n",
    "trainer = build_trainer(restore_state=None, config=rl_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json, codecs\n",
    "ck = json.load(open('/home/sergei/ray_results/./adversarial/train_one_3_2020-07-17_21-34-39m9qon2mn/checkpoint_0/checkpoint', 'r'))\n",
    "w = codecs.decode(ck['weights'].encode(), 'base64')\n",
    "trainer.set_weights(pickle.loads(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer observation_preprocessing_layer_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sergei/better-adversarial-defenses/ray/python/ray/rllib/policy/tf_policy.py:871: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-43-20\n",
      "done: false\n",
      "episode_len_mean: 152.0\n",
      "episode_reward_max: -0.3383652470135523\n",
      "episode_reward_mean: -0.48690533688773474\n",
      "episode_reward_min: -0.6354454267619172\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 2\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.999656677246094\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009587731212377548\n",
      "      policy_loss: -0.1705915331840515\n",
      "      total_loss: 7.459905624389648\n",
      "      vf_explained_var: 0.019690439105033875\n",
      "      vf_loss: 7.628579139709473\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6572747230529785\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03490670397877693\n",
      "      policy_loss: -0.43064308166503906\n",
      "      total_loss: 289547.875\n",
      "      vf_explained_var: -1.4901161193847656e-08\n",
      "      vf_loss: 289548.3125\n",
      "  num_steps_sampled: 400\n",
      "  num_steps_trained: 400\n",
      "iterations_since_restore: 1\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 34.854304635761586\n",
      "  ram_util_percent: 11.24635761589404\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -16.41731323965064\n",
      "  player_2: 16.293715177661962\n",
      "policy_reward_mean:\n",
      "  player_1: -16.67323692203726\n",
      "  player_2: 16.186331585149524\n",
      "policy_reward_min:\n",
      "  player_1: -16.929160604423878\n",
      "  player_2: 16.078947992637083\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 171.56388991491457\n",
      "  mean_inference_ms: 17.908048154112706\n",
      "  mean_processing_ms: 0.8262220463550596\n",
      "time_since_restore: 86.06222319602966\n",
      "time_this_iter_s: 86.06222319602966\n",
      "time_total_s: 86.06222319602966\n",
      "timers:\n",
      "  learn_throughput: 41.098\n",
      "  learn_time_ms: 9732.761\n",
      "  sample_throughput: 5.241\n",
      "  sample_time_ms: 76324.751\n",
      "timestamp: 1595047400\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-44-46\n",
      "done: false\n",
      "episode_len_mean: 147.4\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: 0.23397020375976788\n",
      "episode_reward_min: -0.6354454267619172\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 5\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.99884033203125\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008243048563599586\n",
      "      policy_loss: -0.15107716619968414\n",
      "      total_loss: 7.984766006469727\n",
      "      vf_explained_var: 0.004296213388442993\n",
      "      vf_loss: 8.134195327758789\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.670058012008667\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.027297019958496094\n",
      "      policy_loss: -0.27210700511932373\n",
      "      total_loss: 314019.34375\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 314019.59375\n",
      "  num_steps_sampled: 800\n",
      "  num_steps_trained: 800\n",
      "iterations_since_restore: 2\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 37.897520661157024\n",
      "  ram_util_percent: 11.400000000000004\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -14.711568007019647\n",
      "  player_2: 16.293715177661962\n",
      "policy_reward_mean:\n",
      "  player_1: -15.923255963916494\n",
      "  player_2: 16.157226167676264\n",
      "policy_reward_min:\n",
      "  player_1: -16.929160604423878\n",
      "  player_2: 15.979230703914018\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 172.46289960338225\n",
      "  mean_inference_ms: 17.765181633356956\n",
      "  mean_processing_ms: 0.8225194016129288\n",
      "time_since_restore: 172.32843685150146\n",
      "time_this_iter_s: 86.2662136554718\n",
      "time_total_s: 172.32843685150146\n",
      "timers:\n",
      "  learn_throughput: 42.407\n",
      "  learn_time_ms: 9432.483\n",
      "  sample_throughput: 5.213\n",
      "  sample_time_ms: 76727.483\n",
      "timestamp: 1595047486\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 800\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-46-14\n",
      "done: false\n",
      "episode_len_mean: 152.28571428571428\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: 0.03550381428958919\n",
      "episode_reward_min: -0.6354454267619172\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 7\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.97022819519043\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009374849498271942\n",
      "      policy_loss: -0.11781615763902664\n",
      "      total_loss: 4.8440680503845215\n",
      "      vf_explained_var: 0.010950207710266113\n",
      "      vf_loss: 4.960009574890137\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6566810607910156\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.02631378546357155\n",
      "      policy_loss: -0.2674381136894226\n",
      "      total_loss: 203663.1875\n",
      "      vf_explained_var: 4.470348358154297e-08\n",
      "      vf_loss: 203663.4375\n",
      "  num_steps_sampled: 1200\n",
      "  num_steps_trained: 1200\n",
      "iterations_since_restore: 3\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 39.57822580645162\n",
      "  ram_util_percent: 11.478225806451611\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -14.711568007019647\n",
      "  player_2: 16.3411272843522\n",
      "policy_reward_mean:\n",
      "  player_1: -16.128770689195857\n",
      "  player_2: 16.164274503485444\n",
      "policy_reward_min:\n",
      "  player_1: -16.929160604423878\n",
      "  player_2: 15.979230703914018\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 173.02726304265022\n",
      "  mean_inference_ms: 17.729138629650073\n",
      "  mean_processing_ms: 0.8196992393070752\n",
      "time_since_restore: 259.987939119339\n",
      "time_this_iter_s: 87.65950226783752\n",
      "time_total_s: 259.987939119339\n",
      "timers:\n",
      "  learn_throughput: 42.428\n",
      "  learn_time_ms: 9427.847\n",
      "  sample_throughput: 5.179\n",
      "  sample_time_ms: 77229.922\n",
      "timestamp: 1595047574\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1200\n",
      "training_iteration: 3\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-47-41\n",
      "done: false\n",
      "episode_len_mean: 153.1\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.5773715129351997\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 10\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.96613121032715\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008200100623071194\n",
      "      policy_loss: -0.18487787246704102\n",
      "      total_loss: 4.58615779876709\n",
      "      vf_explained_var: 0.014774680137634277\n",
      "      vf_loss: 4.76939582824707\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.660581111907959\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.017345130443572998\n",
      "      policy_loss: -0.10836160182952881\n",
      "      total_loss: 180444.28125\n",
      "      vf_explained_var: 1.4901161193847656e-08\n",
      "      vf_loss: 180444.390625\n",
      "  num_steps_sampled: 1600\n",
      "  num_steps_trained: 1600\n",
      "iterations_since_restore: 4\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 38.817213114754104\n",
      "  ram_util_percent: 11.5155737704918\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -6.061309052052611\n",
      "  player_2: 16.3411272843522\n",
      "policy_reward_mean:\n",
      "  player_1: -15.23743876881793\n",
      "  player_2: 13.660067255882728\n",
      "policy_reward_min:\n",
      "  player_1: -17.330246802826228\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 173.55593458948593\n",
      "  mean_inference_ms: 17.727375834283485\n",
      "  mean_processing_ms: 0.8096939889686385\n",
      "time_since_restore: 347.3342533111572\n",
      "time_this_iter_s: 87.34631419181824\n",
      "time_total_s: 347.3342533111572\n",
      "timers:\n",
      "  learn_throughput: 42.318\n",
      "  learn_time_ms: 9452.229\n",
      "  sample_throughput: 5.17\n",
      "  sample_time_ms: 77376.865\n",
      "timestamp: 1595047661\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1600\n",
      "training_iteration: 4\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-48-59\n",
      "done: false\n",
      "episode_len_mean: 155.0\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.439494340881015\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 12\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.96213150024414\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009404505603015423\n",
      "      policy_loss: -0.14755314588546753\n",
      "      total_loss: 3.558978796005249\n",
      "      vf_explained_var: 0.0027324706315994263\n",
      "      vf_loss: 3.70465087890625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.652601957321167\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01636825129389763\n",
      "      policy_loss: -0.23857149481773376\n",
      "      total_loss: 109106.5\n",
      "      vf_explained_var: -2.9802322387695312e-08\n",
      "      vf_loss: 109106.7265625\n",
      "  num_steps_sampled: 2000\n",
      "  num_steps_trained: 2000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 28.961467889908253\n",
      "  ram_util_percent: 8.231192660550457\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -6.061309052052611\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.505438037843183\n",
      "  player_2: 14.065943696962167\n",
      "policy_reward_min:\n",
      "  player_1: -17.330246802826228\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 173.20117654705004\n",
      "  mean_inference_ms: 17.64784966013266\n",
      "  mean_processing_ms: 0.8003299021580826\n",
      "time_since_restore: 424.4683210849762\n",
      "time_this_iter_s: 77.13406777381897\n",
      "time_total_s: 424.4683210849762\n",
      "timers:\n",
      "  learn_throughput: 44.097\n",
      "  learn_time_ms: 9070.879\n",
      "  sample_throughput: 5.276\n",
      "  sample_time_ms: 75818.66\n",
      "timestamp: 1595047739\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2000\n",
      "training_iteration: 5\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-50-21\n",
      "done: false\n",
      "episode_len_mean: 152.6\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.9749229397147223\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 15\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.970947265625\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008085336536169052\n",
      "      policy_loss: -0.10574042052030563\n",
      "      total_loss: 6.095095634460449\n",
      "      vf_explained_var: 0.06141003966331482\n",
      "      vf_loss: 6.199219226837158\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.649232864379883\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0186447836458683\n",
      "      policy_loss: -0.2303936779499054\n",
      "      total_loss: 65946.4375\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 65946.65625\n",
      "  num_steps_sampled: 2400\n",
      "  num_steps_trained: 2400\n",
      "iterations_since_restore: 6\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 33.82672413793104\n",
      "  ram_util_percent: 10.979310344827587\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -14.831289111257624\n",
      "  player_2: 12.856366171542902\n",
      "policy_reward_min:\n",
      "  player_1: -17.330246802826228\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 172.6732041024897\n",
      "  mean_inference_ms: 17.521459813542126\n",
      "  mean_processing_ms: 0.7898994336784765\n",
      "time_since_restore: 506.4436948299408\n",
      "time_this_iter_s: 81.9753737449646\n",
      "time_total_s: 506.4436948299408\n",
      "timers:\n",
      "  learn_throughput: 44.281\n",
      "  learn_time_ms: 9033.224\n",
      "  sample_throughput: 5.307\n",
      "  sample_time_ms: 75369.592\n",
      "timestamp: 1595047821\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2400\n",
      "training_iteration: 6\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-51-39\n",
      "done: false\n",
      "episode_len_mean: 154.44444444444446\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.7755078830009445\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 18\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.939289093017578\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00987166352570057\n",
      "      policy_loss: -0.23939475417137146\n",
      "      total_loss: 3.8266749382019043\n",
      "      vf_explained_var: 0.011059343814849854\n",
      "      vf_loss: 4.064095497131348\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.633679151535034\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.018415728583931923\n",
      "      policy_loss: -0.1773742139339447\n",
      "      total_loss: 105285.9296875\n",
      "      vf_explained_var: -7.450580596923828e-08\n",
      "      vf_loss: 105286.09375\n",
      "  num_steps_sampled: 2800\n",
      "  num_steps_trained: 2800\n",
      "iterations_since_restore: 7\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.353153153153155\n",
      "  ram_util_percent: 11.299999999999997\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.188734549141568\n",
      "  player_2: 13.413226666140627\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 171.98180472514215\n",
      "  mean_inference_ms: 17.411099994632508\n",
      "  mean_processing_ms: 0.7824945340823684\n",
      "time_since_restore: 584.5595397949219\n",
      "time_this_iter_s: 78.11584496498108\n",
      "time_total_s: 584.5595397949219\n",
      "timers:\n",
      "  learn_throughput: 44.355\n",
      "  learn_time_ms: 9018.055\n",
      "  sample_throughput: 5.37\n",
      "  sample_time_ms: 74486.239\n",
      "timestamp: 1595047899\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2800\n",
      "training_iteration: 7\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-52-57\n",
      "done: false\n",
      "episode_len_mean: 155.15\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.679943581202425\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 20\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.962997436523438\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.010265516117215157\n",
      "      policy_loss: -0.13697034120559692\n",
      "      total_loss: 2.7211174964904785\n",
      "      vf_explained_var: 0.029545217752456665\n",
      "      vf_loss: 2.856034755706787\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6312501430511475\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0172463096678257\n",
      "      policy_loss: -0.19734029471874237\n",
      "      total_loss: 32541.359375\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 32541.546875\n",
      "  num_steps_sampled: 3200\n",
      "  num_steps_trained: 3200\n",
      "iterations_since_restore: 8\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.437272727272724\n",
      "  ram_util_percent: 11.361818181818188\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.366843078630193\n",
      "  player_2: 13.686899497427765\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 171.4839889759408\n",
      "  mean_inference_ms: 17.34639999421464\n",
      "  mean_processing_ms: 0.777455374754709\n",
      "time_since_restore: 662.4604878425598\n",
      "time_this_iter_s: 77.90094804763794\n",
      "time_total_s: 662.4604878425598\n",
      "timers:\n",
      "  learn_throughput: 44.578\n",
      "  learn_time_ms: 8972.951\n",
      "  sample_throughput: 5.418\n",
      "  sample_time_ms: 73830.583\n",
      "timestamp: 1595047977\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 3200\n",
      "training_iteration: 8\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-54-16\n",
      "done: false\n",
      "episode_len_mean: 156.36363636363637\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.582332371233702\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 22\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.929397583007812\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009487688541412354\n",
      "      policy_loss: -0.17401744425296783\n",
      "      total_loss: 2.2138137817382812\n",
      "      vf_explained_var: 0.062478989362716675\n",
      "      vf_loss: 2.3859336376190186\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.626664161682129\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.021737566217780113\n",
      "      policy_loss: -0.22351984679698944\n",
      "      total_loss: 80086.203125\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 80086.4140625\n",
      "  num_steps_sampled: 3600\n",
      "  num_steps_trained: 3600\n",
      "iterations_since_restore: 9\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 30.355357142857144\n",
      "  ram_util_percent: 11.400000000000004\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.501638686661261\n",
      "  player_2: 13.919306315427555\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 170.9780379833169\n",
      "  mean_inference_ms: 17.291891740543726\n",
      "  mean_processing_ms: 0.7730398913108744\n",
      "time_since_restore: 741.2890477180481\n",
      "time_this_iter_s: 78.82855987548828\n",
      "time_total_s: 741.2890477180481\n",
      "timers:\n",
      "  learn_throughput: 44.566\n",
      "  learn_time_ms: 8975.384\n",
      "  sample_throughput: 5.451\n",
      "  sample_time_ms: 73385.763\n",
      "timestamp: 1595048056\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 3600\n",
      "training_iteration: 9\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_21-55-36\n",
      "done: false\n",
      "episode_len_mean: 156.32\n",
      "episode_reward_max: 1.2676626968943712\n",
      "episode_reward_mean: -1.4059728346553628\n",
      "episode_reward_min: -15.160162731907818\n",
      "episodes_this_iter: 3\n",
      "episodes_total: 25\n",
      "experiment_id: c6ee96fcb50744afb96244184244c6bf\n",
      "hostname: svm\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 23.951248168945312\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.010141189210116863\n",
      "      policy_loss: -0.09169439971446991\n",
      "      total_loss: 2.4856820106506348\n",
      "      vf_explained_var: 0.10534541308879852\n",
      "      vf_loss: 2.575348138809204\n",
      "    player_2:\n",
      "      cur_kl_coeff: 1.0125000476837158\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6213395595550537\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.014543559402227402\n",
      "      policy_loss: -0.1882309913635254\n",
      "      total_loss: 28095.765625\n",
      "      vf_explained_var: -4.470348358154297e-08\n",
      "      vf_loss: 28095.9375\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 128.32.175.10\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 29.66283185840708\n",
      "  ram_util_percent: 11.400000000000004\n",
      "pid: 55814\n",
      "policy_reward_max:\n",
      "  player_1: -3.6979001004616454\n",
      "  player_2: 16.57021067208011\n",
      "policy_reward_mean:\n",
      "  player_1: -15.589867557575491\n",
      "  player_2: 14.183894722920126\n",
      "policy_reward_min:\n",
      "  player_1: -18.592151099811403\n",
      "  player_2: -9.09885367985521\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 170.30594774297373\n",
      "  mean_inference_ms: 17.21855754244262\n",
      "  mean_processing_ms: 0.767352779800888\n",
      "time_since_restore: 821.1280779838562\n",
      "time_this_iter_s: 79.8390302658081\n",
      "time_total_s: 821.1280779838562\n",
      "timers:\n",
      "  learn_throughput: 44.597\n",
      "  learn_time_ms: 8969.301\n",
      "  sample_throughput: 5.469\n",
      "  sample_time_ms: 73139.335\n",
      "timestamp: 1595048136\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 10\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sergei/ray_results/PPO_YouShallNotPassHumans-v0_rllib_2020-07-17_21-41-32btsd_9s9/tmpoccvyew6save_to_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0e323b12d4a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-02b3e772c635>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainer, stop_iters, do_track)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sergei/better-adversarial-defenses/ray/python/ray/tune/trainable.py\u001b[0m in \u001b[0;36msave_to_object\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mObject\u001b[0m \u001b[0mholding\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mtmpdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"save_to_object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# Save all files in subtree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/sergei/miniconda3/lib/python3.7/tempfile.py\u001b[0m in \u001b[0;36mmkdtemp\u001b[0;34m(suffix, prefix, dir)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0o700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;32mcontinue\u001b[0m    \u001b[0;31m# try again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sergei/ray_results/PPO_YouShallNotPassHumans-v0_rllib_2020-07-17_21-41-32btsd_9s9/tmpoccvyew6save_to_object'"
     ]
    }
   ],
   "source": [
    "train(trainer, config['train_steps'], do_track=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array([y for x in stats for y in x['hist_stats']['policy_player_1_reward']]).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.close() for x in created_envs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gym_compete_policy import get_policy_value_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env_cls(env_config).reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = get_policy_value_nets(env_name, 0)\n",
    "policy_net_orig = nets['policy_mean_logstd_flat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainer.get_policy('player_1').model\n",
    "m.policy_net(obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets['policy_mean_logstd'](obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logstd = tf.split(policy_net_orig(obs['player_1'].reshape(1, -1)), 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.clip(np.random.normal(loc=mean[0], scale=np.exp(logstd[0]), size=(17,)), -0.4, 0.4) for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compute_action(obs['player_1'], policy_id='player_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cls(env_config).action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gANjbnVtcHkuY29yZS5tdWx0aWFycmF5Cl9yZWNvbnN0cnVjdApxAGNudW1weQpuZGFycmF5CnEB\\nSwCFcQJDAWJxA4dxBFJxBShLAUsBhXEGY251bXB5CmR0eXBlCnEHWAIAAABpOHEIiYiHcQlScQoo\\nSwNYAQAAADxxC05OTkr/////Sv////9LAHRxDGKJQwgBAAAAAAAAAHENdHEOYi4=\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "codecs.encode(pickle.dumps(np.array([1])), 'base64').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
