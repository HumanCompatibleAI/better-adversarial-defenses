{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym_compete_to_rllib import created_envs, env_name, create_env, env_name_rllib\n",
    "\n",
    "import os\n",
    "os.environ['DISPLAY'] = ':0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CLD_CONTINUED',\n",
       " 'CLD_DUMPED',\n",
       " 'CLD_EXITED',\n",
       " 'CLD_TRAPPED',\n",
       " 'DirEntry',\n",
       " 'EX_CANTCREAT',\n",
       " 'EX_CONFIG',\n",
       " 'EX_DATAERR',\n",
       " 'EX_IOERR',\n",
       " 'EX_NOHOST',\n",
       " 'EX_NOINPUT',\n",
       " 'EX_NOPERM',\n",
       " 'EX_NOUSER',\n",
       " 'EX_OK',\n",
       " 'EX_OSERR',\n",
       " 'EX_OSFILE',\n",
       " 'EX_PROTOCOL',\n",
       " 'EX_SOFTWARE',\n",
       " 'EX_TEMPFAIL',\n",
       " 'EX_UNAVAILABLE',\n",
       " 'EX_USAGE',\n",
       " 'F_LOCK',\n",
       " 'F_OK',\n",
       " 'F_TEST',\n",
       " 'F_TLOCK',\n",
       " 'F_ULOCK',\n",
       " 'MutableMapping',\n",
       " 'NGROUPS_MAX',\n",
       " 'O_ACCMODE',\n",
       " 'O_APPEND',\n",
       " 'O_ASYNC',\n",
       " 'O_CLOEXEC',\n",
       " 'O_CREAT',\n",
       " 'O_DIRECT',\n",
       " 'O_DIRECTORY',\n",
       " 'O_DSYNC',\n",
       " 'O_EXCL',\n",
       " 'O_LARGEFILE',\n",
       " 'O_NDELAY',\n",
       " 'O_NOATIME',\n",
       " 'O_NOCTTY',\n",
       " 'O_NOFOLLOW',\n",
       " 'O_NONBLOCK',\n",
       " 'O_RDONLY',\n",
       " 'O_RDWR',\n",
       " 'O_RSYNC',\n",
       " 'O_SYNC',\n",
       " 'O_TRUNC',\n",
       " 'O_WRONLY',\n",
       " 'POSIX_FADV_DONTNEED',\n",
       " 'POSIX_FADV_NOREUSE',\n",
       " 'POSIX_FADV_NORMAL',\n",
       " 'POSIX_FADV_RANDOM',\n",
       " 'POSIX_FADV_SEQUENTIAL',\n",
       " 'POSIX_FADV_WILLNEED',\n",
       " 'PRIO_PGRP',\n",
       " 'PRIO_PROCESS',\n",
       " 'PRIO_USER',\n",
       " 'P_ALL',\n",
       " 'P_NOWAIT',\n",
       " 'P_NOWAITO',\n",
       " 'P_PGID',\n",
       " 'P_PID',\n",
       " 'P_WAIT',\n",
       " 'PathLike',\n",
       " 'RTLD_DEEPBIND',\n",
       " 'RTLD_GLOBAL',\n",
       " 'RTLD_LAZY',\n",
       " 'RTLD_LOCAL',\n",
       " 'RTLD_NODELETE',\n",
       " 'RTLD_NOLOAD',\n",
       " 'RTLD_NOW',\n",
       " 'R_OK',\n",
       " 'SCHED_BATCH',\n",
       " 'SCHED_FIFO',\n",
       " 'SCHED_IDLE',\n",
       " 'SCHED_OTHER',\n",
       " 'SCHED_RESET_ON_FORK',\n",
       " 'SCHED_RR',\n",
       " 'SEEK_CUR',\n",
       " 'SEEK_END',\n",
       " 'SEEK_SET',\n",
       " 'ST_APPEND',\n",
       " 'ST_MANDLOCK',\n",
       " 'ST_NOATIME',\n",
       " 'ST_NODEV',\n",
       " 'ST_NODIRATIME',\n",
       " 'ST_NOEXEC',\n",
       " 'ST_NOSUID',\n",
       " 'ST_RDONLY',\n",
       " 'ST_RELATIME',\n",
       " 'ST_SYNCHRONOUS',\n",
       " 'ST_WRITE',\n",
       " 'TMP_MAX',\n",
       " 'WCONTINUED',\n",
       " 'WCOREDUMP',\n",
       " 'WEXITED',\n",
       " 'WEXITSTATUS',\n",
       " 'WIFCONTINUED',\n",
       " 'WIFEXITED',\n",
       " 'WIFSIGNALED',\n",
       " 'WIFSTOPPED',\n",
       " 'WNOHANG',\n",
       " 'WNOWAIT',\n",
       " 'WSTOPPED',\n",
       " 'WSTOPSIG',\n",
       " 'WTERMSIG',\n",
       " 'WUNTRACED',\n",
       " 'W_OK',\n",
       " 'XATTR_CREATE',\n",
       " 'XATTR_REPLACE',\n",
       " 'XATTR_SIZE_MAX',\n",
       " 'X_OK',\n",
       " '_Environ',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_check_methods',\n",
       " '_execvpe',\n",
       " '_exists',\n",
       " '_exit',\n",
       " '_fspath',\n",
       " '_fwalk',\n",
       " '_get_exports_list',\n",
       " '_putenv',\n",
       " '_spawnvef',\n",
       " '_unsetenv',\n",
       " '_wrap_close',\n",
       " 'abc',\n",
       " 'abort',\n",
       " 'access',\n",
       " 'altsep',\n",
       " 'chdir',\n",
       " 'chmod',\n",
       " 'chown',\n",
       " 'chroot',\n",
       " 'close',\n",
       " 'closerange',\n",
       " 'confstr',\n",
       " 'confstr_names',\n",
       " 'cpu_count',\n",
       " 'ctermid',\n",
       " 'curdir',\n",
       " 'defpath',\n",
       " 'device_encoding',\n",
       " 'devnull',\n",
       " 'dup',\n",
       " 'dup2',\n",
       " 'environ',\n",
       " 'environb',\n",
       " 'error',\n",
       " 'execl',\n",
       " 'execle',\n",
       " 'execlp',\n",
       " 'execlpe',\n",
       " 'execv',\n",
       " 'execve',\n",
       " 'execvp',\n",
       " 'execvpe',\n",
       " 'extsep',\n",
       " 'fchdir',\n",
       " 'fchmod',\n",
       " 'fchown',\n",
       " 'fdatasync',\n",
       " 'fdopen',\n",
       " 'fork',\n",
       " 'forkpty',\n",
       " 'fpathconf',\n",
       " 'fsdecode',\n",
       " 'fsencode',\n",
       " 'fspath',\n",
       " 'fstat',\n",
       " 'fstatvfs',\n",
       " 'fsync',\n",
       " 'ftruncate',\n",
       " 'fwalk',\n",
       " 'get_blocking',\n",
       " 'get_exec_path',\n",
       " 'get_inheritable',\n",
       " 'get_terminal_size',\n",
       " 'getcwd',\n",
       " 'getcwdb',\n",
       " 'getegid',\n",
       " 'getenv',\n",
       " 'getenvb',\n",
       " 'geteuid',\n",
       " 'getgid',\n",
       " 'getgrouplist',\n",
       " 'getgroups',\n",
       " 'getloadavg',\n",
       " 'getlogin',\n",
       " 'getpgid',\n",
       " 'getpgrp',\n",
       " 'getpid',\n",
       " 'getppid',\n",
       " 'getpriority',\n",
       " 'getresgid',\n",
       " 'getresuid',\n",
       " 'getsid',\n",
       " 'getuid',\n",
       " 'getxattr',\n",
       " 'initgroups',\n",
       " 'isatty',\n",
       " 'kill',\n",
       " 'killpg',\n",
       " 'lchown',\n",
       " 'linesep',\n",
       " 'link',\n",
       " 'listdir',\n",
       " 'listxattr',\n",
       " 'lockf',\n",
       " 'lseek',\n",
       " 'lstat',\n",
       " 'major',\n",
       " 'makedev',\n",
       " 'makedirs',\n",
       " 'minor',\n",
       " 'mkdir',\n",
       " 'mkfifo',\n",
       " 'mknod',\n",
       " 'name',\n",
       " 'nice',\n",
       " 'open',\n",
       " 'openpty',\n",
       " 'pardir',\n",
       " 'path',\n",
       " 'pathconf',\n",
       " 'pathconf_names',\n",
       " 'pathsep',\n",
       " 'pipe',\n",
       " 'pipe2',\n",
       " 'popen',\n",
       " 'posix_fadvise',\n",
       " 'posix_fallocate',\n",
       " 'pread',\n",
       " 'preadv',\n",
       " 'putenv',\n",
       " 'pwrite',\n",
       " 'pwritev',\n",
       " 'read',\n",
       " 'readlink',\n",
       " 'readv',\n",
       " 'register_at_fork',\n",
       " 'remove',\n",
       " 'removedirs',\n",
       " 'removexattr',\n",
       " 'rename',\n",
       " 'renames',\n",
       " 'replace',\n",
       " 'rmdir',\n",
       " 'scandir',\n",
       " 'sched_get_priority_max',\n",
       " 'sched_get_priority_min',\n",
       " 'sched_getaffinity',\n",
       " 'sched_getparam',\n",
       " 'sched_getscheduler',\n",
       " 'sched_param',\n",
       " 'sched_rr_get_interval',\n",
       " 'sched_setaffinity',\n",
       " 'sched_setparam',\n",
       " 'sched_setscheduler',\n",
       " 'sched_yield',\n",
       " 'sendfile',\n",
       " 'sep',\n",
       " 'set_blocking',\n",
       " 'set_inheritable',\n",
       " 'setegid',\n",
       " 'seteuid',\n",
       " 'setgid',\n",
       " 'setgroups',\n",
       " 'setpgid',\n",
       " 'setpgrp',\n",
       " 'setpriority',\n",
       " 'setregid',\n",
       " 'setresgid',\n",
       " 'setresuid',\n",
       " 'setreuid',\n",
       " 'setsid',\n",
       " 'setuid',\n",
       " 'setxattr',\n",
       " 'spawnl',\n",
       " 'spawnle',\n",
       " 'spawnlp',\n",
       " 'spawnlpe',\n",
       " 'spawnv',\n",
       " 'spawnve',\n",
       " 'spawnvp',\n",
       " 'spawnvpe',\n",
       " 'st',\n",
       " 'stat',\n",
       " 'stat_result',\n",
       " 'statvfs',\n",
       " 'statvfs_result',\n",
       " 'strerror',\n",
       " 'supports_bytes_environ',\n",
       " 'supports_dir_fd',\n",
       " 'supports_effective_ids',\n",
       " 'supports_fd',\n",
       " 'supports_follow_symlinks',\n",
       " 'symlink',\n",
       " 'sync',\n",
       " 'sys',\n",
       " 'sysconf',\n",
       " 'sysconf_names',\n",
       " 'system',\n",
       " 'tcgetpgrp',\n",
       " 'tcsetpgrp',\n",
       " 'terminal_size',\n",
       " 'times',\n",
       " 'times_result',\n",
       " 'truncate',\n",
       " 'ttyname',\n",
       " 'umask',\n",
       " 'uname',\n",
       " 'uname_result',\n",
       " 'unlink',\n",
       " 'unsetenv',\n",
       " 'urandom',\n",
       " 'utime',\n",
       " 'wait',\n",
       " 'wait3',\n",
       " 'wait4',\n",
       " 'waitid',\n",
       " 'waitid_result',\n",
       " 'waitpid',\n",
       " 'walk',\n",
       " 'write',\n",
       " 'writev']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dir(os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cls = create_env\n",
    "env_config = {'with_video': True}\n",
    "\n",
    "def build_trainer_config(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Build configuration for 1 run.\"\"\"\n",
    "    obs_space = env_cls(env_config).observation_space\n",
    "    act_space = env_cls(env_config).action_space\n",
    "\n",
    "    policy_template = \"player_%d\"\n",
    "\n",
    "    def get_agent_config(agent_id):\n",
    "        agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "            'model': {\n",
    "                        \"custom_model\": \"GymCompetePretrainedModel\",\n",
    "                        \"custom_model_config\": {\n",
    "                            \"agent_id\": agent_id - 1,\n",
    "                            \"env_name\": env_name,\n",
    "                            \"model_config\": {},\n",
    "                            \"name\": \"model_%s\" % (agent_id - 1)\n",
    "                        },           \n",
    "                        \n",
    "                    },\n",
    "            \n",
    "            \"framework\": \"tfe\",\n",
    "        })\n",
    "        \n",
    "        return agent_config\n",
    "        \n",
    "#         agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "#                     \"model\": {\n",
    "#                         \"use_lstm\": False,\n",
    "#                         \"fcnet_hiddens\": [64, 64],\n",
    "#                         #\"custom_action_dist\": \"DiagGaussian\",\n",
    "#                     },\n",
    "#                     \"framework\": \"tfe\",\n",
    "#                 })\n",
    "        \n",
    "#         return agent_config\n",
    "\n",
    "    N_POLICIES = 2\n",
    "\n",
    "    policies = {policy_template % i: get_agent_config(i) for i in range(1, 1  + N_POLICIES)}\n",
    "    policies_keys = list(sorted(policies.keys()))\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        assert agent_id in [\"player_1\", \"player_2\"]\n",
    "        agent_ids = [\"player_1\", \"player_2\"]\n",
    "        \n",
    "        # selecting the corresponding policy (only for 2 policies)\n",
    "        return policies_keys[agent_ids.index(agent_id)]\n",
    "\n",
    "        # randomly choosing an opponent\n",
    "        # return np.random.choice(list(policies.keys()))\n",
    "    \n",
    "    if train_policies is None:\n",
    "        train_policies = list(policies.keys())\n",
    "        \n",
    "    for k in train_policies:\n",
    "        assert k in policies.keys()\n",
    "\n",
    "    config = {\n",
    "        \"env\": env_name_rllib,\n",
    "        \"env_config\": env_config,\n",
    "    #    \"gamma\": 0.9,\n",
    "      \"num_workers\": 0,\n",
    "    #  \"num_envs_per_worker\": 10,\n",
    "    #   \"rollout_fragment_length\": 10,\n",
    "       \"train_batch_size\": config['train_batch_size'],\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": train_policies,\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"framework\": \"tfe\",\n",
    "        #\"train_batch_size\": 512\n",
    "        #\"num_cpus_per_worker\": 2\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_trainer(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Create a RPS trainer for 2 agents, restore state, and only train specific policies.\"\"\"\n",
    "    \n",
    "    print(\"Using config\")\n",
    "    print(config)\n",
    "    cls = PPOTrainer\n",
    "    trainer = cls(config=config)\n",
    "    env = trainer.workers.local_worker().env\n",
    "    if restore_state is not None:\n",
    "        trainer.restore_from_object(restore_state)\n",
    "    return trainer\n",
    "\n",
    "def train(trainer, stop_iters, do_track=True):\n",
    "    \"\"\"Train the agents and return the state of the trainer.\"\"\"\n",
    "    for _ in range(stop_iters):\n",
    "        results = trainer.train()\n",
    "        print(pretty_print(results))\n",
    "        if do_track:\n",
    "            track.log(**results)\n",
    "    o = trainer.save_to_object()\n",
    "    return o\n",
    "\n",
    "trainer = None\n",
    "\n",
    "def train_one(config, restore_state=None, do_track=True):\n",
    "    print(config)\n",
    "    rl_config = build_trainer_config(restore_state=restore_state,\n",
    "                              train_policies=config['train_policies'],\n",
    "                              config=config)\n",
    "    global trainer\n",
    "    trainer = build_trainer(restore_state=None, config=rl_config)\n",
    "    train(trainer, config['train_steps'], do_track=do_track)\n",
    "\n",
    "\n",
    "# try changing learning rate\n",
    "config = {'train_batch_size': 128}\n",
    "\n",
    "config['train_steps'] = 10\n",
    "config['train_policies'] = [] #['player_1', 'player_2']\n",
    "config['num_workers'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 128, 'train_steps': 10, 'train_policies': [], 'num_workers': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLFW error: 65544, desc: b'X11: RandR gamma ramp support seems broken'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "{'env_name': 'multicomp/SumoAnts-v0', 'agent_a_type': 'zoo', 'agent_a_path': '1', 'agent_b_type': 'zoo', 'agent_b_path': '2', 'record_traj': False, 'record_traj_params': {'save_dir': 'data/trajectories', 'agent_indices': None}, 'transparent_params': None, 'num_env': 1, 'episodes': 20, 'timesteps': None, 'render': True, 'videos': False, 'video_params': {'save_dir': None, 'single_file': True, 'annotated': True, 'annotation_params': {'camera_config': 'default', 'short_labels': False, 'resolution': [1920, 1080], 'font_size': 70, 'font': 'times'}}, 'index_keys': [], 'noisy_agent_index': None, 'noisy_agent_magnitude': 1.0, 'mask_agent_index': None, 'mask_agent_masking_type': 'initialization', 'mask_agent_noise': None, 'seed': 0}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "{'env_name': 'multicomp/SumoAnts-v0', 'agent_a_type': 'zoo', 'agent_a_path': '1', 'agent_b_type': 'zoo', 'agent_b_path': '2', 'record_traj': False, 'record_traj_params': {'save_dir': 'data/trajectories', 'agent_indices': None}, 'transparent_params': None, 'num_env': 1, 'episodes': 20, 'timesteps': None, 'render': True, 'videos': False, 'video_params': {'save_dir': None, 'single_file': True, 'annotated': True, 'annotation_params': {'camera_config': 'default', 'short_labels': False, 'resolution': [1920, 1080], 'font_size': 70, 'font': 'times'}}, 'index_keys': [], 'noisy_agent_index': None, 'noisy_agent_magnitude': 1.0, 'mask_agent_index': None, 'mask_agent_masking_type': 'initialization', 'mask_agent_noise': None, 'seed': 0}\n",
      "Using config\n",
      "{'env': 'YouShallNotPassHumans-v0_rllib', 'env_config': {'with_video': True}, 'num_workers': 0, 'train_batch_size': 128, 'multiagent': {'policies_to_train': [], 'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 0, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_0'}}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1'}}, 'framework': 'tfe'})}, 'policy_mapping_fn': <function build_trainer_config.<locals>.select_policy at 0x7f4acad37200>}, 'framework': 'tfe'}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "{'env_name': 'multicomp/SumoAnts-v0', 'agent_a_type': 'zoo', 'agent_a_path': '1', 'agent_b_type': 'zoo', 'agent_b_path': '2', 'record_traj': False, 'record_traj_params': {'save_dir': 'data/trajectories', 'agent_indices': None}, 'transparent_params': None, 'num_env': 1, 'episodes': 20, 'timesteps': None, 'render': True, 'videos': False, 'video_params': {'save_dir': None, 'single_file': True, 'annotated': True, 'annotation_params': {'camera_config': 'default', 'short_labels': False, 'resolution': [1920, 1080], 'font_size': 70, 'font': 'times'}}, 'index_keys': [], 'noisy_agent_index': None, 'noisy_agent_magnitude': 1.0, 'mask_agent_index': None, 'mask_agent_masking_type': 'initialization', 'mask_agent_noise': None, 'seed': 0}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           24384       observation_preprocessing_layer[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer  (None, 17)           0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer ( multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /scratch/sergei/better-adversarial-defenses/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           24384       observation_preprocessing_layer_2\n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer_ (None, 17)           0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer_1 multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer observation_preprocessing_layer_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer observation_preprocessing_layer_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /scratch/sergei/better-adversarial-defenses/ray/python/ray/rllib/policy/tf_policy.py:871: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_15-19-00\n",
      "done: false\n",
      "episode_len_mean: 123.0\n",
      "episode_reward_max: 482.60556004739516\n",
      "episode_reward_mean: 482.60556004739516\n",
      "episode_reward_min: 482.60556004739516\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: 12ce72ed4f034a1fbc2183f132f1f8ab\n",
      "hostname: astar\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.979798316955566\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.036086469888687134\n",
      "      policy_loss: -0.33002710342407227\n",
      "      total_loss: 138874.6875\n",
      "      vf_explained_var: 5.960464477539063e-08\n",
      "      vf_loss: 138875.0\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6644766330718994\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.035298317670822144\n",
      "      policy_loss: -0.2872884273529053\n",
      "      total_loss: 98108.296875\n",
      "      vf_explained_var: 5.960464477539063e-08\n",
      "      vf_loss: 98108.578125\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "iterations_since_restore: 1\n",
      "node_ip: 128.32.175.9\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.52941176470588\n",
      "  ram_util_percent: 59.16941176470588\n",
      "pid: 5010\n",
      "policy_reward_max:\n",
      "  player_1: 313.63788415185405\n",
      "  player_2: 168.96767589554102\n",
      "policy_reward_mean:\n",
      "  player_1: 313.63788415185405\n",
      "  player_2: 168.96767589554102\n",
      "policy_reward_min:\n",
      "  player_1: 313.63788415185405\n",
      "  player_2: 168.96767589554102\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 451.79227335536063\n",
      "  mean_inference_ms: 75.80793319056876\n",
      "  mean_processing_ms: 2.48019730866845\n",
      "time_since_restore: 121.3795998096466\n",
      "time_this_iter_s: 121.3795998096466\n",
      "time_total_s: 121.3795998096466\n",
      "timers:\n",
      "  learn_throughput: 13.512\n",
      "  learn_time_ms: 14801.594\n",
      "  sample_throughput: 1.877\n",
      "  sample_time_ms: 106570.509\n",
      "timestamp: 1595024340\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_15-21-02\n",
      "done: false\n",
      "episode_len_mean: 124.0\n",
      "episode_reward_max: 535.237972489519\n",
      "episode_reward_mean: 362.5667375493272\n",
      "episode_reward_min: 69.85668011106742\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 3\n",
      "experiment_id: 12ce72ed4f034a1fbc2183f132f1f8ab\n",
      "hostname: astar\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.982230186462402\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03506788611412048\n",
      "      policy_loss: -0.2768567204475403\n",
      "      total_loss: 132686.6875\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 132686.96875\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6613426208496094\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03621017932891846\n",
      "      policy_loss: -0.26115816831588745\n",
      "      total_loss: 99543.5\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 99543.75\n",
      "  num_steps_sampled: 400\n",
      "  num_steps_trained: 400\n",
      "iterations_since_restore: 2\n",
      "node_ip: 128.32.175.9\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.54294117647058\n",
      "  ram_util_percent: 59.49294117647058\n",
      "pid: 5010\n",
      "policy_reward_max:\n",
      "  player_1: 313.63788415185405\n",
      "  player_2: 275.12232335484003\n",
      "policy_reward_mean:\n",
      "  player_1: 241.67753913665607\n",
      "  player_2: 120.88919841267113\n",
      "policy_reward_min:\n",
      "  player_1: 151.27908412343484\n",
      "  player_2: -81.42240401236764\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 456.7986997009605\n",
      "  mean_inference_ms: 73.2111959549727\n",
      "  mean_processing_ms: 2.63778295273346\n",
      "time_since_restore: 243.63214683532715\n",
      "time_this_iter_s: 122.25254702568054\n",
      "time_total_s: 243.63214683532715\n",
      "timers:\n",
      "  learn_throughput: 13.57\n",
      "  learn_time_ms: 14738.061\n",
      "  sample_throughput: 1.868\n",
      "  sample_time_ms: 107073.131\n",
      "timestamp: 1595024462\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_15-23-06\n",
      "done: false\n",
      "episode_len_mean: 119.75\n",
      "episode_reward_max: 535.237972489519\n",
      "episode_reward_mean: 288.90696633370203\n",
      "episode_reward_min: 67.92765268682658\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 4\n",
      "experiment_id: 12ce72ed4f034a1fbc2183f132f1f8ab\n",
      "hostname: astar\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.983866214752197\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.023955583572387695\n",
      "      policy_loss: -0.2426968514919281\n",
      "      total_loss: 49189.1171875\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 49189.3515625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.652766227722168\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0248158760368824\n",
      "      policy_loss: -0.2656852602958679\n",
      "      total_loss: 62719.5859375\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 62719.83984375\n",
      "  num_steps_sampled: 600\n",
      "  num_steps_trained: 600\n",
      "iterations_since_restore: 3\n",
      "node_ip: 128.32.175.9\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 98.5046511627907\n",
      "  ram_util_percent: 59.55058139534884\n",
      "pid: 5010\n",
      "policy_reward_max:\n",
      "  player_1: 313.63788415185405\n",
      "  player_2: 275.12232335484003\n",
      "policy_reward_mean:\n",
      "  player_1: 252.76933187546385\n",
      "  player_2: 36.13763445823818\n",
      "policy_reward_min:\n",
      "  player_1: 151.27908412343484\n",
      "  player_2: -218.1170574050607\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 457.95863548164357\n",
      "  mean_inference_ms: 73.19092807137257\n",
      "  mean_processing_ms: 2.6167855735206484\n",
      "time_since_restore: 367.0895564556122\n",
      "time_this_iter_s: 123.45740962028503\n",
      "time_total_s: 367.0895564556122\n",
      "timers:\n",
      "  learn_throughput: 13.569\n",
      "  learn_time_ms: 14739.072\n",
      "  sample_throughput: 1.858\n",
      "  sample_time_ms: 107619.699\n",
      "timestamp: 1595024586\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600\n",
      "training_iteration: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_one(config, do_track=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gym_compete_policy import get_policy_value_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env_cls(env_config).reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(acts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = get_policy_value_nets(env_name, 0)\n",
    "policy_net_orig = nets['policy_mean_logstd_flat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainer.get_policy('player_1').model\n",
    "m.policy_net(obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets['policy_mean_logstd'](obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logstd = tf.split(policy_net_orig(obs['player_1'].reshape(1, -1)), 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([np.clip(np.random.normal(loc=mean[0], scale=np.exp(logstd[0]), size=(17,)), -0.4, 0.4) for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.compute_action(obs['player_1'], policy_id='player_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cls(env_config).action_space.high"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
