{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym_compete_to_rllib import created_envs, env_name, create_env, env_name_rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cls = create_env\n",
    "env_config = {'with_video': True}\n",
    "\n",
    "def build_trainer_config(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Build configuration for 1 run.\"\"\"\n",
    "    obs_space = env_cls(env_config).observation_space\n",
    "    act_space = env_cls(env_config).action_space\n",
    "\n",
    "    policy_template = \"player_%d\"\n",
    "\n",
    "    def get_agent_config(agent_id):\n",
    "        agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "            'model': {\n",
    "                        \"custom_model\": \"GymCompetePretrainedModel\",\n",
    "                        \"custom_model_config\": {\n",
    "                            \"agent_id\": agent_id - 1,\n",
    "                            \"env_name\": env_name,\n",
    "                            \"model_config\": {},\n",
    "                            \"name\": \"model_%s\" % (agent_id - 1)\n",
    "                        },           \n",
    "                        \n",
    "                    },\n",
    "            \n",
    "            \"framework\": \"tfe\",\n",
    "        })\n",
    "        \n",
    "        return agent_config\n",
    "        \n",
    "#         agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "#                     \"model\": {\n",
    "#                         \"use_lstm\": False,\n",
    "#                         \"fcnet_hiddens\": [64, 64],\n",
    "#                         #\"custom_action_dist\": \"DiagGaussian\",\n",
    "#                     },\n",
    "#                     \"framework\": \"tfe\",\n",
    "#                 })\n",
    "        \n",
    "#         return agent_config\n",
    "\n",
    "    N_POLICIES = 2\n",
    "\n",
    "    policies = {policy_template % i: get_agent_config(i) for i in range(1, 1  + N_POLICIES)}\n",
    "    policies_keys = list(sorted(policies.keys()))\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        assert agent_id in [\"player_1\", \"player_2\"]\n",
    "        agent_ids = [\"player_1\", \"player_2\"]\n",
    "        \n",
    "        # selecting the corresponding policy (only for 2 policies)\n",
    "        return policies_keys[agent_ids.index(agent_id)]\n",
    "\n",
    "        # randomly choosing an opponent\n",
    "        # return np.random.choice(list(policies.keys()))\n",
    "    \n",
    "    if train_policies is None:\n",
    "        train_policies = list(policies.keys())\n",
    "        \n",
    "    for k in train_policies:\n",
    "        assert k in policies.keys()\n",
    "\n",
    "    config = {\n",
    "        \"env\": env_name_rllib,\n",
    "        \"env_config\": env_config,\n",
    "    #    \"gamma\": 0.9,\n",
    "      \"num_workers\": 0,\n",
    "    #  \"num_envs_per_worker\": 10,\n",
    "    #   \"rollout_fragment_length\": 10,\n",
    "       \"train_batch_size\": config['train_batch_size'],\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": train_policies,\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"framework\": \"tfe\",\n",
    "        #\"train_batch_size\": 512\n",
    "        #\"num_cpus_per_worker\": 2\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_trainer(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Create a RPS trainer for 2 agents, restore state, and only train specific policies.\"\"\"\n",
    "    \n",
    "    print(\"Using config\")\n",
    "    print(config)\n",
    "    cls = PPOTrainer\n",
    "    trainer = cls(config=config)\n",
    "    env = trainer.workers.local_worker().env\n",
    "    if restore_state is not None:\n",
    "        trainer.restore_from_object(restore_state)\n",
    "    return trainer\n",
    "\n",
    "def train(trainer, stop_iters, do_track=True):\n",
    "    \"\"\"Train the agents and return the state of the trainer.\"\"\"\n",
    "    for _ in range(stop_iters):\n",
    "        results = trainer.train()\n",
    "        print(pretty_print(results))\n",
    "        if do_track:\n",
    "            track.log(**results)\n",
    "    o = trainer.save_to_object()\n",
    "    return o\n",
    "\n",
    "trainer = None\n",
    "\n",
    "def train_one(config, restore_state=None, do_track=True):\n",
    "    print(config)\n",
    "    rl_config = build_trainer_config(restore_state=restore_state,\n",
    "                              train_policies=config['train_policies'],\n",
    "                              config=config)\n",
    "    global trainer\n",
    "    trainer = build_trainer(restore_state=None, config=rl_config)\n",
    "    train(trainer, config['train_steps'], do_track=do_track)\n",
    "\n",
    "\n",
    "# try changing learning rate\n",
    "config = {'train_batch_size': 128}\n",
    "\n",
    "config['train_steps'] = 10\n",
    "config['train_policies'] = [] #['player_1', 'player_2']\n",
    "config['num_workers'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 128, 'train_steps': 10, 'train_policies': [], 'num_workers': 3}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip install 'ray[tune]' to see TensorBoard files.\n",
      "Could not instantiate TBXLogger: No module named 'tensorboardX'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config\n",
      "{'env': 'YouShallNotPassHumans-v0_rllib', 'env_config': {'with_video': True}, 'num_workers': 0, 'train_batch_size': 128, 'multiagent': {'policies_to_train': [], 'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 0, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_0'}}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1'}}, 'framework': 'tfe'})}, 'policy_mapping_fn': <function build_trainer_config.<locals>.select_policy at 0x7f8de9831e50>}, 'framework': 'tfe'}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           24384       observation_preprocessing_layer[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer  (None, 17)           0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer ( multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           24384       observation_preprocessing_layer_2\n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer_ (None, 17)           0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer_1 multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer observation_preprocessing_layer_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer observation_preprocessing_layer_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergei/git/chai/ray/python/ray/rllib/policy/tf_policy.py:869: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-48-16\n",
      "done: false\n",
      "episode_len_mean: 109.0\n",
      "episode_reward_max: 317.8782422193219\n",
      "episode_reward_mean: 317.8782422193219\n",
      "episode_reward_min: 317.8782422193219\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 1\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.974283218383789\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0320662260055542\n",
      "      policy_loss: -0.31707143783569336\n",
      "      total_loss: 64708.31640625\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 64708.625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6698226928710938\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03327154368162155\n",
      "      policy_loss: -0.26614660024642944\n",
      "      total_loss: 71085.625\n",
      "      vf_explained_var: 2.9802322387695312e-08\n",
      "      vf_loss: 71085.8828125\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.619565217391308\n",
      "  ram_util_percent: 23.410869565217393\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 213.5627206162485\n",
      "  player_2: 104.31552160307315\n",
      "policy_reward_mean:\n",
      "  player_1: 213.5627206162485\n",
      "  player_2: 104.31552160307315\n",
      "policy_reward_min:\n",
      "  player_1: 213.5627206162485\n",
      "  player_2: 104.31552160307315\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 131.47633822996224\n",
      "  mean_inference_ms: 13.013318996524337\n",
      "  mean_processing_ms: 0.4509183304819895\n",
      "time_since_restore: 32.36856961250305\n",
      "time_this_iter_s: 32.36856961250305\n",
      "time_total_s: 32.36856961250305\n",
      "timers:\n",
      "  learn_throughput: 61.97\n",
      "  learn_time_ms: 3227.38\n",
      "  sample_throughput: 6.864\n",
      "  sample_time_ms: 29138.885\n",
      "timestamp: 1594950496\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-48-49\n",
      "done: false\n",
      "episode_len_mean: 115.33333333333333\n",
      "episode_reward_max: 317.8782422193219\n",
      "episode_reward_mean: 227.76029938679144\n",
      "episode_reward_min: 160.54144902753924\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 3\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.969640731811523\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03347206860780716\n",
      "      policy_loss: -0.22927260398864746\n",
      "      total_loss: 46397.69140625\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 46397.9140625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6631388664245605\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03217076510190964\n",
      "      policy_loss: -0.2497987002134323\n",
      "      total_loss: 83230.109375\n",
      "      vf_explained_var: -2.9802322387695312e-08\n",
      "      vf_loss: 83230.359375\n",
      "  num_steps_sampled: 400\n",
      "  num_steps_trained: 400\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.0625\n",
      "  ram_util_percent: 24.84375\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 213.5627206162485\n",
      "  player_2: 120.940148476346\n",
      "policy_reward_mean:\n",
      "  player_1: 160.27211240844989\n",
      "  player_2: 67.48818697834152\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -22.791109144394603\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 133.52145464316402\n",
      "  mean_inference_ms: 13.047210035395366\n",
      "  mean_processing_ms: 0.45924743837901705\n",
      "time_since_restore: 66.03957080841064\n",
      "time_this_iter_s: 33.67100119590759\n",
      "time_total_s: 66.03957080841064\n",
      "timers:\n",
      "  learn_throughput: 60.141\n",
      "  learn_time_ms: 3325.503\n",
      "  sample_throughput: 6.736\n",
      "  sample_time_ms: 29692.287\n",
      "timestamp: 1594950529\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-49-25\n",
      "done: false\n",
      "episode_len_mean: 114.0\n",
      "episode_reward_max: 405.9534329637507\n",
      "episode_reward_mean: 252.4014684462987\n",
      "episode_reward_min: 160.54144902753924\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 5\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.967167854309082\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.028009939938783646\n",
      "      policy_loss: -0.31813934445381165\n",
      "      total_loss: 41981.4609375\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 41981.765625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6566991806030273\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.02792464569211006\n",
      "      policy_loss: -0.285426527261734\n",
      "      total_loss: 57209.0625\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 57209.33203125\n",
      "  num_steps_sampled: 600\n",
      "  num_steps_trained: 600\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.942\n",
      "  ram_util_percent: 24.796000000000003\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 253.97762828844634\n",
      "  player_2: 151.97580467530443\n",
      "policy_reward_mean:\n",
      "  player_1: 180.49151943980502\n",
      "  player_2: 71.90994900649366\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -22.791109144394603\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 135.219032795857\n",
      "  mean_inference_ms: 13.21494054304454\n",
      "  mean_processing_ms: 0.4815751210712954\n",
      "time_since_restore: 101.33288216590881\n",
      "time_this_iter_s: 35.29331135749817\n",
      "time_total_s: 101.33288216590881\n",
      "timers:\n",
      "  learn_throughput: 59.306\n",
      "  learn_time_ms: 3372.317\n",
      "  sample_throughput: 6.578\n",
      "  sample_time_ms: 30403.386\n",
      "timestamp: 1594950565\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600\n",
      "training_iteration: 3\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-50-02\n",
      "done: false\n",
      "episode_len_mean: 114.16666666666667\n",
      "episode_reward_max: 405.9534329637507\n",
      "episode_reward_mean: 263.0072403039913\n",
      "episode_reward_min: 160.54144902753924\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 6\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.9642229080200195\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.019623367115855217\n",
      "      policy_loss: -0.29048410058021545\n",
      "      total_loss: 70484.4453125\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 70484.71875\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6604576110839844\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.019218668341636658\n",
      "      policy_loss: -0.24837398529052734\n",
      "      total_loss: 148226.71875\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 148226.96875\n",
      "  num_steps_sampled: 800\n",
      "  num_steps_trained: 800\n",
      "iterations_since_restore: 4\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.80188679245283\n",
      "  ram_util_percent: 24.86037735849057\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 278.472080821663\n",
      "  player_2: 151.97580467530443\n",
      "policy_reward_mean:\n",
      "  player_1: 196.82161300344805\n",
      "  player_2: 66.18562730054332\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -22.791109144394603\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 136.1435586880227\n",
      "  mean_inference_ms: 13.336493594305784\n",
      "  mean_processing_ms: 0.4877438501929882\n",
      "time_since_restore: 138.69887971878052\n",
      "time_this_iter_s: 37.365997552871704\n",
      "time_total_s: 138.69887971878052\n",
      "timers:\n",
      "  learn_throughput: 55.785\n",
      "  learn_time_ms: 3585.201\n",
      "  sample_throughput: 6.433\n",
      "  sample_time_ms: 31087.707\n",
      "timestamp: 1594950602\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 800\n",
      "training_iteration: 4\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-50-39\n",
      "done: false\n",
      "episode_len_mean: 121.125\n",
      "episode_reward_max: 639.2918771731532\n",
      "episode_reward_mean: 304.7293118756204\n",
      "episode_reward_min: 160.54144902753924\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 8\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.963881492614746\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01951488107442856\n",
      "      policy_loss: -0.17516431212425232\n",
      "      total_loss: 57677.1015625\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 57677.26171875\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6564133167266846\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01910906285047531\n",
      "      policy_loss: -0.22726179659366608\n",
      "      total_loss: 96702.78125\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 96703.0\n",
      "  num_steps_sampled: 1000\n",
      "  num_steps_trained: 1000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.1\n",
      "  ram_util_percent: 25.003846153846155\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 421.61587270538905\n",
      "  player_2: 217.67600446776396\n",
      "policy_reward_mean:\n",
      "  player_1: 238.21346301290555\n",
      "  player_2: 66.51584886271479\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -82.66297736930551\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 137.8106397322819\n",
      "  mean_inference_ms: 13.546958384754364\n",
      "  mean_processing_ms: 0.4965752383131767\n",
      "time_since_restore: 175.83239245414734\n",
      "time_this_iter_s: 37.13351273536682\n",
      "time_total_s: 175.83239245414734\n",
      "timers:\n",
      "  learn_throughput: 55.132\n",
      "  learn_time_ms: 3627.664\n",
      "  sample_throughput: 6.342\n",
      "  sample_time_ms: 31537.083\n",
      "timestamp: 1594950639\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1000\n",
      "training_iteration: 5\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-51-14\n",
      "done: false\n",
      "episode_len_mean: 120.66666666666667\n",
      "episode_reward_max: 639.2918771731532\n",
      "episode_reward_mean: 293.116561331228\n",
      "episode_reward_min: 160.54144902753924\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 9\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.965359687805176\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.019833918660879135\n",
      "      policy_loss: -0.22488054633140564\n",
      "      total_loss: 110504.6328125\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 110504.84375\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.656216621398926\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.017910035327076912\n",
      "      policy_loss: -0.26881733536720276\n",
      "      total_loss: 142331.375\n",
      "      vf_explained_var: -2.9802322387695312e-08\n",
      "      vf_loss: 142331.640625\n",
      "  num_steps_sampled: 1200\n",
      "  num_steps_trained: 1200\n",
      "iterations_since_restore: 6\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.991999999999997\n",
      "  ram_util_percent: 25.087999999999997\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 421.61587270538905\n",
      "  player_2: 217.67600446776396\n",
      "policy_reward_mean:\n",
      "  player_1: 236.9902764269633\n",
      "  player_2: 56.12628490426471\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -82.66297736930551\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 138.3797556074819\n",
      "  mean_inference_ms: 13.62756470541655\n",
      "  mean_processing_ms: 0.49778840050406603\n",
      "time_since_restore: 211.14079213142395\n",
      "time_this_iter_s: 35.30839967727661\n",
      "time_total_s: 211.14079213142395\n",
      "timers:\n",
      "  learn_throughput: 55.297\n",
      "  learn_time_ms: 3616.827\n",
      "  sample_throughput: 6.335\n",
      "  sample_time_ms: 31571.61\n",
      "timestamp: 1594950674\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1200\n",
      "training_iteration: 6\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-51-49\n",
      "done: false\n",
      "episode_len_mean: 120.81818181818181\n",
      "episode_reward_max: 1417.9559399176803\n",
      "episode_reward_mean: 400.47637316912676\n",
      "episode_reward_min: 160.54144902753924\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 11\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.9592671394348145\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01864796131849289\n",
      "      policy_loss: -0.25357094407081604\n",
      "      total_loss: 34515.76171875\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 34516.0\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.653517723083496\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01693926751613617\n",
      "      policy_loss: -0.21361961960792542\n",
      "      total_loss: 38188.5703125\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 38188.7734375\n",
      "  num_steps_sampled: 1400\n",
      "  num_steps_trained: 1400\n",
      "iterations_since_restore: 7\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.373469387755105\n",
      "  ram_util_percent: 24.900000000000002\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 1337.2557718154558\n",
      "  player_2: 217.67600446776396\n",
      "policy_reward_mean:\n",
      "  player_1: 340.8533727366801\n",
      "  player_2: 59.623000432446595\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -82.66297736930551\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 139.16699094536048\n",
      "  mean_inference_ms: 13.739599841613813\n",
      "  mean_processing_ms: 0.4992743865093639\n",
      "time_since_restore: 245.72321343421936\n",
      "time_this_iter_s: 34.58242130279541\n",
      "time_total_s: 245.72321343421936\n",
      "timers:\n",
      "  learn_throughput: 55.794\n",
      "  learn_time_ms: 3584.623\n",
      "  sample_throughput: 6.346\n",
      "  sample_time_ms: 31517.051\n",
      "timestamp: 1594950709\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1400\n",
      "training_iteration: 7\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-52-25\n",
      "done: false\n",
      "episode_len_mean: 121.92307692307692\n",
      "episode_reward_max: 1417.9559399176803\n",
      "episode_reward_mean: 391.53308006824216\n",
      "episode_reward_min: 145.78568666594498\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 13\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.960725784301758\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.020365776494145393\n",
      "      policy_loss: -0.2550554871559143\n",
      "      total_loss: 47039.84375\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 47040.0859375\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.648188829421997\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01993827521800995\n",
      "      policy_loss: -0.2673588991165161\n",
      "      total_loss: 25695.39453125\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 25695.6484375\n",
      "  num_steps_sampled: 1600\n",
      "  num_steps_trained: 1600\n",
      "iterations_since_restore: 8\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.738\n",
      "  ram_util_percent: 25.033999999999995\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 1337.2557718154558\n",
      "  player_2: 217.67600446776396\n",
      "policy_reward_mean:\n",
      "  player_1: 340.23217624636203\n",
      "  player_2: 51.30090382188004\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -165.73779389714815\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 139.7562561805897\n",
      "  mean_inference_ms: 13.812094169905533\n",
      "  mean_processing_ms: 0.500817324633225\n",
      "time_since_restore: 281.30661392211914\n",
      "time_this_iter_s: 35.58340048789978\n",
      "time_total_s: 281.30661392211914\n",
      "timers:\n",
      "  learn_throughput: 55.621\n",
      "  learn_time_ms: 3595.78\n",
      "  sample_throughput: 6.336\n",
      "  sample_time_ms: 31565.822\n",
      "timestamp: 1594950745\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1600\n",
      "training_iteration: 8\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-53-10\n",
      "done: false\n",
      "episode_len_mean: 122.0\n",
      "episode_reward_max: 1417.9559399176803\n",
      "episode_reward_mean: 385.8485192828387\n",
      "episode_reward_min: 145.78568666594498\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 14\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 1.0125000476837158\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.962982177734375\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.011879336088895798\n",
      "      policy_loss: -0.1844642162322998\n",
      "      total_loss: 48198.7421875\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 48198.9140625\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6503119468688965\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.017393773421645164\n",
      "      policy_loss: -0.2075238972902298\n",
      "      total_loss: 31130.9140625\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 31131.11328125\n",
      "  num_steps_sampled: 1800\n",
      "  num_steps_trained: 1800\n",
      "iterations_since_restore: 9\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 18.8703125\n",
      "  ram_util_percent: 25.0265625\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 1337.2557718154558\n",
      "  player_2: 217.67600446776396\n",
      "policy_reward_mean:\n",
      "  player_1: 331.45075593535086\n",
      "  player_2: 54.397763347487704\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -165.73779389714815\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 140.28043264589624\n",
      "  mean_inference_ms: 13.905081676471164\n",
      "  mean_processing_ms: 0.5026988705163101\n",
      "time_since_restore: 326.30524945259094\n",
      "time_this_iter_s: 44.9986355304718\n",
      "time_total_s: 326.30524945259094\n",
      "timers:\n",
      "  learn_throughput: 54.254\n",
      "  learn_time_ms: 3686.394\n",
      "  sample_throughput: 6.141\n",
      "  sample_time_ms: 32567.905\n",
      "timestamp: 1594950790\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1800\n",
      "training_iteration: 9\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-17_03-53-46\n",
      "done: false\n",
      "episode_len_mean: 120.5625\n",
      "episode_reward_max: 1417.9559399176803\n",
      "episode_reward_mean: 374.9069813250714\n",
      "episode_reward_min: 145.78568666594498\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 16\n",
      "experiment_id: 51d9306d264e403484b769ee70757b7f\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 1.0125000476837158\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 4.965009689331055\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.016532255336642265\n",
      "      policy_loss: -0.22881579399108887\n",
      "      total_loss: 41645.5625\n",
      "      vf_explained_var: 5.960464477539063e-08\n",
      "      vf_loss: 41645.77734375\n",
      "    player_2:\n",
      "      cur_kl_coeff: 0.675000011920929\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 2.6482925415039062\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.01865498721599579\n",
      "      policy_loss: -0.22090482711791992\n",
      "      total_loss: 22315.6640625\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 22315.873046875\n",
      "  num_steps_sampled: 2000\n",
      "  num_steps_trained: 2000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 19.08235294117647\n",
      "  ram_util_percent: 25.023529411764713\n",
      "pid: 9401\n",
      "policy_reward_max:\n",
      "  player_1: 1337.2557718154558\n",
      "  player_2: 217.67600446776396\n",
      "policy_reward_mean:\n",
      "  player_1: 325.6402754088584\n",
      "  player_2: 49.266705916212956\n",
      "policy_reward_min:\n",
      "  player_1: 83.92105843716728\n",
      "  player_2: -165.73779389714815\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 141.13533391433268\n",
      "  mean_inference_ms: 14.05955696512805\n",
      "  mean_processing_ms: 0.5054547253936543\n",
      "time_since_restore: 362.4542670249939\n",
      "time_this_iter_s: 36.149017572402954\n",
      "time_total_s: 362.4542670249939\n",
      "timers:\n",
      "  learn_throughput: 54.53\n",
      "  learn_time_ms: 3667.678\n",
      "  sample_throughput: 6.139\n",
      "  sample_time_ms: 32575.951\n",
      "timestamp: 1594950826\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2000\n",
      "training_iteration: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_one(config, do_track=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gym_compete_policy import get_policy_value_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    }
   ],
   "source": [
    "obs = env_cls(env_config).reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = [trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3999962 ,  0.3999962 , -0.39984488, -0.3999962 ,  0.39890635,\n",
       "       -0.36216214,  0.01549192, -0.39968854,  0.22517362,  0.18174249,\n",
       "       -0.00849053,  0.37303784, -0.39654046, -0.39927378, -0.366909  ,\n",
       "        0.39690655, -0.3999962 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acts, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           24384       observation_preprocessing_layer_4\n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           4160        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer_ (None, 17)           0           concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer_2 multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n"
     ]
    }
   ],
   "source": [
    "nets = get_policy_value_nets(env_name, 0)\n",
    "policy_net_orig = nets['policy_mean_logstd_flat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 34), dtype=float32, numpy=\n",
       "array([[-2.644432  ,  1.6587822 , -1.3111818 , -2.5256457 ,  1.0890175 ,\n",
       "        -0.632396  ,  0.02013881, -1.5281134 ,  0.31184098,  0.24941634,\n",
       "        -0.01156497,  0.69752496, -1.1240312 , -1.470026  , -0.8531909 ,\n",
       "         0.9602913 , -2.8689044 , -1.200372  , -1.0477431 , -1.2787865 ,\n",
       "        -1.0475501 , -1.1048975 , -1.2324022 , -1.5336553 , -0.9083957 ,\n",
       "        -1.0634612 , -1.1298783 , -1.6375971 , -1.1652852 , -0.99094224,\n",
       "        -0.85480416, -0.82656866, -1.1858863 , -0.9486331 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = trainer.get_policy('player_1').model\n",
    "m.policy_net(obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 17, 2), dtype=float32, numpy=\n",
       "array([[[-2.6844046 , -1.1979917 ],\n",
       "        [ 1.6134702 , -1.0522642 ],\n",
       "        [-1.2499714 , -1.2768428 ],\n",
       "        [-2.5103867 , -1.0497713 ],\n",
       "        [ 1.2004242 , -1.1014946 ],\n",
       "        [-0.6241886 , -1.2318319 ],\n",
       "        [ 0.01984857, -1.5337685 ],\n",
       "        [-1.5142651 , -0.9082957 ],\n",
       "        [ 0.31476304, -1.0616486 ],\n",
       "        [ 0.1913874 , -1.1258026 ],\n",
       "        [-0.02376649, -1.6367302 ],\n",
       "        [ 0.7484977 , -1.1637001 ],\n",
       "        [-1.1188809 , -0.9924653 ],\n",
       "        [-1.473376  , -0.85632914],\n",
       "        [-0.8569962 , -0.82374895],\n",
       "        [ 0.9788799 , -1.1841927 ],\n",
       "        [-2.9387057 , -0.9448246 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nets['policy_mean_logstd'](obs['player_1'].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, logstd = tf.split(policy_net_orig(obs['player_1'].reshape(1, -1)), 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.4       ,  0.4       , -0.39953831, -0.4       ,  0.39932675,\n",
       "       -0.3645706 ,  0.0283459 , -0.39952335,  0.21959492,  0.14225847,\n",
       "       -0.03031551,  0.37967087, -0.39714089, -0.39899308, -0.36727401,\n",
       "        0.39759381, -0.4       ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.clip(np.random.normal(loc=mean[0], scale=np.exp(logstd[0]), size=(17,)), -0.4, 0.4) for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3999962 ,  0.3999962 , -0.3999962 , -0.3999962 ,  0.39809477,\n",
       "       -0.363111  ,  0.00771117, -0.39989665,  0.20237626,  0.18369985,\n",
       "       -0.01843473,  0.3739839 , -0.39581993, -0.39893857, -0.36984372,\n",
       "        0.39702076, -0.3999962 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([trainer.compute_action(obs['player_1'], policy_id='player_1') for _ in range(1000)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.0000001e-01,  4.0000001e-01, -4.0000001e-01, -4.0000001e-01,\n",
       "        4.0000001e-01, -4.0000001e-01,  2.8095581e-04, -4.0000001e-01,\n",
       "        1.8436955e-01, -7.9319030e-03,  5.6343660e-02,  4.0000001e-01,\n",
       "       -4.0000001e-01, -4.0000001e-01, -4.0000001e-01,  4.0000001e-01,\n",
       "       -4.0000001e-01], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.compute_action(obs['player_1'], policy_id='player_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4,\n",
       "       0.4, 0.4, 0.4, 0.4], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_cls(env_config).action_space.high"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
