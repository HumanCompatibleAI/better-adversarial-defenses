{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.registry import register_env, _global_registry, ENV_CREATOR\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym_compete_to_rllib import GymCompeteToRLLibAdapter\n",
    "\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "\n",
    "from load_gym_compete_policy import get_policy_value_nets\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.models.tf.tf_action_dist import DiagGaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_keras_model(model, n_times):\n",
    "    \"\"\"Make inputs work \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelModel(TFModelV2):\n",
    "    \"\"\"Create an RLLib policy from policy+value keras models.\"\"\"\n",
    "    def __init__(self, *args, policy_net=None, value_net=None, **kwargs):\n",
    "        super(KerasModelModel, self).__init__(*args, **kwargs)\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.register_variables(policy_net.variables + value_net.variables)\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict[\"obs\"]\n",
    "        model_out = self.policy_net(obs)\n",
    "        self._value_out = self.value_net(obs)\n",
    "        #if obs.shape[0] == 1:\n",
    "        self._value_out = self._value_out[0]\n",
    "        return model_out, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self._value_out\n",
    "    \n",
    "class GymCompetePretrainedModel(KerasModelModel):\n",
    "    \"\"\"Load a policy from gym_compete.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        env_name = args[3]['custom_model_config']['env_name']\n",
    "        agent_id = args[3]['custom_model_config']['agent_id']\n",
    "        nets = get_policy_value_nets(env_name, agent_id)\n",
    "        n_out = int(nets['policy_mean_logstd_flat'].output_shape[1])\n",
    "        super(GymCompetePretrainedModel, self).__init__(*args, **kwargs,\n",
    "                                                        policy_net=nets['policy_mean_logstd_flat'],\n",
    "                                                        value_net=nets['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68 / 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"GymCompetePretrainedModel\", GymCompetePretrainedModel)\n",
    "ModelCatalog.register_custom_action_dist(\"DiagGaussian\", DiagGaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {}\n",
    "env_name = 'multicomp/YouShallNotPassHumans-v0'\n",
    "env_name_rllib = env_name.split('/')[1] + '_rllib'\n",
    "def create_env(config=None, env_name=env_name):\n",
    "    env = gym.make(env_name)\n",
    "    return GymCompeteToRLLibAdapter(lambda: env)\n",
    "register_env(env_name_rllib, create_env)\n",
    "env_cls = create_env\n",
    "\n",
    "\n",
    "def build_trainer_config(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Build configuration for 1 run.\"\"\"\n",
    "    obs_space = env_cls(env_config).observation_space\n",
    "    act_space = env_cls(env_config).action_space\n",
    "\n",
    "    policy_template = \"player_%d\"\n",
    "\n",
    "    def get_agent_config(agent_id):\n",
    "        agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "            'model': {\n",
    "                        \"custom_model\": \"GymCompetePretrainedModel\",\n",
    "                        \"custom_model_config\": {\n",
    "                            \"agent_id\": agent_id,\n",
    "                            \"env_name\": env_name,\n",
    "                            \"model_config\": {},\n",
    "                            \"name\": \"model_%s\" % (agent_id - 1)\n",
    "                        },           \n",
    "                        \n",
    "                    },\n",
    "            \n",
    "            \"framework\": \"tfe\",\n",
    "        })\n",
    "        \n",
    "        return agent_config\n",
    "        \n",
    "        agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "                    \"model\": {\n",
    "                        \"use_lstm\": False,\n",
    "                        \"fcnet_hiddens\": [64, 64],\n",
    "                        #\"custom_action_dist\": \"DiagGaussian\",\n",
    "                    },\n",
    "                    \"framework\": \"tfe\",\n",
    "                })\n",
    "        \n",
    "        return agent_config\n",
    "\n",
    "    N_POLICIES = 2\n",
    "\n",
    "    policies = {policy_template % i: get_agent_config(i) for i in range(1, 1  + N_POLICIES)}\n",
    "    policies_keys = list(sorted(policies.keys()))\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        assert agent_id in [\"player_1\", \"player_2\"]\n",
    "        agent_ids = [\"player_1\", \"player_2\"]\n",
    "        \n",
    "        # selecting the corresponding policy (only for 2 policies)\n",
    "        return policies_keys[agent_ids.index(agent_id)]\n",
    "\n",
    "        # randomly choosing an opponent\n",
    "        # return np.random.choice(list(policies.keys()))\n",
    "    \n",
    "    if train_policies is None:\n",
    "        train_policies = list(policies.keys())\n",
    "        \n",
    "    for k in train_policies:\n",
    "        assert k in policies.keys()\n",
    "\n",
    "    config = {\n",
    "        \"env\": env_name_rllib,\n",
    "    #    \"gamma\": 0.9,\n",
    "      \"num_workers\": 0,\n",
    "    #  \"num_envs_per_worker\": 10,\n",
    "    #   \"rollout_fragment_length\": 10,\n",
    "       \"train_batch_size\": config['train_batch_size'],\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": train_policies,\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"framework\": \"tfe\",\n",
    "        #\"train_batch_size\": 512\n",
    "        #\"num_cpus_per_worker\": 2\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_trainer(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Create a RPS trainer for 2 agents, restore state, and only train specific policies.\"\"\"\n",
    "    \n",
    "    print(\"Using config\")\n",
    "    print(config)\n",
    "    cls = PPOTrainer\n",
    "    trainer = cls(config=config)\n",
    "    env = trainer.workers.local_worker().env\n",
    "    if restore_state is not None:\n",
    "        trainer.restore_from_object(restore_state)\n",
    "    return trainer\n",
    "\n",
    "def train(trainer, stop_iters, do_track=True):\n",
    "    \"\"\"Train the agents and return the state of the trainer.\"\"\"\n",
    "    for _ in range(stop_iters):\n",
    "        results = trainer.train()\n",
    "        print(pretty_print(results))\n",
    "        if do_track:\n",
    "            track.log(**results)\n",
    "    o = trainer.save_to_object()\n",
    "    return o\n",
    "\n",
    "trainer = None\n",
    "\n",
    "def train_one(config, restore_state=None, do_track=True):\n",
    "    print(config)\n",
    "    rl_config = build_trainer_config(restore_state=restore_state,\n",
    "                              train_policies=config['train_policies'],\n",
    "                              config=config)\n",
    "    global trainer\n",
    "    trainer = build_trainer(restore_state=None, config=rl_config)\n",
    "    train(trainer, config['train_steps'], do_track=do_track)\n",
    "\n",
    "\n",
    "# try changing learning rate\n",
    "config = {'train_batch_size': 128}\n",
    "\n",
    "config['train_steps'] = 3\n",
    "config['train_policies'] = ['player_1', 'player_2']\n",
    "config['num_workers'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip install 'ray[tune]' to see TensorBoard files.\n",
      "Could not instantiate TBXLogger: No module named 'tensorboardX'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 128, 'train_steps': 3, 'train_policies': ['player_1', 'player_2'], 'num_workers': 3}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Using config\n",
      "{'env': 'YouShallNotPassHumans-v0_rllib', 'num_workers': 0, 'train_batch_size': 128, 'multiagent': {'policies_to_train': ['player_1', 'player_2'], 'policies': {'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_0'}}, 'framework': 'tfe'}), 'player_2': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 2, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1'}}, 'framework': 'tfe'})}, 'policy_mapping_fn': <function build_trainer_config.<locals>.select_policy at 0x7fa354244ee0>}, 'framework': 'tfe'}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           24384       observation_preprocessing_layer_2\n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer_ (None, 17)           0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer_1 multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'multiagent-competition/gym_compete/agent_zoo/YouShallNotPassHumans-v0/agent3_parameters-v1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ab5b46cfb78b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-de82a2a60ae5>\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(config, restore_state, do_track)\u001b[0m\n\u001b[1;32m    115\u001b[0m                               config=config)\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrl_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_track\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-de82a2a60ae5>\u001b[0m in \u001b[0;36mbuild_trainer\u001b[0;34m(restore_state, train_policies, config)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrestore_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Evaluation setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mbefore_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;31m# Creating all workers (excluding evaluation workers).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             self.workers = self._make_workers(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 env_creator, self._policy, config, self.config[\"num_workers\"])\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_plan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[0;34m(self, env_creator, policy, config, num_workers)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mWorkerSet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mWorkerSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \"\"\"\n\u001b[0;32m--> 704\u001b[0;31m         return WorkerSet(\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, policy, trainer_config, num_workers, logdir, _setup)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Always create a local worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             self._local_worker = self._make_worker(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 RolloutWorker, env_creator, policy, 0, self._local_config)\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m_make_worker\u001b[0;34m(self, cls, env_creator, policy, worker_index, config)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \"extra_python_environs_for_worker\", None)\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         worker = cls(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, policy, policy_mapping_fn, policies_to_train, tf_session_creator, rollout_fragment_length, batch_mode, episode_horizon, preprocessor_pref, sample_async, compress_observations, num_envs, observation_fn, observation_filter, clip_rewards, clip_actions, env_config, model_config, policy_config, worker_index, num_workers, monitor_path, log_dir, log_level, callbacks, input_creator, input_evaluation, output_creator, remote_worker_envs, remote_env_batch_wait_ms, soft_horizon, no_done_at_end, seed, extra_python_environs, fake_sampler)\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \"This could be due to a bad CUDA or TF installation.\")\n\u001b[1;32m    414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             self.policy_map, self.preprocessors = self._build_policy_map(\n\u001b[0m\u001b[1;32m    416\u001b[0m                 policy_dict, policy_config)\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m_build_policy_map\u001b[0;34m(self, policy_dict, policy_config)\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                     \u001b[0mpolicy_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0mpolicy_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/policy/eager_tf_policy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, observation_space, action_space, config)\u001b[0m\n\u001b[1;32m    231\u001b[0m                                         config)\n\u001b[1;32m    232\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                 self.model = ModelCatalog.get_model_v2(\n\u001b[0m\u001b[1;32m    234\u001b[0m                     \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/chai/ray/python/ray/rllib/models/catalog.py\u001b[0m in \u001b[0;36mget_model_v2\u001b[0;34m(obs_space, action_space, num_outputs, model_config, framework, name, model_interface, default_model, **model_kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                         \u001b[0;31m# config[\"custom_model_config\"] anymore).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                             instance = model_cls(obs_space, action_space,\n\u001b[0m\u001b[1;32m    330\u001b[0m                                                  \u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                                                  name, **model_kwargs)\n",
      "\u001b[0;32m<ipython-input-5-be73c93c5b0d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0menv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'custom_model_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'env_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0magent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'custom_model_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'agent_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_policy_value_nets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy_mean_logstd_flat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         super(GymCompetePretrainedModel, self).__init__(*args, **kwargs,\n",
      "\u001b[0;32m~/git/chai/load_gym_compete_policy.py\u001b[0m in \u001b[0;36mget_policy_value_nets\u001b[0;34m(env_name, agent_id, pickle_path, variables_spec, version)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0magent_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mpolicy_unpickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv_name_2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/agent%s_parameters-v%d.pkl'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvariables_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mvariables_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_variables_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhid_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'multiagent-competition/gym_compete/agent_zoo/YouShallNotPassHumans-v0/agent3_parameters-v1.pkl'"
     ]
    }
   ],
   "source": [
    "train_one(config, do_track=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
