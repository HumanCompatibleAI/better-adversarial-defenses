{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib import agents\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from gym.spaces import Discrete, Box\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from functools import partial\n",
    "from ray.tune.registry import register_env, _global_registry, ENV_CREATOR\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo.ppo_tf_policy import PPOTFPolicy\n",
    "from ray.rllib.models import ModelCatalog\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import track\n",
    "\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from gym_compete_to_rllib import GymCompeteToRLLibAdapter\n",
    "\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "\n",
    "from load_gym_compete_policy import get_policy_value_nets\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.models.tf.tf_action_dist import DiagGaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_keras_model(model, n_times):\n",
    "    \"\"\"Make inputs work \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelModel(TFModelV2):\n",
    "    \"\"\"Create an RLLib policy from policy+value keras models.\"\"\"\n",
    "    def __init__(self, *args, policy_net=None, value_net=None, **kwargs):\n",
    "        super(KerasModelModel, self).__init__(*args, **kwargs)\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.register_variables(policy_net.variables + value_net.variables)\n",
    "        \n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict[\"obs\"]\n",
    "        model_out = self.policy_net(obs)\n",
    "        self._value_out = self.value_net(obs)\n",
    "        #if obs.shape[0] == 1:\n",
    "        self._value_out = self._value_out[0]\n",
    "        return model_out, state\n",
    "    \n",
    "    def value_function(self):\n",
    "        return self._value_out\n",
    "    \n",
    "class GymCompetePretrainedModel(KerasModelModel):\n",
    "    \"\"\"Load a policy from gym_compete.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        env_name = args[3]['custom_model_config']['env_name']\n",
    "        agent_id = args[3]['custom_model_config']['agent_id']\n",
    "        nets = get_policy_value_nets(env_name, agent_id)\n",
    "        n_out = int(nets['policy_mean_logstd_flat'].output_shape[1])\n",
    "        super(GymCompetePretrainedModel, self).__init__(*args, **kwargs,\n",
    "                                                        policy_net=nets['policy_mean_logstd_flat'],\n",
    "                                                        value_net=nets['value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "68 / 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"GymCompetePretrainedModel\", GymCompetePretrainedModel)\n",
    "ModelCatalog.register_custom_action_dist(\"DiagGaussian\", DiagGaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {}\n",
    "env_name = 'multicomp/YouShallNotPassHumans-v0'\n",
    "env_name_rllib = env_name.split('/')[1] + '_rllib'\n",
    "def create_env(config=None, env_name=env_name):\n",
    "    env = gym.make(env_name)\n",
    "    return GymCompeteToRLLibAdapter(lambda: env)\n",
    "register_env(env_name_rllib, create_env)\n",
    "env_cls = create_env\n",
    "\n",
    "\n",
    "def build_trainer_config(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Build configuration for 1 run.\"\"\"\n",
    "    obs_space = env_cls(env_config).observation_space\n",
    "    act_space = env_cls(env_config).action_space\n",
    "\n",
    "    policy_template = \"player_%d\"\n",
    "\n",
    "    def get_agent_config(agent_id):\n",
    "        agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "            'model': {\n",
    "                        \"custom_model\": \"GymCompetePretrainedModel\",\n",
    "                        \"custom_model_config\": {\n",
    "                            \"agent_id\": agent_id,\n",
    "                            \"env_name\": env_name,\n",
    "                            \"model_config\": {},\n",
    "                            \"name\": \"model_%s\" % agent_id\n",
    "                        },           \n",
    "                        \n",
    "                    },\n",
    "            \n",
    "            \"framework\": \"tfe\",\n",
    "        })\n",
    "        \n",
    "        return agent_config\n",
    "        \n",
    "        agent_config = (PPOTFPolicy, obs_space, act_space, {\n",
    "                    \"model\": {\n",
    "                        \"use_lstm\": False,\n",
    "                        \"fcnet_hiddens\": [64, 64],\n",
    "                        #\"custom_action_dist\": \"DiagGaussian\",\n",
    "                    },\n",
    "                    \"framework\": \"tfe\",\n",
    "                })\n",
    "        \n",
    "        return agent_config\n",
    "\n",
    "    N_POLICIES = 2\n",
    "\n",
    "    policies = {policy_template % i: get_agent_config(i) for i in range(N_POLICIES)}\n",
    "    policies_keys = list(sorted(policies.keys()))\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        assert agent_id in [\"player_1\", \"player_2\"]\n",
    "        agent_ids = [\"player_1\", \"player_2\"]\n",
    "        \n",
    "        # selecting the corresponding policy (only for 2 policies)\n",
    "        return policies_keys[agent_ids.index(agent_id)]\n",
    "\n",
    "        # randomly choosing an opponent\n",
    "        # return np.random.choice(list(policies.keys()))\n",
    "    \n",
    "    if train_policies is None:\n",
    "        train_policies = list(policies.keys())\n",
    "        \n",
    "    for k in train_policies:\n",
    "        assert k in policies.keys()\n",
    "\n",
    "    config = {\n",
    "        \"env\": env_name_rllib,\n",
    "    #    \"gamma\": 0.9,\n",
    "      \"num_workers\": 0,\n",
    "    #  \"num_envs_per_worker\": 10,\n",
    "    #   \"rollout_fragment_length\": 10,\n",
    "       \"train_batch_size\": config['train_batch_size'],\n",
    "        \"multiagent\": {\n",
    "            \"policies_to_train\": train_policies,\n",
    "            \"policies\": policies,\n",
    "            \"policy_mapping_fn\": select_policy,\n",
    "        },\n",
    "        \"framework\": \"tfe\",\n",
    "        #\"train_batch_size\": 512\n",
    "        #\"num_cpus_per_worker\": 2\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def build_trainer(restore_state=None, train_policies=None, config=None):\n",
    "    \"\"\"Create a RPS trainer for 2 agents, restore state, and only train specific policies.\"\"\"\n",
    "    \n",
    "    print(\"Using config\")\n",
    "    print(config)\n",
    "    cls = PPOTrainer\n",
    "    trainer = cls(config=config)\n",
    "    env = trainer.workers.local_worker().env\n",
    "    if restore_state is not None:\n",
    "        trainer.restore_from_object(restore_state)\n",
    "    return trainer\n",
    "\n",
    "def train(trainer, stop_iters, do_track=True):\n",
    "    \"\"\"Train the agents and return the state of the trainer.\"\"\"\n",
    "    for _ in range(stop_iters):\n",
    "        results = trainer.train()\n",
    "        print(pretty_print(results))\n",
    "        if do_track:\n",
    "            track.log(**results)\n",
    "    o = trainer.save_to_object()\n",
    "    return o\n",
    "\n",
    "trainer = None\n",
    "\n",
    "def train_one(config, restore_state=None, do_track=True):\n",
    "    print(config)\n",
    "    rl_config = build_trainer_config(restore_state=restore_state,\n",
    "                              train_policies=config['train_policies'],\n",
    "                              config=config)\n",
    "    global trainer\n",
    "    trainer = build_trainer(restore_state=None, config=rl_config)\n",
    "    train(trainer, config['train_steps'], do_track=do_track)\n",
    "\n",
    "\n",
    "# try changing learning rate\n",
    "config = {'train_batch_size': 128}\n",
    "\n",
    "config['train_steps'] = 3\n",
    "config['train_policies'] = ['player_1']\n",
    "config['num_workers'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_batch_size': 128, 'train_steps': 3, 'train_policies': ['player_1'], 'num_workers': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip install 'ray[tune]' to see TensorBoard files.\n",
      "Could not instantiate TBXLogger: No module named 'tensorboardX'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Using config\n",
      "{'env': 'YouShallNotPassHumans-v0_rllib', 'num_workers': 0, 'train_batch_size': 128, 'multiagent': {'policies_to_train': ['player_1'], 'policies': {'player_0': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 0, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_0'}}, 'framework': 'tfe'}), 'player_1': (<class 'ray.rllib.policy.tf_policy_template.PPOTFPolicy'>, Box(380,), Box(17,), {'model': {'custom_model': 'GymCompetePretrainedModel', 'custom_model_config': {'agent_id': 1, 'env_name': 'multicomp/YouShallNotPassHumans-v0', 'model_config': {}, 'name': 'model_1'}}, 'framework': 'tfe'})}, 'policy_mapping_fn': <function build_trainer_config.<locals>.select_policy at 0x7f138a407e50>}, 'framework': 'tfe'}\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           24384       observation_preprocessing_layer[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer  (None, 17)           0           concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer ( multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n",
      "Creating agent humanoid_blocker\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Creating agent humanoid\n",
      "Reading agent XML from: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/humanoid_body.xml\n",
      "Scene XML path: /home/sergei/git/chai/multiagent-competition/gym_compete/new_envs/assets/world_body.humanoid_body.humanoid_body.xml\n",
      "Created Scene with agents\n",
      "tf eager True\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 380)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observation_preprocessing_layer (None, 380)          761         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           24384       observation_preprocessing_layer_2\n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64)           4160        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 17)           1105        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "std (UnconnectedVariableLayer)  (None, 17)           17          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_mean (Reshape)          (None, 17, 1)        0           mean[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_std (Reshape)           (None, 17, 1)        0           std[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 17, 2)        0           reshape_mean[0][0]               \n",
      "                                                                 reshape_std[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "diagonal_normal_sampling_layer_ (None, 17)           0           concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 30,427\n",
      "Trainable params: 29,666\n",
      "Non-trainable params: 761\n",
      "__________________________________________________________________________________________________\n",
      "obsmean <class 'numpy.ndarray'> (380,)\n",
      "tf eager True\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "observation_preprocessing_la multiple                  761       \n",
      "_________________________________________________________________\n",
      "h1 (Dense)                   multiple                  24384     \n",
      "_________________________________________________________________\n",
      "h2 (Dense)                   multiple                  4160      \n",
      "_________________________________________________________________\n",
      "value (Dense)                multiple                  65        \n",
      "_________________________________________________________________\n",
      "value_postprocessing_layer_1 multiple                  2         \n",
      "=================================================================\n",
      "Total params: 29,372\n",
      "Trainable params: 28,609\n",
      "Non-trainable params: 763\n",
      "_________________________________________________________________\n",
      "Weights delta 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer observation_preprocessing_layer_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer observation_preprocessing_layer_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /home/sergei/git/chai/ray/python/ray/rllib/policy/tf_policy.py:869: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "custom_metrics: {}\n",
      "date: 2020-07-16_19-24-36\n",
      "done: false\n",
      "episode_len_mean: 31.6\n",
      "episode_reward_max: -84.2729919588802\n",
      "episode_reward_mean: -147.172563104019\n",
      "episode_reward_min: -250.01155990045035\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 5\n",
      "experiment_id: a433b741dba642afb553b79749ccb6ab\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.20000000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 17.20184898376465\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.03948865830898285\n",
      "      policy_loss: -0.2843933403491974\n",
      "      total_loss: 94885.4375\n",
      "      vf_explained_var: 0.0\n",
      "      vf_loss: 94885.71875\n",
      "  num_steps_sampled: 200\n",
      "  num_steps_trained: 200\n",
      "iterations_since_restore: 1\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 11.024999999999999\n",
      "  ram_util_percent: 15.125\n",
      "pid: 32321\n",
      "policy_reward_max:\n",
      "  player_0: -6.640434660889745\n",
      "  player_1: -77.63255729799045\n",
      "policy_reward_mean:\n",
      "  player_0: -31.524047780161983\n",
      "  player_1: -115.64851532385698\n",
      "policy_reward_min:\n",
      "  player_0: -80.40348114100169\n",
      "  player_1: -169.6080787594487\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 3.2663096242876195\n",
      "  mean_inference_ms: 13.199867893807328\n",
      "  mean_processing_ms: 0.46398153352500193\n",
      "time_since_restore: 5.41273045539856\n",
      "time_this_iter_s: 5.41273045539856\n",
      "time_total_s: 5.41273045539856\n",
      "timers:\n",
      "  learn_throughput: 99.949\n",
      "  learn_time_ms: 2001.014\n",
      "  sample_throughput: 58.674\n",
      "  sample_time_ms: 3408.647\n",
      "timestamp: 1594920276\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200\n",
      "training_iteration: 1\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-16_19-24-42\n",
      "done: false\n",
      "episode_len_mean: 32.63636363636363\n",
      "episode_reward_max: -84.2729919588802\n",
      "episode_reward_mean: -132.11670563929835\n",
      "episode_reward_min: -250.01155990045035\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 11\n",
      "experiment_id: a433b741dba642afb553b79749ccb6ab\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.30000001192092896\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 16.324100494384766\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.039966922253370285\n",
      "      policy_loss: -0.2826278507709503\n",
      "      total_loss: 117664.5703125\n",
      "      vf_explained_var: -5.960464477539063e-08\n",
      "      vf_loss: 117664.8359375\n",
      "  num_steps_sampled: 400\n",
      "  num_steps_trained: 400\n",
      "iterations_since_restore: 2\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 10.9\n",
      "  ram_util_percent: 15.2\n",
      "pid: 32321\n",
      "policy_reward_max:\n",
      "  player_0: -0.2964114565918763\n",
      "  player_1: -75.9938077335899\n",
      "policy_reward_mean:\n",
      "  player_0: -27.046743880907492\n",
      "  player_1: -105.06996175839086\n",
      "policy_reward_min:\n",
      "  player_0: -80.40348114100169\n",
      "  player_1: -169.6080787594487\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 3.4075459243968598\n",
      "  mean_inference_ms: 13.559844142344103\n",
      "  mean_processing_ms: 0.4713559416620917\n",
      "time_since_restore: 11.134587287902832\n",
      "time_this_iter_s: 5.7218568325042725\n",
      "time_total_s: 11.134587287902832\n",
      "timers:\n",
      "  learn_throughput: 101.028\n",
      "  learn_time_ms: 1979.646\n",
      "  sample_throughput: 55.784\n",
      "  sample_time_ms: 3585.261\n",
      "timestamp: 1594920282\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 400\n",
      "training_iteration: 2\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2020-07-16_19-24-47\n",
      "done: false\n",
      "episode_len_mean: 33.529411764705884\n",
      "episode_reward_max: -77.41414321917742\n",
      "episode_reward_mean: -139.12061312133088\n",
      "episode_reward_min: -256.84880905577\n",
      "episodes_this_iter: 6\n",
      "episodes_total: 17\n",
      "experiment_id: a433b741dba642afb553b79749ccb6ab\n",
      "hostname: Sergei\n",
      "info:\n",
      "  learner:\n",
      "    player_1:\n",
      "      cur_kl_coeff: 0.44999998807907104\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 16.613975524902344\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.02809898927807808\n",
      "      policy_loss: -0.19455410540103912\n",
      "      total_loss: 47783.3515625\n",
      "      vf_explained_var: 5.960464477539063e-08\n",
      "      vf_loss: 47783.53125\n",
      "  num_steps_sampled: 600\n",
      "  num_steps_trained: 600\n",
      "iterations_since_restore: 3\n",
      "node_ip: 172.23.114.216\n",
      "num_healthy_workers: 0\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 11.2625\n",
      "  ram_util_percent: 15.2\n",
      "pid: 32321\n",
      "policy_reward_max:\n",
      "  player_0: 4.503258636986674\n",
      "  player_1: -75.9938077335899\n",
      "policy_reward_mean:\n",
      "  player_0: -34.091707680601225\n",
      "  player_1: -105.02890544072967\n",
      "policy_reward_min:\n",
      "  player_0: -180.46450221477681\n",
      "  player_1: -169.6080787594487\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 3.4712287635577153\n",
      "  mean_inference_ms: 13.483066234540333\n",
      "  mean_processing_ms: 0.47139261424784445\n",
      "time_since_restore: 16.43358302116394\n",
      "time_this_iter_s: 5.298995733261108\n",
      "time_total_s: 16.43358302116394\n",
      "timers:\n",
      "  learn_throughput: 100.674\n",
      "  learn_time_ms: 1986.606\n",
      "  sample_throughput: 57.319\n",
      "  sample_time_ms: 3489.267\n",
      "timestamp: 1594920287\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 600\n",
      "training_iteration: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_one(config, do_track=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'learned00'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-801783c9a28c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learned00'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'learned00'"
     ]
    }
   ],
   "source": [
    "for v in trainer.get_weights()['learned00']:\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.get_policy('player_1').dist_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
