{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A simple multi-agent env with two agents playing rock paper scissors.\n",
    "\n",
    "This demonstrates running the following policies in competition:\n",
    "    (1) heuristic policy of repeating the same move\n",
    "    (2) heuristic policy of beating the last opponent move\n",
    "    (3) LSTM/feedforward PG policies\n",
    "    (4) LSTM policy with custom entropy loss\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import random\n",
    "from gym.spaces import Discrete\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.pg.pg import PGTrainer\n",
    "from ray.rllib.agents.pg.pg_tf_policy import PGTFPolicy\n",
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray.rllib.utils import try_import_tf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--stop\", type=int, default=1000)\n",
    "\n",
    "tf = try_import_tf()\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RockPaperScissorsEnv(MultiAgentEnv):\n",
    "    \"\"\"Two-player environment for rock paper scissors.\n",
    "\n",
    "    The observation is simply the last opponent action.\"\"\"\n",
    "\n",
    "    def __init__(self, _):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Discrete(3)\n",
    "        self.player1 = \"player1\"\n",
    "        self.player2 = \"player2\"\n",
    "        self.last_move = None\n",
    "        self.num_moves = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_move = (0, 0)\n",
    "        self.num_moves = 0\n",
    "        return {\n",
    "            self.player1: self.last_move[1],\n",
    "            self.player2: self.last_move[0],\n",
    "        }\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        move1 = action_dict[self.player1]\n",
    "        move2 = action_dict[self.player2]\n",
    "        self.last_move = (move1, move2)\n",
    "        obs = {\n",
    "            self.player1: self.last_move[1],\n",
    "            self.player2: self.last_move[0],\n",
    "        }\n",
    "        r1, r2 = {\n",
    "            (ROCK, ROCK): (0, 0),\n",
    "            (ROCK, PAPER): (-1, 1),\n",
    "            (ROCK, SCISSORS): (1, -1),\n",
    "            (PAPER, ROCK): (1, -1),\n",
    "            (PAPER, PAPER): (0, 0),\n",
    "            (PAPER, SCISSORS): (-1, 1),\n",
    "            (SCISSORS, ROCK): (-1, 1),\n",
    "            (SCISSORS, PAPER): (1, -1),\n",
    "            (SCISSORS, SCISSORS): (0, 0),\n",
    "        }[move1, move2]\n",
    "        rew = {\n",
    "            self.player1: r1,\n",
    "            self.player2: r2,\n",
    "        }\n",
    "        self.num_moves += 1\n",
    "        done = {\n",
    "            \"__all__\": self.num_moves >= 10,\n",
    "        }\n",
    "        return obs, rew, done, {}\n",
    "\n",
    "\n",
    "class AlwaysSameHeuristic(Policy):\n",
    "    \"\"\"Pick a random move and stick with it for the entire episode.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return [random.choice([ROCK, PAPER, SCISSORS])]\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        return list(state_batches[0]), state_batches, {}\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        pass\n",
    "\n",
    "    def get_weights(self):\n",
    "        pass\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BeatLastHeuristic(Policy):\n",
    "    \"\"\"Play the move that would beat the last move of the opponent.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.exploration = self._create_exploration()\n",
    "\n",
    "    def compute_actions(self,\n",
    "                        obs_batch,\n",
    "                        state_batches=None,\n",
    "                        prev_action_batch=None,\n",
    "                        prev_reward_batch=None,\n",
    "                        info_batch=None,\n",
    "                        episodes=None,\n",
    "                        **kwargs):\n",
    "        def successor(x):\n",
    "            if x[ROCK] == 1:\n",
    "                return PAPER\n",
    "            elif x[PAPER] == 1:\n",
    "                return SCISSORS\n",
    "            elif x[SCISSORS] == 1:\n",
    "                return ROCK\n",
    "\n",
    "        return [successor(x) for x in obs_batch], [], {}\n",
    "\n",
    "    def learn_on_batch(self, samples):\n",
    "        pass\n",
    "\n",
    "    def get_weights(self):\n",
    "        pass\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_same_policy(args):\n",
    "    \"\"\"Use the same policy for both agents (trivial case).\"\"\"\n",
    "\n",
    "    tune.run(\n",
    "        \"PG\",\n",
    "        stop={\"timesteps_total\": args.stop},\n",
    "        config={\"env\": RockPaperScissorsEnv})\n",
    "\n",
    "\n",
    "def run_heuristic_vs_learned(args, use_lstm=False, trainer=\"PG\"):\n",
    "    \"\"\"Run heuristic policies vs a learned agent.\n",
    "\n",
    "    The learned agent should eventually reach a reward of ~5 with\n",
    "    use_lstm=False, and ~7 with use_lstm=True. The reason the LSTM policy\n",
    "    can perform better is since it can distinguish between the always_same vs\n",
    "    beat_last heuristics.\n",
    "    \"\"\"\n",
    "\n",
    "    def select_policy(agent_id):\n",
    "        if agent_id == \"player1\":\n",
    "            return \"learned\"\n",
    "        else:\n",
    "            return random.choice([\"always_same\", \"beat_last\"])\n",
    "\n",
    "    tune.run(\n",
    "        trainer,\n",
    "        stop={\"timesteps_total\": args.stop},\n",
    "        config={\n",
    "            \"env\": RockPaperScissorsEnv,\n",
    "            \"gamma\": 0.9,\n",
    "            \"num_workers\": 0,\n",
    "            \"num_envs_per_worker\": 4,\n",
    "            \"rollout_fragment_length\": 10,\n",
    "            \"train_batch_size\": 200,\n",
    "            \"multiagent\": {\n",
    "                \"policies_to_train\": [\"learned\"],\n",
    "                \"policies\": {\n",
    "                    \"always_same\": (AlwaysSameHeuristic, Discrete(3),\n",
    "                                    Discrete(3), {}),\n",
    "                    \"beat_last\": (BeatLastHeuristic, Discrete(3), Discrete(3),\n",
    "                                  {}),\n",
    "                    \"learned\": (None, Discrete(3), Discrete(3), {\n",
    "                        \"model\": {\n",
    "                            \"use_lstm\": use_lstm\n",
    "                        }\n",
    "                    }),\n",
    "                },\n",
    "                \"policy_mapping_fn\": select_policy,\n",
    "            },\n",
    "        })\n",
    "\n",
    "\n",
    "def run_with_custom_entropy_loss(args):\n",
    "    \"\"\"Example of customizing the loss function of an existing policy.\n",
    "\n",
    "    This performs about the same as the default loss does.\"\"\"\n",
    "\n",
    "    def entropy_policy_gradient_loss(policy, model, dist_class, train_batch):\n",
    "        logits, _ = model.from_batch(train_batch)\n",
    "        action_dist = dist_class(logits, model)\n",
    "        return (-0.1 * action_dist.entropy() - tf.reduce_mean(\n",
    "            action_dist.logp(train_batch[\"actions\"]) *\n",
    "            train_batch[\"advantages\"]))\n",
    "\n",
    "    EntropyPolicy = PGTFPolicy.with_updates(\n",
    "        loss_fn=entropy_policy_gradient_loss)\n",
    "    EntropyLossPG = PGTrainer.with_updates(\n",
    "        name=\"EntropyPG\", get_policy_class=lambda _: EntropyPolicy)\n",
    "    run_heuristic_vs_learned(args, use_lstm=True, trainer=EntropyLossPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-23 17:15:45,379\tINFO resource_spec.py:204 -- Starting Ray with 2.83 GiB memory available for workers and up to 1.42 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-06-23 17:15:46,904\tINFO services.py:1168 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m E0623 17:15:49.548642600    3903 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925349.548612900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m E0623 17:15:49.549015700    3903 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3896)\u001b[0m E0623 17:15:50.038192800    3896 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.038172900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3896)\u001b[0m E0623 17:15:50.038433600    3896 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m E0623 17:15:50.071091200    3899 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.071067600\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m E0623 17:15:50.071341400    3899 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0623 17:15:47.508080900    3878 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925347.508057900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0623 17:15:47.508590300    3878 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[33m(pid=raylet)\u001b[0m E0623 17:15:47.888002200    3878 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m E0623 17:15:50.049885300    3902 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.049867100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m E0623 17:15:50.050107400    3902 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3904)\u001b[0m E0623 17:15:50.236120000    3904 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.236101500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3904)\u001b[0m E0623 17:15:50.236356300    3904 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3905)\u001b[0m E0623 17:15:50.500420500    3905 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.500398700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3905)\u001b[0m E0623 17:15:50.500681700    3905 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3897)\u001b[0m E0623 17:15:50.504752500    3897 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.504732700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3897)\u001b[0m E0623 17:15:50.505008300    3897 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3907)\u001b[0m E0623 17:15:50.568904000    3907 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.568881300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3907)\u001b[0m E0623 17:15:50.569280900    3907 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3906)\u001b[0m E0623 17:15:50.565111900    3906 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.565092900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3906)\u001b[0m E0623 17:15:50.565336000    3906 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3908)\u001b[0m E0623 17:15:50.559930400    3908 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.559908400\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3908)\u001b[0m E0623 17:15:50.560208900    3908 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3901)\u001b[0m E0623 17:15:50.750460800    3901 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925350.750420100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3901)\u001b[0m E0623 17:15:50.750763700    3901 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m E0623 17:15:51.433570300    3900 socket_utils_common_posix.cc:208] check for SO_REUSEPORT: {\"created\":\"@1592925351.433552300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":185,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m E0623 17:15:51.433805100    3900 socket_utils_common_posix.cc:313] setsockopt(TCP_USER_TIMEOUT) Protocol not available\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m 2020-06-23 17:15:54,585\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m 2020-06-23 17:15:54,587\tINFO trainer.py:578 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m 2020-06-23 17:15:55,089\tWARNING trainer_template.py:123 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m 2020-06-23 17:15:55,089\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=3899)\u001b[0m 2020-06-23 17:15:55,089\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-15-55\n",
      "  done: false\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: c3dcbbcb5122435a84e9038466325533\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      model: {}\n",
      "    num_steps_sampled: 400\n",
      "    num_steps_trained: 400\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.8\n",
      "    ram_util_percent: 76.0\n",
      "  pid: 3899\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.03301207698992828\n",
      "    mean_inference_ms: 1.4000033857810554\n",
      "    mean_processing_ms: 0.49212204283149685\n",
      "  time_since_restore: 0.45296287536621094\n",
      "  time_this_iter_s: 0.45296287536621094\n",
      "  time_total_s: 0.45296287536621094\n",
      "  timers:\n",
      "    learn_throughput: 6667.07\n",
      "    learn_time_ms: 59.996\n",
      "    sample_throughput: 1020.34\n",
      "    sample_time_ms: 392.026\n",
      "  timestamp: 1592925355\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>RUNNING </td><td>192.168.1.45:3899</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.452963</td><td style=\"text-align: right;\"> 400</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-15-56\n",
      "  done: true\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: c3dcbbcb5122435a84e9038466325533\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      model: {}\n",
      "    num_steps_sampled: 1200\n",
      "    num_steps_trained: 1200\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 3899\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.034886251661536256\n",
      "    mean_inference_ms: 1.3272085008224939\n",
      "    mean_processing_ms: 0.491137815304975\n",
      "  time_since_restore: 1.1745717525482178\n",
      "  time_this_iter_s: 0.3583505153656006\n",
      "  time_total_s: 1.1745717525482178\n",
      "  timers:\n",
      "    learn_throughput: 14988.579\n",
      "    learn_time_ms: 26.687\n",
      "    sample_throughput: 1098.766\n",
      "    sample_time_ms: 364.045\n",
      "  timestamp: 1592925356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1200\n",
      "  training_iteration: 3\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1.17457</td><td style=\"text-align: right;\">1200</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_same_policy: ok.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m 2020-06-23 17:16:01,383\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m 2020-06-23 17:16:01,390\tINFO trainer.py:578 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m 2020-06-23 17:16:01,911\tWARNING trainer_template.py:123 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m 2020-06-23 17:16:01,912\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=3900)\u001b[0m 2020-06-23 17:16:01,912\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-16-02\n",
      "  done: false\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 08a38c276f904a1689c9e39d980a76e8\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      learned:\n",
      "        model: {}\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.1\n",
      "    ram_util_percent: 76.2\n",
      "  pid: 3900\n",
      "  policy_reward_max:\n",
      "    always_same: 4.0\n",
      "    beat_last: 4.0\n",
      "    learned: 4.0\n",
      "  policy_reward_mean:\n",
      "    always_same: 0.6923076923076923\n",
      "    beat_last: 0.2857142857142857\n",
      "    learned: -0.55\n",
      "  policy_reward_min:\n",
      "    always_same: -2.0\n",
      "    beat_last: -4.0\n",
      "    learned: -4.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07451281828038832\n",
      "    mean_inference_ms: 2.506003660314223\n",
      "    mean_processing_ms: 1.536271151374368\n",
      "  time_since_restore: 0.2836737632751465\n",
      "  time_this_iter_s: 0.2836737632751465\n",
      "  time_total_s: 0.2836737632751465\n",
      "  timers:\n",
      "    learn_throughput: 3438.433\n",
      "    learn_time_ms: 58.166\n",
      "    sample_throughput: 891.192\n",
      "    sample_time_ms: 224.419\n",
      "  timestamp: 1592925362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>RUNNING </td><td>192.168.1.45:3900</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.283674</td><td style=\"text-align: right;\"> 200</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-16-02\n",
      "  done: true\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 08a38c276f904a1689c9e39d980a76e8\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      learned:\n",
      "        model: {}\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 3900\n",
      "  policy_reward_max:\n",
      "    always_same: 4.0\n",
      "    beat_last: 5.0\n",
      "    learned: 6.0\n",
      "  policy_reward_mean:\n",
      "    always_same: -0.07692307692307693\n",
      "    beat_last: 0.3958333333333333\n",
      "    learned: -0.15\n",
      "  policy_reward_min:\n",
      "    always_same: -6.0\n",
      "    beat_last: -4.0\n",
      "    learned: -5.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0792821452293502\n",
      "    mean_inference_ms: 1.7834565792182753\n",
      "    mean_processing_ms: 1.3736229573026144\n",
      "  time_since_restore: 0.8187234401702881\n",
      "  time_this_iter_s: 0.1218259334564209\n",
      "  time_total_s: 0.8187234401702881\n",
      "  timers:\n",
      "    learn_throughput: 12438.366\n",
      "    learn_time_ms: 16.079\n",
      "    sample_throughput: 1367.605\n",
      "    sample_time_ms: 146.241\n",
      "  timestamp: 1592925362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 5\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        0.818723</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_heuristic_vs_learned(w/ lstm): ok.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m 2020-06-23 17:16:06,486\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m 2020-06-23 17:16:06,493\tINFO trainer.py:578 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m 2020-06-23 17:16:06,947\tWARNING trainer_template.py:123 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m 2020-06-23 17:16:06,947\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=3903)\u001b[0m 2020-06-23 17:16:06,948\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for PG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-16-07\n",
      "  done: false\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 18b020a609bf4380afaf3b7751edaeab\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      learned:\n",
      "        model: {}\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0\n",
      "    ram_util_percent: 75.9\n",
      "  pid: 3903\n",
      "  policy_reward_max:\n",
      "    always_same: 2.0\n",
      "    beat_last: 4.0\n",
      "    learned: 5.0\n",
      "  policy_reward_mean:\n",
      "    always_same: -0.125\n",
      "    beat_last: -0.16666666666666666\n",
      "    learned: 0.15\n",
      "  policy_reward_min:\n",
      "    always_same: -4.0\n",
      "    beat_last: -5.0\n",
      "    learned: -4.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06963692459405639\n",
      "    mean_inference_ms: 2.3529716566497205\n",
      "    mean_processing_ms: 1.1078890632180607\n",
      "  time_since_restore: 0.24641966819763184\n",
      "  time_this_iter_s: 0.24641966819763184\n",
      "  time_total_s: 0.24641966819763184\n",
      "  timers:\n",
      "    learn_throughput: 3570.486\n",
      "    learn_time_ms: 56.015\n",
      "    sample_throughput: 1056.672\n",
      "    sample_time_ms: 189.274\n",
      "  timestamp: 1592925367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n",
      "Result for PG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-16-07\n",
      "  done: true\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 18b020a609bf4380afaf3b7751edaeab\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      learned:\n",
      "        model: {}\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 3903\n",
      "  policy_reward_max:\n",
      "    always_same: 8.0\n",
      "    beat_last: 6.0\n",
      "    learned: 5.0\n",
      "  policy_reward_mean:\n",
      "    always_same: -0.023255813953488372\n",
      "    beat_last: 0.03508771929824561\n",
      "    learned: -0.01\n",
      "  policy_reward_min:\n",
      "    always_same: -5.0\n",
      "    beat_last: -5.0\n",
      "    learned: -8.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06611916347199089\n",
      "    mean_inference_ms: 1.6375171591327011\n",
      "    mean_processing_ms: 1.060223121137181\n",
      "  time_since_restore: 0.7186262607574463\n",
      "  time_this_iter_s: 0.11274456977844238\n",
      "  time_total_s: 0.7186262607574463\n",
      "  timers:\n",
      "    learn_throughput: 13438.157\n",
      "    learn_time_ms: 14.883\n",
      "    sample_throughput: 1565.834\n",
      "    sample_time_ms: 127.728\n",
      "  timestamp: 1592925367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 5\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/PG<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PG_RockPaperScissorsEnv_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        0.718626</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_heuristic_vs_learned (w/o lstm): ok.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/EntropyPG<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>EntropyPG_RockPaperScissorsEnv_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m 2020-06-23 17:16:11,042\tINFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m 2020-06-23 17:16:11,047\tINFO trainer.py:578 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m 2020-06-23 17:16:11,536\tWARNING trainer_template.py:123 -- The experimental distributed execution API is enabled for this algorithm. Disable this by setting 'use_exec_api': False.\n",
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m 2020-06-23 17:16:11,537\tINFO trainable.py:217 -- Getting current IP.\n",
      "\u001b[2m\u001b[36m(pid=3902)\u001b[0m 2020-06-23 17:16:11,537\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "Result for EntropyPG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 706e86afc4174bc4a67f74d4e41de712\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      learned:\n",
      "        model: {}\n",
      "    num_steps_sampled: 200\n",
      "    num_steps_trained: 200\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.0\n",
      "    ram_util_percent: 75.8\n",
      "  pid: 3902\n",
      "  policy_reward_max:\n",
      "    always_same: 6.0\n",
      "    beat_last: 5.0\n",
      "    learned: 6.0\n",
      "  policy_reward_mean:\n",
      "    always_same: 1.2857142857142858\n",
      "    beat_last: -0.07692307692307693\n",
      "    learned: -0.4\n",
      "  policy_reward_min:\n",
      "    always_same: -3.0\n",
      "    beat_last: -6.0\n",
      "    learned: -6.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06907126482795264\n",
      "    mean_inference_ms: 2.085797926958869\n",
      "    mean_processing_ms: 1.0948882383458751\n",
      "  time_since_restore: 0.2687561511993408\n",
      "  time_this_iter_s: 0.2687561511993408\n",
      "  time_total_s: 0.2687561511993408\n",
      "  timers:\n",
      "    learn_throughput: 2168.983\n",
      "    learn_time_ms: 92.209\n",
      "    sample_throughput: 1137.987\n",
      "    sample_time_ms: 175.749\n",
      "  timestamp: 1592925371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200\n",
      "  training_iteration: 1\n",
      "  trial_id: '00000'\n",
      "  \n",
      "Result for EntropyPG_RockPaperScissorsEnv_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-06-23_17-16-12\n",
      "  done: true\n",
      "  episode_len_mean: 10.0\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 706e86afc4174bc4a67f74d4e41de712\n",
      "  experiment_tag: '0'\n",
      "  hostname: Sergei\n",
      "  info:\n",
      "    learner:\n",
      "      learned:\n",
      "        model: {}\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.45\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf: {}\n",
      "  pid: 3902\n",
      "  policy_reward_max:\n",
      "    always_same: 6.0\n",
      "    beat_last: 5.0\n",
      "    learned: 6.0\n",
      "  policy_reward_mean:\n",
      "    always_same: 0.11764705882352941\n",
      "    beat_last: -0.5510204081632653\n",
      "    learned: 0.21\n",
      "  policy_reward_min:\n",
      "    always_same: -5.0\n",
      "    beat_last: -6.0\n",
      "    learned: -6.0\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06334997872179846\n",
      "    mean_inference_ms: 1.50268566480958\n",
      "    mean_processing_ms: 1.0152802235145473\n",
      "  time_since_restore: 0.7026069164276123\n",
      "  time_this_iter_s: 0.0919349193572998\n",
      "  time_total_s: 0.7026069164276123\n",
      "  timers:\n",
      "    learn_throughput: 8763.127\n",
      "    learn_time_ms: 22.823\n",
      "    sample_throughput: 1715.042\n",
      "    sample_time_ms: 116.615\n",
      "  timestamp: 1592925372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 5\n",
      "  trial_id: '00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/15.8 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/2.83 GiB heap, 0.0/0.98 GiB objects<br>Result logdir: /home/sergei/ray_results/EntropyPG<br>Number of trials: 1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>EntropyPG_RockPaperScissorsEnv_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">        0.702607</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_with_custom_entropy_loss: ok.\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args([])\n",
    "run_same_policy(args)\n",
    "print(\"run_same_policy: ok.\")\n",
    "run_heuristic_vs_learned(args, use_lstm=True)\n",
    "print(\"run_heuristic_vs_learned(w/ lstm): ok.\")\n",
    "run_heuristic_vs_learned(args, use_lstm=False)\n",
    "print(\"run_heuristic_vs_learned (w/o lstm): ok.\")\n",
    "run_with_custom_entropy_loss(args)\n",
    "print(\"run_with_custom_entropy_loss: ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.examples as e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
